Directory structure:
└── orchestrator/
    ├── README.md
    ├── Dockerfile
    ├── Makefile
    ├── main.go
    ├── orchestrator.proto
    ├── upload.sh
    ├── cmd/
    │   ├── inspect-data/
    │   │   └── main.go
    │   ├── inspect-header/
    │   │   └── main.go
    │   ├── mock-nbd/
    │   │   └── mock.go
    │   ├── mock-sandbox/
    │   │   └── mock.go
    │   ├── mock-snapshot/
    │   │   └── mock.go
    │   └── simulate-headers-merge/
    │       └── main.go
    └── internal/
        ├── consul/
        │   └── node.go
        ├── dns/
        │   └── server.go
        ├── sandbox/
        │   ├── checks.go
        │   ├── checks_test.go
        │   ├── cleanup.go
        │   ├── envd.go
        │   ├── metrics.go
        │   ├── sandbox_linux.go
        │   ├── sandbox_other.go
        │   ├── block/
        │   │   ├── cache.go
        │   │   ├── chunk.go
        │   │   ├── device.go
        │   │   ├── local.go
        │   │   ├── overlay.go
        │   │   └── tracker.go
        │   ├── build/
        │   │   ├── build.go
        │   │   ├── cache.go
        │   │   ├── cache_test.go
        │   │   ├── diff.go
        │   │   ├── local_diff.go
        │   │   └── storage_diff.go
        │   ├── fc/
        │   │   ├── client_linux.go
        │   │   ├── client_other.go
        │   │   ├── mmds.go
        │   │   └── process.go
        │   ├── nbd/
        │   │   ├── dispatch.go
        │   │   ├── path_direct_linux.go
        │   │   ├── path_direct_other.go
        │   │   └── pool.go
        │   ├── network/
        │   │   ├── blocking_rules.go
        │   │   ├── host_linux.go
        │   │   ├── host_other.go
        │   │   ├── network_linux.go
        │   │   ├── network_other.go
        │   │   ├── pool.go
        │   │   ├── slot.go
        │   │   ├── storage.go
        │   │   ├── storage_kv.go
        │   │   └── storage_memory.go
        │   ├── rootfs/
        │   │   └── cow.go
        │   ├── socket/
        │   │   └── socket.go
        │   ├── template/
        │   │   ├── cache.go
        │   │   ├── file.go
        │   │   ├── local_file.go
        │   │   ├── storage.go
        │   │   ├── storage_file.go
        │   │   ├── storage_template.go
        │   │   └── template.go
        │   └── uffd/
        │       ├── handler.go
        │       ├── serve_linux.go
        │       └── serve_other.go
        └── server/
            ├── main.go
            ├── sandboxes.go
            ├── sandboxes_test.go
            └── template_cache.go

================================================
File: README.md
================================================
# Orchestrator


================================================
File: Dockerfile
================================================
FROM golang:1.23 AS builder

WORKDIR /build/shared

COPY .shared/go.mod .shared/go.sum ./
RUN go mod download

COPY .shared/pkg pkg

WORKDIR /build/orchestrator

COPY go.mod go.sum ./
RUN go mod download

COPY main.go Makefile ./
COPY internal internal

ARG COMMIT_SHA
RUN --mount=type=cache,target=/root/.cache/go-build make build-local COMMIT_SHA=${COMMIT_SHA}

FROM scratch

COPY --from=builder /build/orchestrator/bin/orchestrator .



================================================
File: Makefile
================================================
ENV := $(shell cat ../../.last_used_env || echo "not-set")
-include ../../.env.${ENV}

client := gcloud compute instances list --format='csv(name)' --project $(GCP_PROJECT_ID) | grep "client"

.PHONY: init
init:
	brew install protobuf
	go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28
	go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2

.PHONY: generate
generate:
	# You need to install protobuf (brew install protobuf) and following go packages: protoc-gen-go, protoc-gen-go-grpc
	# https://grpc.io/docs/languages/go/quickstart/
	@echo "Generating..."
	@protoc --go_out=../shared/pkg/grpc/orchestrator/ --go_opt=paths=source_relative --go-grpc_out=../shared/pkg/grpc/orchestrator/ --go-grpc_opt=paths=source_relative orchestrator.proto
	@echo "Done"

.PHONY: build
build:
	$(eval COMMIT_SHA := $(shell git rev-parse --short HEAD))
	@rm -rf .shared/
	@cp -r ../shared .shared/
	@docker build --platform linux/amd64 --output=bin --build-arg COMMIT_SHA="$(COMMIT_SHA)" .
	@rm -rf .shared/

.PHONY: build-local
build-local:
	# Allow for passing commit sha directly for docker builds
	$(eval COMMIT_SHA ?= $(shell git rev-parse --short HEAD))
	CGO_ENABLED=1 GOOS=linux GOARCH=amd64 go build -o bin/orchestrator -ldflags "-X=main.commitSHA=$(COMMIT_SHA)" .

.PHONY: build-debug
build-debug:
	CGO_ENABLED=1 GOOS=linux GOARCH=amd64 go build -race -gcflags=all="-N -l" -o bin/orchestrator .

.PHONY: run-debug
run-debug:
	make build-debug
	sudo -E GOTRACEBACK=crash \
	GODEBUG=madvdontneed=1 \
	NODE_ID="testclient" \
	TEMPLATE_BUCKET_NAME=$(TEMPLATE_BUCKET_NAME) \
	ENVD_TIMEOUT=$(ENVD_TIMEOUT) \
	./bin/orchestrator

.PHONY: upload
upload:
	./upload.sh $(GCP_PROJECT_ID)

.PHONY: build-and-upload
build-and-upload: build upload

.PHONY: mock
mock:
	sudo TEMPLATE_BUCKET_NAME=$(TEMPLATE_BUCKET_NAME) CONSUL_TOKEN=$(CONSUL_TOKEN) NODE_ID="testclient" go run cmd/mock-sandbox/mock.go -template 5wzg6c91u51yaebviysf -build "f0370054-b669-eeee-b33b-573d5287c6ef" -alive 1 -count 2

.PHONY: mock-nbd
mock-nbd:
	sudo go run -gcflags=all="-N -l" cmd/mock-nbd/mock.go

.PHONY: killall
killall:
	gcloud compute instance-groups list-instances $(PREFIX)orch-client-ig \
          	  --zone=$(GCP_ZONE) \
          	  --project=$(GCP_PROJECT_ID) \
          	  --format="value(instance)" \
          	  --quiet | xargs -I {} -P 5 sh -c "gcloud compute ssh {} --project=$(GCP_PROJECT_ID) --zone=$(GCP_ZONE) --command='sudo killall -9 firecracker'"
	@echo "Killing all firecracker processes"

.PHONY: kill-old
kill-old:
	gcloud compute instance-groups list-instances $(PREFIX)orch-client-ig \
          	  --zone=$(GCP_ZONE) \
          	  --project=$(GCP_PROJECT_ID) \
          	  --format="value(instance)" \
          	  --quiet | xargs -I {} -P 5 sh -c "gcloud compute ssh {} --project=$(GCP_PROJECT_ID) --zone=$(GCP_ZONE) --command='sudo killall -9 --older-than 24h firecracker'"
	@echo "Killing all firecracker processes"

.PHONY: mock-snapshot
mock-snapshot:
	sudo TEMPLATE_BUCKET_NAME=$(TEMPLATE_BUCKET_NAME) CONSUL_TOKEN=$(CONSUL_TOKEN) NODE_ID="testclient" go run cmd/mock-snapshot/mock.go  -template 5wzg6c91u51yaebviysf -build "f0370054-b669-4d7e-b33b-573d5287c6ef" -alive 1 -count 1

.PHONY: test
test:
	go test -v ./...



================================================
File: main.go
================================================
package main

import (
	"context"
	"errors"
	"flag"
	"fmt"
	"log"
	"os"
	"os/signal"
	"sync"
	"sync/atomic"
	"syscall"

	"go.uber.org/zap"
	"go.uber.org/zap/zapcore"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/consul"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/server"
	"github.com/e2b-dev/infra/packages/shared/pkg/env"
	"github.com/e2b-dev/infra/packages/shared/pkg/logger"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/telemetry"
)

const (
	defaultPort = 5008
	ServiceName = "orchestrator"
)

var commitSHA string

func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sig, sigCancel := signal.NotifyContext(ctx, syscall.SIGINT, syscall.SIGTERM)
	defer sigCancel()

	var port uint

	flag.UintVar(&port, "port", defaultPort, "orchestrator server port")
	flag.Parse()

	wg := &sync.WaitGroup{}
	exitCode := &atomic.Int32{}
	telemetrySignal := make(chan struct{})

	// defer waiting on the waitgroup so that this runs even when
	// there's a panic.
	defer wg.Wait()

	if !env.IsLocal() {
		shutdown := telemetry.InitOTLPExporter(ctx, server.ServiceName, "no")
		wg.Add(1)
		go func() {
			defer wg.Done()
			<-telemetrySignal
			if err := shutdown(ctx); err != nil {
				log.Printf("telemetry shutdown: %v", err)
				exitCode.Add(1)
			}
		}()
	}

	logger := zap.Must(logger.NewLogger(ctx, logger.LoggerConfig{
		ServiceName: ServiceName,
		IsInternal:  true,
		IsDebug:     env.IsDebug(),
		Cores:       []zapcore.Core{logger.GetOTELCore(ServiceName)},
	}))
	defer logger.Sync()
	zap.ReplaceGlobals(logger)

	sbxLoggerExternal := sbxlogger.NewLogger(
		ctx,
		sbxlogger.SandboxLoggerConfig{
			ServiceName:      ServiceName,
			IsInternal:       false,
			CollectorAddress: os.Getenv("LOGS_COLLECTOR_ADDRESS"),
		},
	)
	defer sbxLoggerExternal.Sync()
	sbxlogger.SetSandboxLoggerExternal(sbxLoggerExternal)

	sbxLoggerInternal := sbxlogger.NewLogger(
		ctx,
		sbxlogger.SandboxLoggerConfig{
			ServiceName:      ServiceName,
			IsInternal:       true,
			CollectorAddress: os.Getenv("LOGS_COLLECTOR_ADDRESS"),
		},
	)
	defer sbxLoggerInternal.Sync()
	sbxlogger.SetSandboxLoggerInternal(sbxLoggerInternal)

	log.Println("Starting orchestrator", "commit", commitSHA)

	clientID := consul.GetClientID()

	srv, err := server.New(ctx, port, clientID)
	if err != nil {
		zap.L().Fatal("failed to create server", zap.Error(err))
	}

	wg.Add(1)
	go func() {
		defer wg.Done()
		var err error

		defer func() {
			// recover the panic because the service manages a number of go routines
			// that can panic, so catching this here allows for the rest of the process
			// to terminate in a more orderly manner.
			if perr := recover(); perr != nil {
				// many of the panics use log.Panicf which means we're going to log
				// some panic messages twice, but this seems ok, and temporary while
				// we clean up logging.
				log.Printf("caught panic in service: %v", perr)
				exitCode.Add(1)
				err = errors.Join(err, fmt.Errorf("server panic: %v", perr))
			}

			// if we encountered an err, but the signal context was NOT canceled, then
			// the outer context needs to be canceled so the remainder of the service
			// can shutdown.
			if err != nil && sig.Err() == nil {
				log.Printf("service ended early without signal")
				cancel()
			}
		}()

		// this sets the error declared above so the function
		// in the defer can check it.
		if err = srv.Start(ctx); err != nil {
			log.Printf("orchestrator service: %v", err)
			exitCode.Add(1)
		}
	}()

	wg.Add(1)
	go func() {
		defer wg.Done()
		defer close(telemetrySignal)
		<-sig.Done()
		if err := srv.Close(ctx); err != nil {
			log.Printf("grpc service: %v", err)
			exitCode.Add(1)
		}
	}()

	wg.Wait()

	os.Exit(int(exitCode.Load()))
}



================================================
File: orchestrator.proto
================================================
syntax = "proto3";

import "google/protobuf/empty.proto";
import "google/protobuf/timestamp.proto";

option go_package = "https://github.com/e2b-dev/infra/orchestrator";

message SandboxConfig {
  // Data required for creating a new sandbox.
  string template_id = 1;
  string build_id = 2;
  
  string kernel_version = 3;
  string firecracker_version = 4;
  
  bool huge_pages = 5;
  
  string sandbox_id = 6;
  map<string, string> env_vars = 7;
  
  // Metadata about the sandbox.
  map<string, string> metadata = 8;
  optional string alias = 9;
  string envd_version = 10;
  
  int64 vcpu = 11;
  int64 ram_mb = 12;

  string team_id = 13;
  // Maximum length of the sandbox in Hours.
  int64 max_sandbox_length = 14;

  int64 total_disk_size_mb = 15;

  bool snapshot = 16;
  string base_template_id = 17;

  optional bool auto_pause = 18;
}

message SandboxCreateRequest {
  SandboxConfig sandbox = 1;

  google.protobuf.Timestamp start_time = 2;
  google.protobuf.Timestamp end_time = 3;
}

message SandboxCreateResponse {
  string client_id = 1;
}

message SandboxUpdateRequest {
  string sandbox_id = 1;

  google.protobuf.Timestamp end_time = 2;
}

message SandboxDeleteRequest {
  string sandbox_id = 1;
}

message SandboxPauseRequest {
  string sandbox_id = 1;
  string template_id = 2;
  string build_id = 3;
}

message RunningSandbox {
  SandboxConfig config = 1;
  string client_id = 2;

  google.protobuf.Timestamp start_time = 3;
  google.protobuf.Timestamp end_time = 4;
}

message SandboxListResponse {
  repeated RunningSandbox sandboxes = 1;
}

message CachedBuildInfo {
  string build_id = 1;
  google.protobuf.Timestamp expiration_time = 2;
}

message SandboxListCachedBuildsResponse {
  repeated CachedBuildInfo builds = 1;
}



service SandboxService {
  rpc Create(SandboxCreateRequest) returns (SandboxCreateResponse);
  rpc Update(SandboxUpdateRequest) returns (google.protobuf.Empty);
  rpc List(google.protobuf.Empty) returns (SandboxListResponse);
  rpc Delete(SandboxDeleteRequest) returns (google.protobuf.Empty);
  rpc Pause(SandboxPauseRequest) returns (google.protobuf.Empty);

  rpc ListCachedBuilds(google.protobuf.Empty) returns (SandboxListCachedBuildsResponse);
}



================================================
File: upload.sh
================================================
#!/bin/bash

set -euo pipefail

GCP_PROJECT_ID=$1

chmod +x bin/orchestrator

gsutil -h "Cache-Control:no-cache, max-age=0" \
  cp bin/orchestrator "gs://${GCP_PROJECT_ID}-fc-env-pipeline/orchestrator"



================================================
File: cmd/inspect-data/main.go
================================================
package main

import (
	"bytes"
	"context"
	"flag"
	"fmt"
	"log"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
)

func main() {
	buildId := flag.String("build", "", "build id")
	kind := flag.String("kind", "", "'memfile' or 'rootfs'")
	start := flag.Int64("start", 0, "start block")
	end := flag.Int64("end", 0, "end block")

	flag.Parse()

	template := storage.NewTemplateFiles(
		"",
		*buildId,
		"",
		"",
		false,
	)

	var storagePath string
	var blockSize int64

	if *kind == "memfile" {
		storagePath = template.StorageMemfilePath()
		blockSize = 2097152
	} else if *kind == "rootfs" {
		storagePath = template.StorageRootfsPath()
		blockSize = 4096
	} else {
		log.Fatalf("invalid kind: %s", *kind)
	}

	if *end == 0 {
		*end = blockSize
	}

	ctx := context.Background()

	obj := gcs.NewObject(ctx, gcs.GetTemplateBucket(), storagePath)

	size, err := obj.Size()
	if err != nil {
		log.Fatalf("failed to get object size: %s", err)
	}

	if *start > size/blockSize {
		log.Fatalf("start block %d is out of bounds (maximum is %d)", *start, size/blockSize)
	}

	if *end > size/blockSize {
		log.Fatalf("end block %d is out of bounds (maximum is %d)", *end, size/blockSize)
	}

	if *start > *end {
		log.Fatalf("start block %d is greater than end block %d", *start, *end)
	}

	fmt.Printf("\nMETADATA\n")
	fmt.Printf("========\n")
	fmt.Printf("Storage path       %s/%s\n", gcs.GetTemplateBucket().BucketName(), storagePath)
	fmt.Printf("Build ID           %s\n", *buildId)
	fmt.Printf("Size               %d B (%d MiB)\n", size, size/1024/1024)
	fmt.Printf("Block size         %d B\n", blockSize)

	b := make([]byte, blockSize)

	fmt.Printf("\nDATA\n")
	fmt.Printf("====\n")

	emptyCount := 0
	nonEmptyCount := 0

	for i := *start * blockSize; i < *end*blockSize; i += blockSize {
		_, err := obj.ReadAt(b, i)
		if err != nil {
			log.Fatalf("failed to read block: %s", err)
		}

		nonZeroCount := blockSize - int64(bytes.Count(b, []byte("\x00")))

		if nonZeroCount > 0 {
			nonEmptyCount++
			fmt.Printf("%-10d [%11d,%11d) %d non-zero bytes\n", i/blockSize, i, i+blockSize, nonZeroCount)
		} else {
			emptyCount++
			fmt.Printf("%-10d [%11d,%11d) EMPTY\n", i/blockSize, i, i+blockSize)
		}
	}

	fmt.Printf("\nSUMMARY\n")
	fmt.Printf("=======\n")
	fmt.Printf("Empty inspected blocks: %d\n", emptyCount)
	fmt.Printf("Non-empty inspected blocks: %d\n", nonEmptyCount)
	fmt.Printf("Total inspected blocks: %d\n", emptyCount+nonEmptyCount)
	fmt.Printf("Total inspected size: %d B (%d MiB)\n", int64(emptyCount+nonEmptyCount)*blockSize, int64(emptyCount+nonEmptyCount)*blockSize/1024/1024)
	fmt.Printf("Empty inspected size: %d B (%d MiB)\n", int64(emptyCount)*blockSize, int64(emptyCount)*blockSize/1024/1024)
}



================================================
File: cmd/inspect-header/main.go
================================================
package main

import (
	"context"
	"flag"
	"fmt"
	"log"
	"unsafe"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
)

func main() {
	buildId := flag.String("build", "", "build id")
	kind := flag.String("kind", "", "'memfile' or 'rootfs'")

	flag.Parse()

	template := storage.NewTemplateFiles(
		"",
		*buildId,
		"",
		"",
		false,
	)

	var storagePath string

	if *kind == "memfile" {
		storagePath = template.StorageMemfileHeaderPath()
	} else if *kind == "rootfs" {
		storagePath = template.StorageRootfsHeaderPath()
	} else {
		log.Fatalf("invalid kind: %s", *kind)
	}

	ctx := context.Background()

	obj := gcs.NewObject(ctx, gcs.GetTemplateBucket(), storagePath)

	h, err := header.Deserialize(obj)
	if err != nil {
		log.Fatalf("failed to deserialize header: %s", err)
	}

	fmt.Printf("\nMETADATA\n")
	fmt.Printf("========\n")
	fmt.Printf("Storage path       %s/%s\n", gcs.GetTemplateBucket().BucketName(), storagePath)
	fmt.Printf("Version            %d\n", h.Metadata.Version)
	fmt.Printf("Generation         %d\n", h.Metadata.Generation)
	fmt.Printf("Build ID           %s\n", h.Metadata.BuildId)
	fmt.Printf("Base build ID      %s\n", h.Metadata.BaseBuildId)
	fmt.Printf("Size               %d B (%d MiB)\n", h.Metadata.Size, h.Metadata.Size/1024/1024)
	fmt.Printf("Block size         %d B\n", h.Metadata.BlockSize)
	fmt.Printf("Blocks             %d\n", (h.Metadata.Size+h.Metadata.BlockSize-1)/h.Metadata.BlockSize)

	totalSize := int64(unsafe.Sizeof(header.BuildMap{})) * int64(len(h.Mapping)) / 1024
	var sizeMessage string

	if totalSize == 0 {
		sizeMessage = "<1 KiB"
	} else {
		sizeMessage = fmt.Sprintf("%d KiB", totalSize)
	}

	fmt.Printf("\nMAPPING (%d maps, uses %s in storage)\n", len(h.Mapping), sizeMessage)
	fmt.Printf("=======\n")

	for _, mapping := range h.Mapping {
		fmt.Println(mapping.Format(h.Metadata.BlockSize))
	}

	fmt.Printf("\nMAPPING SUMMARY\n")
	fmt.Printf("===============\n")

	builds := make(map[string]int64)

	for _, mapping := range h.Mapping {
		builds[mapping.BuildId.String()] += int64(mapping.Length)
	}

	for build, size := range builds {
		var additionalInfo string

		if build == h.Metadata.BuildId.String() {
			additionalInfo = " (current)"
		} else if build == h.Metadata.BaseBuildId.String() {
			additionalInfo = " (base)"
		}

		fmt.Printf("%s%s: %d blocks, %d MiB (%0.2f%%)\n", build, additionalInfo, uint64(size)/h.Metadata.BlockSize, uint64(size)/1024/1024, float64(size)/float64(h.Metadata.Size)*100)
	}
}



================================================
File: cmd/mock-nbd/mock.go
================================================
package main

import (
	"bytes"
	"context"
	"crypto/rand"
	"fmt"
	"os"
	"os/signal"

	"github.com/pojntfx/go-nbd/pkg/backend"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/nbd"
)

const blockSize = 4096

type DeviceWithClose struct {
	backend.Backend
}

func (d *DeviceWithClose) Close() error {
	return nil
}

func (d *DeviceWithClose) Slice(offset, length int64) ([]byte, error) {
	b := make([]byte, length)

	_, err := d.Backend.ReadAt(b, offset)
	if err != nil {
		return nil, err
	}

	return b, nil
}

func main() {
	data := make([]byte, blockSize*8)
	rand.Read(data)

	device := &DeviceWithClose{
		Backend: backend.NewMemoryBackend(data),
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	done := make(chan os.Signal, 1)
	signal.Notify(done, os.Interrupt)
	devicePool, err := nbd.NewDevicePool()
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create device pool: %v\n", err)

		return
	}

	go func() {
		<-done

		cancel()
	}()

	for i := 0; ; i++ {
		select {
		case <-ctx.Done():
			return
		default:
		}
		fmt.Printf("----------------------------------------\n")
		fmt.Printf("[%d] starting mock nbd server\n", i)

		readData, err := MockNbd(ctx, device, i, devicePool)
		if err != nil {
			fmt.Fprintf(os.Stderr, "[%d] failed to mock nbd: %v\n", i, err)

			return
		}

		if !bytes.Equal(data, readData) {
			fmt.Fprintf(os.Stderr, "[%d] data mismatch\n", i)

			return
		}
	}
}

func MockNbd(ctx context.Context, device *DeviceWithClose, index int, devicePool *nbd.DevicePool) ([]byte, error) {
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	size, err := device.Size()
	if err != nil {
		return nil, fmt.Errorf("failed to get size: %w", err)
	}

	deviceIndex, err := devicePool.GetDevice(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get device: %w", err)
	}

	var mnt *nbd.DirectPathMount

	defer func() {
		counter := 0

		for {
			counter += 1
			err = devicePool.ReleaseDevice(deviceIndex)
			if err != nil {
				if counter%10 == 0 {
					fmt.Printf("[%d] failed to release device: %v\n", index, err)
				}

				if mnt != nil {
					mnt.Close()
				}

				continue
			}

			fmt.Printf("[%d] released device: %d\n", index, deviceIndex)

			return
		}
	}()

	mnt = nbd.NewDirectPathMount(device, devicePool)

	go func() {
		<-ctx.Done()

		mnt.Close()
	}()

	_, err = mnt.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to open: %w", err)
	}

	data := make([]byte, size)
	_, err = mnt.Backend.ReadAt(data, 0)
	if err != nil {
		return nil, fmt.Errorf("failed to read: %w", err)
	}

	fmt.Printf("[%d] Read %d bytes from nbd\n", index, len(data))

	cancel()

	return data, nil
}



================================================
File: cmd/mock-sandbox/mock.go
================================================
package main

import (
	"context"
	"flag"
	"fmt"
	"log"
	"os"
	"os/signal"
	"strconv"
	"time"

	"go.opentelemetry.io/otel"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/dns"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/nbd"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/network"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
	"github.com/e2b-dev/infra/packages/shared/pkg/chdb"
	"github.com/e2b-dev/infra/packages/shared/pkg/grpc/orchestrator"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
)

func main() {
	templateId := flag.String("template", "", "template id")
	buildId := flag.String("build", "", "build id")
	sandboxId := flag.String("sandbox", "", "sandbox id")
	keepAlive := flag.Int("alive", 0, "keep alive")
	count := flag.Int("count", 1, "number of serially spawned sandboxes")

	devicePool, err := nbd.NewDevicePool()
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create device pool: %v\n", err)

		return
	}

	flag.Parse()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	done := make(chan os.Signal, 1)
	signal.Notify(done, os.Interrupt)

	go func() {
		<-done

		cancel()
	}()

	dnsServer := dns.New()
	go func() {
		log.Printf("Starting DNS server")

		err := dnsServer.Start("127.0.0.4", 53)
		if err != nil {
			log.Fatalf("Failed running DNS server: %s\n", err.Error())
		}
	}()

	templateCache, err := template.NewCache(ctx)
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create template cache: %v\n", err)

		return
	}

	networkPool, err := network.NewPool(ctx, *count, 0, "mock-node")
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create network pool: %v\n", err)

		return
	}
	defer networkPool.Close()

	for i := 0; i < *count; i++ {
		fmt.Println("--------------------------------")
		fmt.Printf("Starting sandbox %d\n", i)

		v := i

		err = mockSandbox(
			ctx,
			*templateId,
			*buildId,
			*sandboxId+"-"+strconv.Itoa(v),
			dnsServer,
			time.Duration(*keepAlive)*time.Second,
			networkPool,
			templateCache,
			devicePool,
		)
		if err != nil {
			break
		}
	}
}

func mockSandbox(
	ctx context.Context,
	templateId,
	buildId,
	sandboxId string,
	dns *dns.DNS,
	keepAlive time.Duration,
	networkPool *network.Pool,
	templateCache *template.Cache,
	devicePool *nbd.DevicePool,
) error {
	tracer := otel.Tracer(fmt.Sprintf("sandbox-%s", sandboxId))
	childCtx, _ := tracer.Start(ctx, "mock-sandbox")

	start := time.Now()

	loggerCfg := sbxlogger.SandboxLoggerConfig{
		ServiceName:      "mock-sandbox",
		IsInternal:       true,
		CollectorAddress: "http://localhost:8080",
	}
	sbxlogger.SetSandboxLoggerInternal(sbxlogger.NewLogger(ctx, loggerCfg))
	sbxlogger.SetSandboxLoggerExternal(sbxlogger.NewLogger(ctx, loggerCfg))

	mockStore := chdb.NewMockStore()

	sbx, cleanup, err := sandbox.NewSandbox(
		childCtx,
		tracer,
		dns,
		networkPool,
		templateCache,
		&orchestrator.SandboxConfig{
			TemplateId: templateId,
			// FirecrackerVersion: "v1.10.1_1fcdaec",
			// KernelVersion:      "vmlinux-6.1.102",
			FirecrackerVersion: "v1.7.0-dev_8bb88311",
			KernelVersion:      "vmlinux-5.10.186",
			TeamId:             "test-team",
			BuildId:            buildId,
			HugePages:          true,
			MaxSandboxLength:   1,
			SandboxId:          sandboxId,
			EnvdVersion:        "0.1.1",
			RamMb:              512,
			Vcpu:               2,
		},
		"trace-test-1",
		time.Now(),
		time.Now(),
		true,
		templateId,
		"testclient",
		devicePool,
		mockStore,
		"true",
		"true",
	)
	defer func() {
		cleanupErr := cleanup.Run()
		if cleanupErr != nil {
			fmt.Fprintf(os.Stderr, "failed to cleanup sandbox: %v\n", cleanupErr)
		}
	}()

	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create sandbox: %v\n", err)

		return err
	}

	duration := time.Since(start)

	fmt.Printf("[Sandbox is running] - started in %dms \n", duration.Milliseconds())

	time.Sleep(keepAlive)

	err = sbx.Stop()
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to stop sandbox: %v\n", err)

		return err
	}

	return nil
}



================================================
File: cmd/mock-snapshot/mock.go
================================================
package main

import (
	"context"
	"flag"
	"fmt"
	"log"
	"os"
	"os/signal"
	"strconv"
	"time"

	"go.opentelemetry.io/otel"
	"golang.org/x/sync/errgroup"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/dns"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/nbd"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/network"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
	"github.com/e2b-dev/infra/packages/shared/pkg/chdb"
	"github.com/e2b-dev/infra/packages/shared/pkg/grpc/orchestrator"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
)

func main() {
	templateId := flag.String("template", "", "template id")
	buildId := flag.String("build", "", "build id")
	sandboxId := flag.String("sandbox", "", "sandbox id")
	keepAlive := flag.Int("alive", 0, "keep alive")
	count := flag.Int("count", 1, "number of serially spawned sandboxes")

	flag.Parse()

	devicePool, err := nbd.NewDevicePool()
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create device pool: %v\n", err)
		return
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	done := make(chan os.Signal, 1)
	signal.Notify(done, os.Interrupt)

	go func() {
		<-done

		cancel()
	}()

	dnsServer := dns.New()
	go func() {
		log.Printf("Starting DNS server")

		err := dnsServer.Start("127.0.0.4", 53)
		if err != nil {
			log.Fatalf("Failed running DNS server: %s\n", err.Error())
		}
	}()

	templateCache, err := template.NewCache(ctx)
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create template cache: %v\n", err)

		return
	}

	networkPool, err := network.NewPool(ctx, *count, 0, "mock-node")
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create network pool: %v\n", err)

		return
	}
	defer networkPool.Close()

	eg, ctx := errgroup.WithContext(ctx)

	for i := 0; i < *count; i++ {
		fmt.Println("--------------------------------")
		fmt.Printf("Starting sandbox %d\n", i)

		v := i

		err = mockSnapshot(
			ctx,
			*templateId,
			*buildId,
			*sandboxId+"-"+strconv.Itoa(v),
			dnsServer,
			time.Duration(*keepAlive)*time.Second,
			networkPool,
			templateCache,
			devicePool,
		)
		if err != nil {
			fmt.Fprintf(os.Stderr, "failed to start sandbox: %v\n", err)
			return
		}
	}

	err = eg.Wait()
	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to start sandboxes: %v\n", err)
	}
}

func mockSnapshot(
	ctx context.Context,
	templateId,
	buildId,
	sandboxId string,
	dns *dns.DNS,
	keepAlive time.Duration,
	networkPool *network.Pool,
	templateCache *template.Cache,
	devicePool *nbd.DevicePool,
) error {
	tracer := otel.Tracer(fmt.Sprintf("sandbox-%s", sandboxId))
	childCtx, _ := tracer.Start(ctx, "mock-sandbox")

	loggerCfg := sbxlogger.SandboxLoggerConfig{
		ServiceName:      "mock-snapshot",
		IsInternal:       true,
		CollectorAddress: "http://localhost:8080",
	}
	sbxlogger.SetSandboxLoggerInternal(sbxlogger.NewLogger(ctx, loggerCfg))
	sbxlogger.SetSandboxLoggerExternal(sbxlogger.NewLogger(ctx, loggerCfg))

	mockStore := chdb.NewMockStore()

	start := time.Now()

	sbx, cleanup, err := sandbox.NewSandbox(
		childCtx,
		tracer,
		dns,
		networkPool,
		templateCache,
		&orchestrator.SandboxConfig{
			TemplateId:         templateId,
			FirecrackerVersion: "v1.7.0-dev_8bb88311",
			KernelVersion:      "vmlinux-5.10.186",
			TeamId:             "test-team",
			BuildId:            buildId,
			HugePages:          true,
			MaxSandboxLength:   1,
			SandboxId:          sandboxId,
			EnvdVersion:        "0.1.1",
			RamMb:              512,
			Vcpu:               2,
		},
		"trace-test-1",
		time.Now(),
		time.Now(),
		false,
		templateId,
		"testclient",
		devicePool,
		mockStore,
		"true",
		"true",
	)
	defer func() {
		cleanupErr := cleanup.Run()
		if cleanupErr != nil {
			fmt.Fprintf(os.Stderr, "failed to cleanup sandbox: %v\n", cleanupErr)
		}
	}()

	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create sandbox: %v\n", err)

		return err
	}

	duration := time.Since(start)

	fmt.Printf("[Sandbox is running] - started in %dms \n", duration.Milliseconds())

	time.Sleep(keepAlive)

	fmt.Println("Snapshotting sandbox")

	snapshotTime := time.Now()

	snapshotTemplateFiles, err := storage.NewTemplateFiles(
		"snapshot-template",
		"f0370054-b669-eee4-b33b-573d5287c6ef",
		sbx.Config.KernelVersion,
		sbx.Config.FirecrackerVersion,
		sbx.Config.HugePages,
	).NewTemplateCacheFiles()
	if err != nil {
		return fmt.Errorf("failed to create snapshot template files: %w", err)
	}

	err = os.MkdirAll(snapshotTemplateFiles.CacheDir(), 0o755)
	if err != nil {
		return fmt.Errorf("failed to create snapshot template files directory: %w", err)
	}

	defer func() {
		err := os.RemoveAll(snapshotTemplateFiles.CacheDir())
		if err != nil {
			fmt.Fprintf(os.Stderr, "error removing sandbox cache dir '%s': %v\n", snapshotTemplateFiles.CacheDir(), err)
		}
	}()

	fmt.Println("Snapshotting sandbox")

	snapshot, err := sbx.Snapshot(ctx, otel.Tracer("orchestrator-mock"), snapshotTemplateFiles, func() {})
	if err != nil {
		return fmt.Errorf("failed to snapshot sandbox: %w", err)
	}

	fmt.Println("Create snapshot time: ", time.Since(snapshotTime).Milliseconds())

	err = templateCache.AddSnapshot(
		snapshotTemplateFiles.TemplateId,
		snapshotTemplateFiles.BuildId,
		snapshotTemplateFiles.KernelVersion,
		snapshotTemplateFiles.FirecrackerVersion,
		snapshotTemplateFiles.Hugepages(),
		snapshot.MemfileDiffHeader,
		snapshot.RootfsDiffHeader,
		snapshot.Snapfile,
		snapshot.MemfileDiff,
		snapshot.RootfsDiff,
	)
	if err != nil {
		return fmt.Errorf("failed to add snapshot to template cache: %w", err)
	}

	fmt.Println("Add snapshot to template cache time: ", time.Since(snapshotTime).Milliseconds())

	start = time.Now()

	sbx, cleanup2, err := sandbox.NewSandbox(
		childCtx,
		tracer,
		dns,
		networkPool,
		templateCache,
		&orchestrator.SandboxConfig{
			TemplateId:         snapshotTemplateFiles.TemplateId,
			FirecrackerVersion: snapshotTemplateFiles.FirecrackerVersion,
			KernelVersion:      snapshotTemplateFiles.KernelVersion,
			TeamId:             "test-team",
			BuildId:            snapshotTemplateFiles.BuildId,
			HugePages:          snapshotTemplateFiles.Hugepages(),
			MaxSandboxLength:   1,
			SandboxId:          sandboxId,
			EnvdVersion:        "0.1.1",
			RamMb:              512,
			Vcpu:               2,
		},
		"trace-test-1",
		time.Now(),
		time.Now(),
		false,
		templateId,
		"testclient",
		devicePool,
		mockStore,
		"true",
		"true",
	)
	defer func() {
		cleanupErr := cleanup2.Run()
		if cleanupErr != nil {
			fmt.Fprintf(os.Stderr, "failed to cleanup sandbox: %v\n", cleanupErr)
		}
	}()

	if err != nil {
		fmt.Fprintf(os.Stderr, "failed to create sandbox: %v\n", err)

		return err
	}

	duration = time.Since(start)

	fmt.Printf("[Resumed sandbox is running] - started in %dms \n", duration.Milliseconds())

	time.Sleep(keepAlive)

	// b := storage.NewTemplateBuild(
	// 	snapshot.MemfileDiffHeader,
	// 	snapshot.RootfsDiffHeader,
	// 	snapshotTemplateFiles.TemplateFiles,
	// )

	// err = <-b.Upload(
	// 	ctx,
	// 	snapshotTemplateFiles.CacheSnapfilePath(),
	// 	snapshotTemplateFiles.CacheMemfilePath(),
	// 	snapshotTemplateFiles.CacheRootfsPath(),
	// )
	// if err != nil {
	// 	return fmt.Errorf("failed to upload snapshot template files: %w", err)
	// }

	fmt.Println("Upload snapshot time: ", time.Since(snapshotTime).Milliseconds())

	duration = time.Since(snapshotTime)

	return nil
}



================================================
File: cmd/simulate-headers-merge/main.go
================================================
package main

import (
	"context"
	"flag"
	"fmt"
	"log"
	"os"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
	"github.com/google/uuid"
)

func main() {
	baseBuildId := flag.String("base", "", "base build id")
	diffBuildId := flag.String("diff", "", "diff build id")
	kind := flag.String("kind", "", "'memfile' or 'rootfs'")
	visualize := flag.Bool("visualize", false, "visualize the headers")

	flag.Parse()

	baseTemplate := storage.NewTemplateFiles(
		"",
		*baseBuildId,
		"",
		"",
		false,
	)

	diffTemplate := storage.NewTemplateFiles(
		"",
		*diffBuildId,
		"",
		"",
		false,
	)

	var baseStoragePath string
	var diffStoragePath string

	if *kind == "memfile" {
		baseStoragePath = baseTemplate.StorageMemfileHeaderPath()
		diffStoragePath = diffTemplate.StorageMemfileHeaderPath()
	} else if *kind == "rootfs" {
		baseStoragePath = baseTemplate.StorageRootfsHeaderPath()
		diffStoragePath = diffTemplate.StorageRootfsHeaderPath()
	} else {
		log.Fatalf("invalid kind: %s", *kind)
	}

	ctx := context.Background()

	baseObj := gcs.NewObject(ctx, gcs.GetTemplateBucket(), baseStoragePath)
	diffObj := gcs.NewObject(ctx, gcs.GetTemplateBucket(), diffStoragePath)

	baseHeader, err := header.Deserialize(baseObj)
	if err != nil {
		log.Fatalf("failed to deserialize base header: %s", err)
	}

	diffHeader, err := header.Deserialize(diffObj)
	if err != nil {
		log.Fatalf("failed to deserialize diff header: %s", err)
	}

	fmt.Printf("\nBASE METADATA\n")
	fmt.Printf("Storage path       %s/%s\n", gcs.GetTemplateBucket().BucketName(), baseStoragePath)
	fmt.Printf("========\n")

	for _, mapping := range baseHeader.Mapping {
		fmt.Println(mapping.Format(baseHeader.Metadata.BlockSize))
	}

	if *visualize {
		bottomLayers := header.Layers(baseHeader.Mapping)
		delete(*bottomLayers, baseHeader.Metadata.BaseBuildId)

		fmt.Println("")
		fmt.Println(
			header.Visualize(
				baseHeader.Mapping,
				baseHeader.Metadata.Size,
				baseHeader.Metadata.BlockSize,
				128,
				bottomLayers,
				&map[uuid.UUID]struct{}{
					baseHeader.Metadata.BuildId: {},
				},
			),
		)
	}

	if err := header.ValidateMappings(baseHeader.Mapping, baseHeader.Metadata.Size, baseHeader.Metadata.BlockSize); err != nil {
		log.Fatalf("failed to validate base header: %s", err)
	}

	fmt.Printf("\nDIFF METADATA\n")
	fmt.Printf("Storage path       %s/%s\n", gcs.GetTemplateBucket().BucketName(), diffStoragePath)
	fmt.Printf("========\n")

	onlyDiffMappings := make([]*header.BuildMap, 0)

	for _, mapping := range diffHeader.Mapping {
		if mapping.BuildId == diffHeader.Metadata.BuildId {
			onlyDiffMappings = append(onlyDiffMappings, mapping)
		}
	}

	for _, mapping := range onlyDiffMappings {
		fmt.Println(mapping.Format(baseHeader.Metadata.BlockSize))
	}

	if *visualize {
		fmt.Println("")
		fmt.Println(
			header.Visualize(
				onlyDiffMappings,
				baseHeader.Metadata.Size,
				baseHeader.Metadata.BlockSize,
				128,
				nil,
				header.Layers(onlyDiffMappings),
			),
		)
	}

	mergedHeader := header.MergeMappings(baseHeader.Mapping, onlyDiffMappings)

	fmt.Printf("\n\nMERGED METADATA\n")
	fmt.Printf("========\n")

	for _, mapping := range mergedHeader {
		fmt.Println(mapping.Format(baseHeader.Metadata.BlockSize))
	}

	if *visualize {
		bottomLayers := header.Layers(baseHeader.Mapping)
		delete(*bottomLayers, baseHeader.Metadata.BaseBuildId)

		fmt.Println("")
		fmt.Println(
			header.Visualize(
				mergedHeader,
				baseHeader.Metadata.Size,
				baseHeader.Metadata.BlockSize,
				128,
				bottomLayers,
				header.Layers(onlyDiffMappings),
			),
		)
	}

	if err := header.ValidateMappings(mergedHeader, baseHeader.Metadata.Size, baseHeader.Metadata.BlockSize); err != nil {
		fmt.Fprintf(os.Stderr, "\n\n[VALIDATION ERROR]: failed to validate merged header: %s", err)
	}
}



================================================
File: internal/consul/node.go
================================================
package consul

import (
	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

const shortNodeIDLength = 8

func GetClientID() string {
	nodeID := utils.RequiredEnv("NODE_ID", "Nomad ID of the instance node")

	return nodeID[:shortNodeIDLength]
}



================================================
File: internal/dns/server.go
================================================
package dns

import (
	"context"
	"fmt"
	"net"
	"strings"
	"sync"

	resolver "github.com/miekg/dns"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/shared/pkg/smap"
)

const ttl = 0

type DNS struct {
	records *smap.Map[string]

	closer struct {
		once sync.Once
		op   func(context.Context) error
		err  error
	}
}

func New() *DNS {
	return &DNS{
		records: smap.New[string](),
	}
}

func (d *DNS) Add(sandboxID, ip string) {
	d.records.Insert(d.hostname(sandboxID), ip)
}

func (d *DNS) Remove(sandboxID, ip string) {
	d.records.RemoveCb(d.hostname(sandboxID), func(key string, v string, exists bool) bool {
		return v == ip
	})
}

func (d *DNS) get(hostname string) (string, bool) {
	return d.records.Get(hostname)
}

func (*DNS) hostname(sandboxID string) string {
	return fmt.Sprintf("%s.", sandboxID)
}

func (d *DNS) handleDNSRequest(w resolver.ResponseWriter, r *resolver.Msg) {
	m := new(resolver.Msg)
	m.SetReply(r)
	m.Compress = false
	m.Authoritative = true

	for _, q := range m.Question {
		if q.Qtype == resolver.TypeA {
			sandboxID := strings.Split(q.Name, "-")[0]
			ip, found := d.get(sandboxID)
			if found {
				a := &resolver.A{
					Hdr: resolver.RR_Header{
						Name:   q.Name,
						Rrtype: resolver.TypeA,
						Class:  resolver.ClassINET,
						Ttl:    ttl,
					},
					A: net.ParseIP(ip).To4(),
				}

				m.Answer = append(m.Answer, a)
			}
		}
	}

	err := w.WriteMsg(m)
	if err != nil {
		zap.L().Error("failed to write message", zap.Error(err))
	}
}

func (d *DNS) Start(address string, port int) error {
	mux := resolver.NewServeMux()

	mux.HandleFunc(".", d.handleDNSRequest)

	server := resolver.Server{Addr: fmt.Sprintf("%s:%d", address, port), Net: "udp", Handler: mux}

	if err := server.ListenAndServe(); err != nil {
		return fmt.Errorf("DNS server encounterted error: %w", err)
	}

	d.closer.op = server.ShutdownContext

	return nil
}

func (d *DNS) Close(ctx context.Context) error {
	d.closer.once.Do(func() { d.closer.err = d.closer.op(ctx) })
	return d.closer.err
}



================================================
File: internal/sandbox/checks.go
================================================
package sandbox

import (
	"context"
	"fmt"
	"io"
	"net/http"
	"time"

	"go.uber.org/zap"

	"golang.org/x/mod/semver"

	"github.com/e2b-dev/infra/packages/shared/pkg/consts"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

const (
	healthCheckInterval      = 20 * time.Second
	metricsCheckInterval     = 10 * time.Second
	minEnvdVersionForMetrcis = "0.1.5"
)

type metricStore interface {
	LogMetrics(ctx context.Context)
	SendMetrics(ctx context.Context)
}

func (s *Sandbox) logMetricsBasedOnConfig(ctx context.Context, logger metricStore) {
	if s.useLokiMetrics == "true" {
		logger.LogMetrics(ctx)
	}
	if s.useClickhouseMetrics == "true" {
		logger.SendMetrics(ctx)
	}
	if !(s.useClickhouseMetrics == "true") && !(s.useLokiMetrics == "true") { // ensure backward compatibility if neither are set
		logger.LogMetrics(ctx)
	}
}

func (s *Sandbox) logHeathAndUsage(ctx *utils.LockableCancelableContext) {
	healthTicker := time.NewTicker(healthCheckInterval)
	metricsTicker := time.NewTicker(metricsCheckInterval)
	defer func() {
		healthTicker.Stop()
		metricsTicker.Stop()
	}()

	// Get metrics and health status on sandbox startup

	go s.logMetricsBasedOnConfig(ctx, s)
	go s.Healthcheck(ctx, false)

	for {
		select {
		case <-healthTicker.C:
			childCtx, cancel := context.WithTimeout(ctx, time.Second)

			ctx.Lock()
			s.Healthcheck(childCtx, false)
			ctx.Unlock()

			cancel()
		case <-metricsTicker.C:
			go s.logMetricsBasedOnConfig(ctx, s)
		case <-ctx.Done():
			return
		}
	}
}

func (s *Sandbox) Healthcheck(ctx context.Context, alwaysReport bool) {
	var err error
	defer func() {
		ok := err == nil

		if !ok && s.healthy.CompareAndSwap(true, false) {
			sbxlogger.E(s).Healthcheck(sbxlogger.Fail)
			sbxlogger.I(s).Error("healthcheck failed", zap.Error(err))
			return
		}

		if ok && s.healthy.CompareAndSwap(false, true) {
			sbxlogger.E(s).Healthcheck(sbxlogger.Success)
			return
		}

		if alwaysReport {
			if ok {
				sbxlogger.E(s).Healthcheck(sbxlogger.ReportSuccess)
			} else {
				sbxlogger.E(s).Healthcheck(sbxlogger.ReportFail)
				sbxlogger.I(s).Error("control healthcheck failed", zap.Error(err))
			}
		}
	}()

	address := fmt.Sprintf("http://%s:%d/health", s.Slot.HostIP(), consts.DefaultEnvdServerPort)

	request, err := http.NewRequestWithContext(ctx, "GET", address, nil)
	if err != nil {
		return
	}

	response, err := httpClient.Do(request)
	if err != nil {
		return
	}
	defer func() {
		// Drain the response body to reuse the connection
		// From response.Body docstring:
		//  // The default HTTP client's Transport may not reuse HTTP/1.x "keep-alive" TCP connections
		//  if the Body is not read to completion and closed.
		io.Copy(io.Discard, response.Body)
		response.Body.Close()
	}()

	if response.StatusCode != http.StatusNoContent {
		err = fmt.Errorf("unexpected status code: %d", response.StatusCode)
		return
	}

}

func isGTEVersion(curVersion, minVersion string) bool {
	if len(curVersion) > 0 && curVersion[0] != 'v' {
		curVersion = "v" + curVersion
	}

	if !semver.IsValid(curVersion) {
		return false
	}

	return semver.Compare(curVersion, minVersion) >= 0
}



================================================
File: internal/sandbox/checks_test.go
================================================
package sandbox

import (
	"context"
	"reflect"
	"testing"

	"github.com/e2b-dev/infra/packages/shared/pkg/chdb"
)

type fakeMetricStore struct {
	calls []string
}

func (l *fakeMetricStore) LogMetrics(ctx context.Context) {
	l.calls = append(l.calls, "LogMetrics")
}

func (l *fakeMetricStore) SendMetrics(ctx context.Context) {
	l.calls = append(l.calls, "SendMetrics")
}

func TestSandbox_logMetricsBasedOnConfig(t *testing.T) {
	type fields struct {
		ClickhouseStore      chdb.Store
		useLokiMetrics       string
		useClickhouseMetrics string
	}
	type args struct {
		ctx    context.Context
		logger *fakeMetricStore
		want   []string
	}
	tests := []struct {
		name   string
		fields fields
		args   args
	}{
		// cover all the cases
		{
			name: "should call LogMetrics if useLokiMetrics is true and useClickhouseMetrics is not set",
			fields: fields{
				useLokiMetrics: "true",
			},
			args: args{
				logger: &fakeMetricStore{},
				want:   []string{"LogMetrics"},
			},
		},
		{
			name: "should call SendMetrics if useClickhouseMetrics is true and useLokiMetrics is not set",
			fields: fields{
				useClickhouseMetrics: "true",
			},
			args: args{
				logger: &fakeMetricStore{},
				want:   []string{"SendMetrics"},
			},
		},
		{
			name: "should call LogMetrics neither are set",
			fields: fields{
				useLokiMetrics:       "",
				useClickhouseMetrics: "",
			},
			args: args{
				logger: &fakeMetricStore{},
				want:   []string{"LogMetrics"},
			},
		},
		{
			name: "should call both if both are set",
			fields: fields{
				useLokiMetrics:       "true",
				useClickhouseMetrics: "true",
			},
			args: args{
				logger: &fakeMetricStore{},
				want:   []string{"LogMetrics", "SendMetrics"},
			},
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			s := &Sandbox{
				ClickhouseStore:      tt.fields.ClickhouseStore,
				useLokiMetrics:       tt.fields.useLokiMetrics,
				useClickhouseMetrics: tt.fields.useClickhouseMetrics,
			}
			s.logMetricsBasedOnConfig(tt.args.ctx, tt.args.logger)
			if !reflect.DeepEqual(tt.args.logger.calls, tt.args.want) {
				t.Errorf("Sandbox.logMetricsBasedOnConfig() = %v, want %v", tt.args.logger.calls, tt.args.want)
			}
		})
	}
}



================================================
File: internal/sandbox/cleanup.go
================================================
package sandbox

import (
	"errors"
	"fmt"
	"os"
	"sync"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
)

type Cleanup struct {
	cleanup         []func() error
	priorityCleanup []func() error
	error           error
	once            sync.Once
}

func NewCleanup() *Cleanup {
	return &Cleanup{}
}

func (c *Cleanup) Add(f func() error) {
	c.cleanup = append(c.cleanup, f)
}

func (c *Cleanup) AddPriority(f func() error) {
	c.priorityCleanup = append(c.priorityCleanup, f)
}

func (c *Cleanup) Run() error {
	c.once.Do(c.run)
	return c.error
}

func (c *Cleanup) run() {
	var errs []error

	for i := len(c.priorityCleanup) - 1; i >= 0; i-- {
		err := c.priorityCleanup[i]()
		if err != nil {
			errs = append(errs, err)
		}
	}

	for i := len(c.cleanup) - 1; i >= 0; i-- {
		err := c.cleanup[i]()
		if err != nil {
			errs = append(errs, err)
		}
	}

	c.error = errors.Join(errs...)
}

func cleanupFiles(files *storage.SandboxFiles) error {
	var errs []error

	for _, p := range []string{
		files.SandboxFirecrackerSocketPath(),
		files.SandboxUffdSocketPath(),
		files.SandboxCacheRootfsLinkPath(),
	} {
		err := os.RemoveAll(p)
		if err != nil {
			errs = append(errs, fmt.Errorf("failed to delete '%s': %w", p, err))
		}
	}

	return errors.Join(errs...)
}



================================================
File: internal/sandbox/envd.go
================================================
package sandbox

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"time"

	"go.opentelemetry.io/otel/trace"

	"github.com/e2b-dev/infra/packages/shared/pkg/consts"
)

const (
	requestTimeout = 50 * time.Millisecond
	loopDelay      = 5 * time.Millisecond
)

// doRequestWithInfiniteRetries does a request with infinite retries until the context is done.
// The parent context should have a deadline or a timeout.
func doRequestWithInfiniteRetries(ctx context.Context, method, address string, requestBody []byte) (*http.Response, error) {
	for {
		reqCtx, cancel := context.WithTimeout(ctx, requestTimeout)
		request, err := http.NewRequestWithContext(reqCtx, method, address, bytes.NewReader(requestBody))

		if err != nil {
			cancel()
			return nil, err
		}

		response, err := httpClient.Do(request)
		cancel()

		if err == nil {
			return response, nil
		}

		select {
		case <-ctx.Done():
			return nil, fmt.Errorf("%w with cause: %w", ctx.Err(), context.Cause(ctx))
		case <-time.After(loopDelay):
		}
	}
}

func (s *Sandbox) syncOldEnvd(ctx context.Context) error {
	address := fmt.Sprintf("http://%s:%d/sync", s.Slot.HostIP(), consts.OldEnvdServerPort)

	response, err := doRequestWithInfiniteRetries(ctx, "POST", address, nil)
	if err != nil {
		return fmt.Errorf("failed to sync envd: %w", err)
	}

	_, err = io.Copy(io.Discard, response.Body)
	if err != nil {
		return err
	}

	err = response.Body.Close()
	if err != nil {
		return err
	}

	return nil
}

type PostInitJSONBody struct {
	EnvVars *map[string]string `json:"envVars"`
}

func (s *Sandbox) initEnvd(ctx context.Context, tracer trace.Tracer, envVars map[string]string) error {
	childCtx, childSpan := tracer.Start(ctx, "envd-init")
	defer childSpan.End()

	address := fmt.Sprintf("http://%s:%d/init", s.Slot.HostIP(), consts.DefaultEnvdServerPort)

	jsonBody := &PostInitJSONBody{
		EnvVars: &envVars,
	}

	envVarsJSON, err := json.Marshal(jsonBody)
	if err != nil {
		return err
	}

	response, err := doRequestWithInfiniteRetries(childCtx, "POST", address, envVarsJSON)
	if err != nil {
		return fmt.Errorf("failed to init envd: %w", err)
	}

	defer response.Body.Close()
	if response.StatusCode != http.StatusNoContent {
		return fmt.Errorf("unexpected status code: %d", response.StatusCode)
	}

	_, err = io.Copy(io.Discard, response.Body)
	if err != nil {
		return err
	}

	return nil
}



================================================
File: internal/sandbox/metrics.go
================================================
package sandbox

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"time"

	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/shared/pkg/consts"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/models/chmodels"
)

type SandboxMetrics struct {
	Timestamp      int64   `json:"ts"`            // Unix Timestamp in UTC
	CPUCount       uint32  `json:"cpu_count"`     // Total CPU cores
	CPUUsedPercent float32 `json:"cpu_used_pct"`  // Percent rounded to 2 decimal places
	MemTotalMiB    uint64  `json:"mem_total_mib"` // Total virtual memory in MiB
	MemUsedMiB     uint64  `json:"mem_used_mib"`  // Used virtual memory in MiB
}

func (s *Sandbox) GetMetrics(ctx context.Context) (SandboxMetrics, error) {
	address := fmt.Sprintf("http://%s:%d/metrics", s.Slot.HostIP(), consts.DefaultEnvdServerPort)

	request, err := http.NewRequestWithContext(ctx, "GET", address, nil)
	if err != nil {
		return SandboxMetrics{}, err
	}

	response, err := httpClient.Do(request)
	if err != nil {
		return SandboxMetrics{}, err
	}
	defer response.Body.Close()

	if response.StatusCode != http.StatusOK {
		err = fmt.Errorf("unexpected status code: %d", response.StatusCode)
		return SandboxMetrics{}, err
	}

	var metrics SandboxMetrics
	err = json.NewDecoder(response.Body).Decode(&metrics)
	if err != nil {
		return SandboxMetrics{}, err
	}

	return metrics, nil
}

func (s *Sandbox) LogMetrics(ctx context.Context) {
	if isGTEVersion(s.Config.EnvdVersion, minEnvdVersionForMetrcis) {
		metrics, err := s.GetMetrics(ctx)
		if err != nil {
			sbxlogger.E(s).Warn("failed to get metrics", zap.Error(err))
		} else {
			sbxlogger.E(s).Metrics(sbxlogger.SandboxMetricsFields{
				Timestamp:      metrics.Timestamp,
				CPUCount:       metrics.CPUCount,
				CPUUsedPercent: metrics.CPUUsedPercent,
				MemTotalMiB:    metrics.MemTotalMiB,
				MemUsedMiB:     metrics.MemUsedMiB,
			})
		}
	}
}

func (s *Sandbox) SendMetrics(ctx context.Context) {
	if isGTEVersion(s.Config.EnvdVersion, minEnvdVersionForMetrcis) {
		envdMetrics, err := s.GetMetrics(ctx)
		if err != nil {
			sbxlogger.E(s).Warn("failed to get metrics from envd", zap.Error(err))
		} else {
			// XXX update upstream types to avoid this conversion
			metrics := chmodels.Metrics{
				SandboxID:      s.Config.SandboxId,
				TeamID:         s.Config.TeamId,
				Timestamp:      time.Unix(envdMetrics.Timestamp, 0),
				MemTotalMiB:    envdMetrics.MemTotalMiB,
				MemUsedMiB:     envdMetrics.MemUsedMiB,
				CPUCount:       envdMetrics.CPUCount,
				CPUUsedPercent: envdMetrics.CPUUsedPercent,
			}

			err := s.ClickhouseStore.InsertMetrics(ctx, metrics)
			if err != nil {
				sbxlogger.E(s).Warn("failed to insert metrics in ClickHouse", zap.Error(err))
			}
		}
	}
}



================================================
File: internal/sandbox/sandbox_linux.go
================================================
//go:build linux
// +build linux

package sandbox

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"os"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/google/uuid"
	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/trace"
	"go.uber.org/zap"
	"golang.org/x/mod/semver"
	"golang.org/x/sys/unix"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/dns"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/build"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/fc"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/nbd"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/network"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/rootfs"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/uffd"
	"github.com/e2b-dev/infra/packages/shared/pkg/chdb"
	"github.com/e2b-dev/infra/packages/shared/pkg/env"
	"github.com/e2b-dev/infra/packages/shared/pkg/grpc/orchestrator"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
	"github.com/e2b-dev/infra/packages/shared/pkg/telemetry"
	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

var envdTimeout = utils.Must(time.ParseDuration(env.GetEnv("ENVD_TIMEOUT", "10s")))

var httpClient = http.Client{
	Timeout: 10 * time.Second,
}

type Sandbox struct {
	files   *storage.SandboxFiles
	cleanup *Cleanup

	process *fc.Process
	uffd    *uffd.Uffd
	rootfs  *rootfs.CowDevice

	Config    *orchestrator.SandboxConfig
	StartedAt time.Time
	EndAt     time.Time

	Slot network.Slot

	uffdExit chan error

	template template.Template

	healthcheckCtx *utils.LockableCancelableContext
	healthy        atomic.Bool

	ClickhouseStore chdb.Store

	//
	useLokiMetrics       string
	useClickhouseMetrics string
	CleanupID            string
}

func (s *Sandbox) LoggerMetadata() sbxlogger.SandboxMetadata {
	return sbxlogger.SandboxMetadata{
		SandboxID:  s.Config.SandboxId,
		TemplateID: s.Config.TemplateId,
		TeamID:     s.Config.TeamId,
	}
}

// Run cleanup functions for the already initialized resources if there is any error or after you are done with the started sandbox.
func NewSandbox(
	ctx context.Context,
	tracer trace.Tracer,
	dns *dns.DNS,
	networkPool *network.Pool,
	templateCache *template.Cache,
	config *orchestrator.SandboxConfig,
	traceID string,
	startedAt time.Time,
	endAt time.Time,
	isSnapshot bool,
	baseTemplateID string,
	clientID string,
	devicePool *nbd.DevicePool,
	clickhouseStore chdb.Store,
	useLokiMetrics string,
	useClickhouseMetrics string,
) (*Sandbox, *Cleanup, error) {
	childCtx, childSpan := tracer.Start(ctx, "new-sandbox")
	defer childSpan.End()

	cleanup := NewCleanup()

	t, err := templateCache.GetTemplate(
		config.TemplateId,
		config.BuildId,
		config.KernelVersion,
		config.FirecrackerVersion,
		config.HugePages,
		isSnapshot,
	)
	if err != nil {
		return nil, cleanup, fmt.Errorf("failed to get template snapshot data: %w", err)
	}

	networkCtx, networkSpan := tracer.Start(childCtx, "get-network-slot")
	defer networkSpan.End()

	ips, err := networkPool.Get(networkCtx)
	if err != nil {
		return nil, cleanup, fmt.Errorf("failed to get network slot: %w", err)
	}

	cleanup.Add(func() error {
		returnErr := networkPool.Return(ips)
		if returnErr != nil {
			return fmt.Errorf("failed to return network slot: %w", returnErr)
		}

		return nil
	})
	networkSpan.End()

	sandboxFiles := t.Files().NewSandboxFiles(config.SandboxId)

	cleanup.Add(func() error {
		filesErr := cleanupFiles(sandboxFiles)
		if filesErr != nil {
			return fmt.Errorf("failed to cleanup files: %w", filesErr)
		}

		return nil
	})

	_, overlaySpan := tracer.Start(childCtx, "create-rootfs-overlay")
	defer overlaySpan.End()

	readonlyRootfs, err := t.Rootfs()
	if err != nil {
		return nil, cleanup, fmt.Errorf("failed to get rootfs: %w", err)
	}

	rootfsOverlay, err := rootfs.NewCowDevice(
		readonlyRootfs,
		sandboxFiles.SandboxCacheRootfsPath(),
		sandboxFiles.RootfsBlockSize(),
		devicePool,
	)
	if err != nil {
		return nil, cleanup, fmt.Errorf("failed to create overlay file: %w", err)
	}

	cleanup.Add(func() error {
		rootfsOverlay.Close()

		return nil
	})

	go func() {
		runErr := rootfsOverlay.Start(childCtx)
		if runErr != nil {
			zap.L().Error("rootfs overlay error", zap.Error(runErr))
		}
	}()

	memfile, err := t.Memfile()
	if err != nil {
		return nil, cleanup, fmt.Errorf("failed to get memfile: %w", err)
	}
	overlaySpan.End()

	fcUffd, uffdErr := uffd.New(memfile, sandboxFiles.SandboxUffdSocketPath(), sandboxFiles.MemfilePageSize(), clientID)
	if uffdErr != nil {
		return nil, cleanup, fmt.Errorf("failed to create uffd: %w", uffdErr)
	}

	uffdStartErr := fcUffd.Start(config.SandboxId)
	if uffdStartErr != nil {
		return nil, cleanup, fmt.Errorf("failed to start uffd: %w", uffdStartErr)
	}

	cleanup.Add(func() error {
		stopErr := fcUffd.Stop()
		if stopErr != nil {
			return fmt.Errorf("failed to stop uffd: %w", stopErr)
		}

		return nil
	})

	uffdExit := make(chan error, 1)

	uffdStartCtx, cancelUffdStartCtx := context.WithCancelCause(childCtx)
	defer cancelUffdStartCtx(fmt.Errorf("uffd finished starting"))

	go func() {
		uffdWaitErr := <-fcUffd.Exit
		uffdExit <- uffdWaitErr

		cancelUffdStartCtx(fmt.Errorf("uffd process exited: %w", errors.Join(uffdWaitErr, context.Cause(uffdStartCtx))))
	}()

	// todo: check if kernel, firecracker, and envd versions exist
	snapfile, err := t.Snapfile()
	if err != nil {
		return nil, cleanup, fmt.Errorf("failed to get snapfile: %w", err)
	}

	fcHandle, fcErr := fc.NewProcess(
		uffdStartCtx,
		tracer,
		ips,
		sandboxFiles,
		&fc.MmdsMetadata{
			SandboxId:            config.SandboxId,
			TemplateId:           config.TemplateId,
			LogsCollectorAddress: os.Getenv("LOGS_COLLECTOR_PUBLIC_IP"),
			TraceId:              traceID,
			TeamId:               config.TeamId,
		},
		snapfile,
		rootfsOverlay,
		fcUffd.Ready,
		baseTemplateID,
	)
	if fcErr != nil {
		return nil, cleanup, fmt.Errorf("failed to create FC: %w", fcErr)
	}

	fcStartErr := fcHandle.Start(uffdStartCtx, tracer)
	if fcStartErr != nil {
		return nil, cleanup, fmt.Errorf("failed to start FC: %w", fcStartErr)
	}

	telemetry.ReportEvent(childCtx, "initialized FC")

	healthcheckCtx := utils.NewLockableCancelableContext(context.Background())

	sbx := &Sandbox{
		uffdExit:        uffdExit,
		files:           sandboxFiles,
		Slot:            ips,
		template:        t,
		process:         fcHandle,
		uffd:            fcUffd,
		Config:          config,
		StartedAt:       startedAt,
		EndAt:           endAt,
		rootfs:          rootfsOverlay,
		cleanup:         cleanup,
		healthcheckCtx:  healthcheckCtx,
		healthy:         atomic.Bool{}, // defaults to `false`
		ClickhouseStore: clickhouseStore,
		CleanupID:       uuid.New().String(),
	}
	// By default, the sandbox should be healthy, if the status change we report it.
	sbx.healthy.Store(true)

	cleanup.AddPriority(func() error {
		var errs []error

		fcStopErr := fcHandle.Stop()
		if fcStopErr != nil {
			errs = append(errs, fmt.Errorf("failed to stop FC: %w", fcStopErr))
		}

		uffdStopErr := fcUffd.Stop()
		if uffdStopErr != nil {
			errs = append(errs, fmt.Errorf("failed to stop uffd: %w", uffdStopErr))
		}

		healthcheckCtx.Lock()
		healthcheckCtx.Cancel()
		healthcheckCtx.Unlock()

		return errors.Join(errs...)
	})

	// Ensure the syncing takes at most envdTimeout seconds.
	syncCtx, syncCancel := context.WithTimeoutCause(childCtx, envdTimeout, fmt.Errorf("syncing took too long"))
	defer syncCancel()

	// Sync envds.
	if semver.Compare(fmt.Sprintf("v%s", config.EnvdVersion), "v0.1.1") >= 0 {
		initErr := sbx.initEnvd(syncCtx, tracer, config.EnvVars)
		if initErr != nil {
			return nil, cleanup, fmt.Errorf("failed to init new envd: %w", initErr)
		} else {
			telemetry.ReportEvent(childCtx, fmt.Sprintf("[sandbox %s]: initialized new envd", config.SandboxId))
		}
	} else {
		syncErr := sbx.syncOldEnvd(syncCtx)
		if syncErr != nil {
			telemetry.ReportError(childCtx, fmt.Errorf("failed to sync old envd: %w", syncErr))
		} else {
			telemetry.ReportEvent(childCtx, fmt.Sprintf("[sandbox %s]: synced old envd", config.SandboxId))
		}
	}

	sbx.StartedAt = time.Now()

	dns.Add(config.SandboxId, ips.HostIP())

	telemetry.ReportEvent(childCtx, "added DNS record", attribute.String("ip", ips.HostIP()), attribute.String("hostname", config.SandboxId))

	cleanup.Add(func() error {
		dns.Remove(config.SandboxId, ips.HostIP())

		return nil
	})

	go sbx.logHeathAndUsage(healthcheckCtx)

	return sbx, cleanup, nil
}

func (s *Sandbox) Wait() error {
	select {
	case fcErr := <-s.process.Exit:
		stopErr := s.Stop()
		uffdErr := <-s.uffdExit

		return errors.Join(fcErr, stopErr, uffdErr)
	case uffdErr := <-s.uffdExit:
		stopErr := s.Stop()
		fcErr := <-s.process.Exit

		return errors.Join(uffdErr, stopErr, fcErr)
	}
}

func (s *Sandbox) Stop() error {
	err := s.cleanup.Run()
	if err != nil {
		return fmt.Errorf("failed to stop sandbox: %w", err)
	}

	return nil
}

func (s *Sandbox) Snapshot(
	ctx context.Context,
	tracer trace.Tracer,
	snapshotTemplateFiles *storage.TemplateCacheFiles,
	releaseLock func(),
) (*Snapshot, error) {
	ctx, childSpan := tracer.Start(ctx, "sandbox-snapshot")
	defer childSpan.End()

	buildId, err := uuid.Parse(snapshotTemplateFiles.BuildId)
	if err != nil {
		return nil, fmt.Errorf("failed to parse build id: %w", err)
	}

	// MEMFILE & SNAPFILE
	originalMemfile, err := s.template.Memfile()
	if err != nil {
		return nil, fmt.Errorf("failed to get original memfile: %w", err)
	}

	memfileMetadata := &header.Metadata{
		Version:     1,
		Generation:  originalMemfile.Header().Metadata.Generation + 1,
		BlockSize:   originalMemfile.Header().Metadata.BlockSize,
		Size:        originalMemfile.Header().Metadata.Size,
		BuildId:     buildId,
		BaseBuildId: originalMemfile.Header().Metadata.BaseBuildId,
	}

	s.healthcheckCtx.Lock()
	s.healthcheckCtx.Cancel()
	s.healthcheckCtx.Unlock()

	err = s.process.Pause(ctx, tracer)
	if err != nil {
		return nil, fmt.Errorf("error pausing vm: %w", err)
	}

	err = s.uffd.Disable()
	if err != nil {
		return nil, fmt.Errorf("failed to disable uffd: %w", err)
	}

	defer os.RemoveAll(snapshotTemplateFiles.CacheMemfileFullSnapshotPath())

	err = s.process.CreateSnapshot(
		ctx,
		tracer,
		snapshotTemplateFiles.CacheSnapfilePath(),
		snapshotTemplateFiles.CacheMemfileFullSnapshotPath(),
	)
	if err != nil {
		return nil, fmt.Errorf("error creating snapshot: %w", err)
	}

	memfileDirtyPages := s.uffd.Dirty()

	sourceFile, err := os.Open(snapshotTemplateFiles.CacheMemfileFullSnapshotPath())
	if err != nil {
		return nil, fmt.Errorf("failed to open memfile: %w", err)
	}

	memfileDiffFile, err := build.NewLocalDiffFile(
		build.DefaultCachePath,
		buildId.String(),
		build.Memfile,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create memfile diff file: %w", err)
	}

	err = header.CreateDiff(sourceFile, s.files.MemfilePageSize(), memfileDirtyPages, memfileDiffFile)
	if err != nil {
		return nil, fmt.Errorf("failed to create memfile diff: %w", err)
	}

	telemetry.ReportEvent(ctx, "created memfile diff")

	os.RemoveAll(snapshotTemplateFiles.CacheMemfileFullSnapshotPath())

	releaseLock()

	memfileMapping := header.CreateMapping(
		memfileMetadata,
		&buildId,
		memfileDirtyPages,
	)

	telemetry.ReportEvent(ctx, "created memfile mapping")

	memfileMappings := header.MergeMappings(
		originalMemfile.Header().Mapping,
		memfileMapping,
	)

	telemetry.ReportEvent(ctx, "merged memfile mappings")

	snapfile, err := template.NewLocalFile(snapshotTemplateFiles.CacheSnapfilePath())
	if err != nil {
		return nil, fmt.Errorf("failed to create local snapfile: %w", err)
	}

	// ROOTFS
	originalRootfs, err := s.template.Rootfs()
	if err != nil {
		return nil, fmt.Errorf("failed to get original rootfs: %w", err)
	}

	rootfsMetadata := &header.Metadata{
		Version:     1,
		Generation:  originalRootfs.Header().Metadata.Generation + 1,
		BlockSize:   originalRootfs.Header().Metadata.BlockSize,
		Size:        originalRootfs.Header().Metadata.Size,
		BuildId:     buildId,
		BaseBuildId: originalRootfs.Header().Metadata.BaseBuildId,
	}

	nbdPath, err := s.rootfs.Path()
	if err != nil {
		return nil, fmt.Errorf("failed to get rootfs path: %w", err)
	}

	// Flush the data to the operating system's buffer
	file, err := os.Open(nbdPath)
	if err != nil {
		return nil, fmt.Errorf("failed to open rootfs path: %w", err)
	}

	if err := unix.IoctlSetInt(int(file.Fd()), unix.BLKFLSBUF, 0); err != nil {
		return nil, fmt.Errorf("ioctl BLKFLSBUF failed: %w", err)
	}

	err = syscall.Fsync(int(file.Fd()))
	if err != nil {
		return nil, fmt.Errorf("failed to fsync rootfs path: %w", err)
	}

	err = file.Sync()
	if err != nil {
		return nil, fmt.Errorf("failed to sync rootfs path: %w", err)
	}

	telemetry.ReportEvent(ctx, "synced rootfs")

	rootfsDiffFile, err := build.NewLocalDiffFile(build.DefaultCachePath, buildId.String(), build.Rootfs)
	if err != nil {
		return nil, fmt.Errorf("failed to create rootfs diff: %w", err)
	}

	rootfsDirtyBlocks, err := s.rootfs.Export(ctx, rootfsDiffFile, s.Stop)
	if err != nil {
		return nil, fmt.Errorf("failed to export rootfs: %w", err)
	}

	telemetry.ReportEvent(ctx, "exported rootfs")

	rootfsMapping := header.CreateMapping(
		rootfsMetadata,
		&buildId,
		rootfsDirtyBlocks,
	)

	telemetry.ReportEvent(ctx, "created rootfs mapping")

	rootfsMappings := header.MergeMappings(
		originalRootfs.Header().Mapping,
		rootfsMapping,
	)

	telemetry.ReportEvent(ctx, "merged rootfs mappings")

	rootfsDiff, err := rootfsDiffFile.ToDiff(int64(originalRootfs.Header().Metadata.BlockSize))
	if err != nil {
		return nil, fmt.Errorf("failed to convert rootfs diff file to local diff: %w", err)
	}

	telemetry.ReportEvent(ctx, "converted rootfs diff file to local diff")

	memfileDiff, err := memfileDiffFile.ToDiff(int64(originalMemfile.Header().Metadata.BlockSize))
	if err != nil {
		return nil, fmt.Errorf("failed to convert memfile diff file to local diff: %w", err)
	}

	telemetry.ReportEvent(ctx, "converted memfile diff file to local diff")

	telemetry.SetAttributes(ctx,
		attribute.Int64("snapshot.memfile.header.mappings.length", int64(len(memfileMappings))),
		attribute.Int64("snapshot.rootfs.header.mappings.length", int64(len(rootfsMappings))),
		attribute.Int64("snapshot.memfile.diff.size", int64(memfileDirtyPages.Count()*uint(originalMemfile.Header().Metadata.BlockSize))),
		attribute.Int64("snapshot.memfile.mapped_size", int64(memfileMetadata.Size)),
		attribute.Int64("snapshot.memfile.block_size", int64(memfileMetadata.BlockSize)),
		attribute.Int64("snapshot.rootfs.diff.size", int64(rootfsDirtyBlocks.Count()*uint(originalRootfs.Header().Metadata.BlockSize))),
		attribute.Int64("snapshot.rootfs.mapped_size", int64(rootfsMetadata.Size)),
		attribute.Int64("snapshot.rootfs.block_size", int64(rootfsMetadata.BlockSize)),
		attribute.Int64("snapshot.metadata.version", int64(memfileMetadata.Version)),
		attribute.Int64("snapshot.metadata.generation", int64(memfileMetadata.Generation)),
		attribute.String("snapshot.metadata.build_id", memfileMetadata.BuildId.String()),
		attribute.String("snapshot.metadata.base_build_id", memfileMetadata.BaseBuildId.String()),
	)

	return &Snapshot{
		Snapfile:          snapfile,
		MemfileDiff:       memfileDiff,
		MemfileDiffHeader: header.NewHeader(memfileMetadata, memfileMappings),
		RootfsDiff:        rootfsDiff,
		RootfsDiffHeader:  header.NewHeader(rootfsMetadata, rootfsMappings),
	}, nil
}

type Snapshot struct {
	MemfileDiff       build.Diff
	MemfileDiffHeader *header.Header
	RootfsDiff        build.Diff
	RootfsDiffHeader  *header.Header
	Snapfile          *template.LocalFile
}



================================================
File: internal/sandbox/sandbox_other.go
================================================
//go:build !linux
// +build !linux

package sandbox

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"sync/atomic"
	"time"

	"go.opentelemetry.io/otel/trace"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/dns"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/build"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/nbd"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/network"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
	"github.com/e2b-dev/infra/packages/shared/pkg/chdb"
	"github.com/e2b-dev/infra/packages/shared/pkg/grpc/orchestrator"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
)

var httpClient = http.Client{
	Timeout: 10 * time.Second,
}

type NoOpProcess struct {
	Exit chan error
}

type NoOpCleanup struct {
}

func (m *NoOpCleanup) Run() error {
	return errors.New("platform does not support sandbox")
}

type Sandbox struct {

	// YOU ARE IN SANDBOX_OTHER.GO
	// YOU PROBABLY WANT TO BE IN SANDBOX_LINUX.GO

	Config          *orchestrator.SandboxConfig
	process         NoOpProcess
	uffdExit        chan error
	cleanup         NoOpCleanup
	healthy         atomic.Bool
	Slot            network.Slot
	EndAt           time.Time
	StartedAt       time.Time
	ClickhouseStore chdb.Store

	useLokiMetrics       string
	useClickhouseMetrics string

	CleanupID string
}

func (s *Sandbox) LoggerMetadata() sbxlogger.SandboxMetadata {
	panic("platform does not support sandbox")
}

// Run cleanup functions for the already initialized resources if there is any error or after you are done with the started sandbox.
func NewSandbox(

	// YOU ARE IN SANDBOX_OTHER.GO
	// YOU PROBABLY WANT TO BE IN SANDBOX_LINUX.GO

	ctx context.Context,
	tracer trace.Tracer,
	dns *dns.DNS,
	networkPool *network.Pool,
	templateCache *template.Cache,
	config *orchestrator.SandboxConfig,
	traceID string,
	startedAt time.Time,
	endAt time.Time,
	isSnapshot bool,
	baseTemplateID string,
	clientID string,
	devicePool *nbd.DevicePool,
	clickhouseStore chdb.Store,
	useLokiMetrics string,
	useClickhouseMetrics string,
) (*Sandbox, *Cleanup, error) {
	return nil, nil, errors.New("platform does not support sandbox")
}

func (s *Sandbox) Wait() error {
	return errors.New("platform does not support sandbox")
}

func (s *Sandbox) Stop() error {
	err := s.cleanup.Run()
	if err != nil {
		return fmt.Errorf("failed to stop sandbox: %w", err)
	}

	return nil
}

func (s *Sandbox) Snapshot(
	ctx context.Context,
	tracer trace.Tracer,
	snapshotTemplateFiles *storage.TemplateCacheFiles,
	releaseLock func(),
) (*Snapshot, error) {
	return nil, errors.New("platform does not support snapshot")
}

type Snapshot struct {
	MemfileDiff       build.Diff
	MemfileDiffHeader *header.Header
	RootfsDiff        build.Diff
	RootfsDiffHeader  *header.Header
	Snapfile          *template.LocalFile
}



================================================
File: internal/sandbox/block/cache.go
================================================
package block

import (
	"errors"
	"fmt"
	"io"
	"os"
	"sort"
	"sync"
	"sync/atomic"
	"syscall"

	"github.com/bits-and-blooms/bitset"
	"github.com/edsrzf/mmap-go"
	"go.uber.org/zap"
	"golang.org/x/sys/unix"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
)

type ErrCacheClosed struct {
	filePath string
}

func (e *ErrCacheClosed) Error() string {
	return fmt.Sprintf("block cache already closed for path %s", e.filePath)
}

func NewErrCacheClosed(filePath string) *ErrCacheClosed {
	return &ErrCacheClosed{
		filePath: filePath,
	}
}

type Cache struct {
	filePath  string
	size      int64
	blockSize int64
	mmap      *mmap.MMap
	mu        sync.RWMutex
	dirty     sync.Map
	dirtyFile bool
	closed    atomic.Bool
}

// When we are passing filePath that is a file that has content we want to server want to use dirtyFile = true.
func NewCache(size, blockSize int64, filePath string, dirtyFile bool) (*Cache, error) {
	f, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE, 0o644)
	if err != nil {
		return nil, fmt.Errorf("error opening file: %w", err)
	}

	defer f.Close()

	// This should create a sparse file on Linux.
	err = f.Truncate(size)
	if err != nil {
		return nil, fmt.Errorf("error allocating file: %w", err)
	}

	mm, err := mmap.MapRegion(f, int(size), unix.PROT_READ|unix.PROT_WRITE, 0, 0)
	if err != nil {
		return nil, fmt.Errorf("error mapping file: %w", err)
	}

	return &Cache{
		mmap:      &mm,
		filePath:  filePath,
		size:      size,
		blockSize: blockSize,
		dirtyFile: dirtyFile,
	}, nil
}

func (m *Cache) isClosed() bool {
	return m.closed.Load()
}

func (m *Cache) Export(out io.Writer) (*bitset.BitSet, error) {
	m.mu.Lock()
	defer m.mu.Unlock()

	if m.isClosed() {
		return nil, NewErrCacheClosed(m.filePath)
	}

	err := m.mmap.Flush()
	if err != nil {
		return nil, fmt.Errorf("error flushing mmap: %w", err)
	}

	tracked := bitset.New(uint(header.TotalBlocks(m.size, m.blockSize)))

	for _, key := range m.dirtySortedKeys() {
		block := header.BlockIdx(key, m.blockSize)

		tracked.Set(uint(block))

		_, err := out.Write((*m.mmap)[key : key+m.blockSize])
		if err != nil {
			zap.L().Error("error writing to out", zap.Error(err))

			return nil, err
		}
	}

	return tracked, nil
}

func (m *Cache) ReadAt(b []byte, off int64) (int, error) {
	m.mu.RLock()
	defer m.mu.RUnlock()

	if m.isClosed() {
		return 0, NewErrCacheClosed(m.filePath)
	}

	slice, err := m.Slice(off, int64(len(b)))
	if err != nil {
		return 0, fmt.Errorf("error slicing mmap: %w", err)
	}

	return copy(b, slice), nil
}

func (m *Cache) WriteAt(b []byte, off int64) (int, error) {
	m.mu.Lock()
	defer m.mu.Unlock()

	if m.isClosed() {
		return 0, NewErrCacheClosed(m.filePath)
	}

	return m.WriteAtWithoutLock(b, off)
}

func (m *Cache) Close() error {
	m.mu.Lock()
	defer m.mu.Unlock()

	succ := m.closed.CompareAndSwap(false, true)
	if !succ {
		return NewErrCacheClosed(m.filePath)
	}

	return errors.Join(
		m.mmap.Unmap(),
		os.RemoveAll(m.filePath),
	)
}

func (m *Cache) Size() (int64, error) {
	if m.isClosed() {
		return 0, NewErrCacheClosed(m.filePath)
	}

	return m.size, nil
}

// Slice returns a slice of the mmap.
// When using Slice you must ensure thread safety, ideally by only writing to the same block once and the exposing the slice.
func (m *Cache) Slice(off, length int64) ([]byte, error) {
	if m.isClosed() {
		return nil, NewErrCacheClosed(m.filePath)
	}

	if m.dirtyFile || m.isCached(off, length) {
		end := off + length
		if end > m.size {
			end = m.size
		}

		return (*m.mmap)[off:end], nil
	}

	return nil, ErrBytesNotAvailable{}
}

func (m *Cache) isCached(off, length int64) bool {
	for _, blockOff := range header.BlocksOffsets(length, m.blockSize) {
		_, dirty := m.dirty.Load(off + blockOff)
		if !dirty {
			return false
		}
	}

	return true
}

func (m *Cache) setIsCached(off, length int64) {
	for _, blockOff := range header.BlocksOffsets(length, m.blockSize) {
		m.dirty.Store(off+blockOff, struct{}{})
	}
}

// When using WriteAtWithoutLock you must ensure thread safety, ideally by only writing to the same block once and the exposing the slice.
func (m *Cache) WriteAtWithoutLock(b []byte, off int64) (int, error) {
	if m.isClosed() {
		return 0, NewErrCacheClosed(m.filePath)
	}

	end := off + int64(len(b))
	if end > m.size {
		end = m.size
	}

	n := copy((*m.mmap)[off:end], b)

	m.setIsCached(off, end-off)

	return n, nil
}

// dirtySortedKeys returns a sorted list of dirty keys.
// Key represents a block offset.
func (m *Cache) dirtySortedKeys() []int64 {
	var keys []int64
	m.dirty.Range(func(key, _ any) bool {
		keys = append(keys, key.(int64))
		return true
	})
	sort.Slice(keys, func(i, j int) bool {
		return keys[i] < keys[j]
	})

	return keys
}

// FileSize returns the size of the cache on disk.
// The size might differ from the dirty size, as it may not be fully on disk.
func (m *Cache) FileSize() (int64, error) {
	var stat syscall.Stat_t
	err := syscall.Stat(m.filePath, &stat)
	if err != nil {
		return 0, fmt.Errorf("failed to get file stats: %w", err)
	}

	var fsStat syscall.Statfs_t
	err = syscall.Statfs(m.filePath, &fsStat)
	if err != nil {
		return 0, fmt.Errorf("failed to get disk stats for path %s: %w", m.filePath, err)
	}

	return stat.Blocks * int64(fsStat.Bsize), nil
}



================================================
File: internal/sandbox/block/chunk.go
================================================
package block

import (
	"context"
	"errors"
	"fmt"
	"io"

	"go.uber.org/zap"
	"golang.org/x/sync/errgroup"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

const (
	// Chunks must always be bigger or equal to the block size.
	ChunkSize = 4 * 1024 * 1024 // 4 MB
)

type Chunker struct {
	ctx context.Context

	base  io.ReaderAt
	cache *Cache

	size int64

	// TODO: Optimize this so we don't need to keep the fetchers in memory.
	fetchers *utils.WaitMap
}

func NewChunker(
	ctx context.Context,
	size,
	blockSize int64,
	base io.ReaderAt,
	cachePath string,
) (*Chunker, error) {
	cache, err := NewCache(size, blockSize, cachePath, false)
	if err != nil {
		return nil, fmt.Errorf("failed to create file cache: %w", err)
	}

	chunker := &Chunker{
		ctx:      ctx,
		size:     size,
		base:     base,
		cache:    cache,
		fetchers: utils.NewWaitMap(),
	}

	return chunker, nil
}

func (c *Chunker) ReadAt(b []byte, off int64) (int, error) {
	slice, err := c.Slice(off, int64(len(b)))
	if err != nil {
		return 0, fmt.Errorf("failed to slice cache at %d-%d: %w", off, off+int64(len(b)), err)
	}

	return copy(b, slice), nil
}

func (c *Chunker) WriteTo(w io.Writer) (int64, error) {
	for i := int64(0); i < c.size; i += ChunkSize {
		chunk := make([]byte, ChunkSize)

		n, err := c.ReadAt(chunk, i)
		if err != nil {
			return 0, fmt.Errorf("failed to slice cache at %d-%d: %w", i, i+ChunkSize, err)
		}

		_, err = w.Write(chunk[:n])
		if err != nil {
			return 0, fmt.Errorf("failed to write chunk %d to writer: %w", i, err)
		}
	}

	return c.size, nil
}

func (c *Chunker) Slice(off, length int64) ([]byte, error) {
	b, err := c.cache.Slice(off, length)
	if err == nil {
		return b, nil
	}

	if !errors.As(err, &ErrBytesNotAvailable{}) {
		return nil, fmt.Errorf("failed read from cache at offset %d: %w", off, err)
	}

	chunkErr := c.fetchToCache(off, length)
	if chunkErr != nil {
		return nil, fmt.Errorf("failed to ensure data at %d-%d: %w", off, off+length, chunkErr)
	}

	b, cacheErr := c.cache.Slice(off, length)
	if cacheErr != nil {
		return nil, fmt.Errorf("failed to read from cache after ensuring data at %d-%d: %w", off, off+length, cacheErr)
	}

	return b, nil
}

// fetchToCache ensures that the data at the given offset and length is available in the cache.
func (c *Chunker) fetchToCache(off, length int64) error {
	var eg errgroup.Group

	chunks := header.BlocksOffsets(length, ChunkSize)

	startingChunk := header.BlockIdx(off, ChunkSize)
	startingChunkOffset := header.BlockOffset(startingChunk, ChunkSize)

	for _, chunkOff := range chunks {
		// Ensure the closure captures the correct block offset.
		fetchOff := startingChunkOffset + chunkOff

		eg.Go(func() (err error) {
			defer func() {
				if r := recover(); r != nil {
					zap.L().Error("recovered from panic in the fetch handler", zap.Any("error", r))
					err = fmt.Errorf("recovered from panic in the fetch handler: %v", r)
				}
			}()

			err = c.fetchers.Wait(fetchOff, func() error {
				select {
				case <-c.ctx.Done():
					return fmt.Errorf("error fetching range %d-%d: %w", fetchOff, fetchOff+ChunkSize, c.ctx.Err())
				default:
				}

				b := make([]byte, ChunkSize)

				_, err := c.base.ReadAt(b, fetchOff)
				if err != nil && !errors.Is(err, io.EOF) {
					return fmt.Errorf("failed to read chunk from base %d: %w", fetchOff, err)
				}

				_, cacheErr := c.cache.WriteAtWithoutLock(b, fetchOff)
				if cacheErr != nil {
					return fmt.Errorf("failed to write chunk %d to cache: %w", fetchOff, cacheErr)
				}

				return nil
			})

			return err
		})
	}

	err := eg.Wait()
	if err != nil {
		return fmt.Errorf("failed to ensure data at %d-%d: %w", off, off+length, err)
	}

	return nil
}

func (c *Chunker) Close() error {
	return c.cache.Close()
}

func (c *Chunker) FileSize() (int64, error) {
	return c.cache.FileSize()
}



================================================
File: internal/sandbox/block/device.go
================================================
package block

import "io"

type ErrBytesNotAvailable struct{}

func (ErrBytesNotAvailable) Error() string {
	return "The requested bytes are not available on the device"
}

type ReadonlyDevice interface {
	io.ReaderAt
	Slice(off, length int64) ([]byte, error)
	Size() (int64, error)
}

type Device interface {
	ReadonlyDevice
	io.WriterAt
	Close() error
}



================================================
File: internal/sandbox/block/local.go
================================================
package block

import (
	"errors"
	"fmt"
	"os"

	"github.com/edsrzf/mmap-go"
	"golang.org/x/sys/unix"
)

type Local struct {
	m    mmap.MMap
	size int64
	path string
}

func NewLocal(path string) (*Local, error) {
	f, err := os.OpenFile(path, os.O_RDONLY, 0o777)
	if err != nil {
		return nil, fmt.Errorf("failed to open file: %w", err)
	}

	info, err := f.Stat()
	if err != nil {
		return nil, fmt.Errorf("failed to get file info: %w", err)
	}

	defer f.Close()

	m, err := mmap.Map(f, unix.PROT_READ, mmap.RDONLY)
	if err != nil {
		return nil, fmt.Errorf("failed to map region: %w", err)
	}

	return &Local{
		m:    m,
		size: info.Size(),
		path: path,
	}, nil
}

func (d *Local) ReadAt(p []byte, off int64) (int, error) {
	slice, err := d.Slice(off, int64(len(p)))
	if err != nil {
		return 0, fmt.Errorf("failed to slice mmap: %w", err)
	}

	return copy(p, slice), nil
}

func (d *Local) Size() (int64, error) {
	return d.size, nil
}

func (d *Local) Close() error {
	return errors.Join(
		d.m.Unmap(),
		os.Remove(d.path),
	)
}

func (d *Local) Slice(off, length int64) ([]byte, error) {
	end := off + length
	if end > d.size {
		end = d.size
	}

	return d.m[off:end], nil
}



================================================
File: internal/sandbox/block/overlay.go
================================================
package block

import (
	"errors"
	"fmt"
	"sync/atomic"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
)

type Overlay struct {
	device       ReadonlyDevice
	cache        *Cache
	blockSize    int64
	cacheEjected atomic.Bool
}

func NewOverlay(device ReadonlyDevice, cache *Cache, blockSize int64) *Overlay {
	return &Overlay{
		device:    device,
		cache:     cache,
		blockSize: blockSize,
	}
}

func (o *Overlay) ReadAt(p []byte, off int64) (int, error) {
	blocks := header.BlocksOffsets(int64(len(p)), o.blockSize)

	for _, blockOff := range blocks {
		n, err := o.cache.ReadAt(p[blockOff:blockOff+o.blockSize], off+blockOff)
		if err == nil {
			continue
		}

		if !errors.As(err, &ErrBytesNotAvailable{}) {
			return n, fmt.Errorf("error reading from cache: %w", err)
		}

		n, err = o.device.ReadAt(p[blockOff:blockOff+o.blockSize], off+blockOff)
		if err != nil {
			return n, fmt.Errorf("error reading from device: %w", err)
		}
	}

	return len(p), nil
}

func (o *Overlay) EjectCache() (*Cache, error) {
	if !o.cacheEjected.CompareAndSwap(false, true) {
		return nil, fmt.Errorf("cache already ejected")
	}

	return o.cache, nil
}

// This method will not be very optimal if the length is not the same as the block size, because we cannot be just exposing the cache slice,
// but creating and copying the bytes from the cache and device to the new slice.
//
// When we are implementing this we might want to just enforce the length to be the same as the block size.
func (o *Overlay) Slice(off, length int64) ([]byte, error) {
	return nil, fmt.Errorf("not implemented")
}

func (o *Overlay) WriteAt(p []byte, off int64) (int, error) {
	return o.cache.WriteAt(p, off)
}

func (o *Overlay) Size() (int64, error) {
	return o.cache.Size()
}

func (o *Overlay) Close() error {
	if o.cacheEjected.Load() {
		return nil
	}

	return o.cache.Close()
}



================================================
File: internal/sandbox/block/tracker.go
================================================
package block

import (
	"fmt"
	"sync"
	"sync/atomic"

	"github.com/bits-and-blooms/bitset"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
)

type TrackedSliceDevice struct {
	data      ReadonlyDevice
	blockSize int64

	nilTracking atomic.Bool
	dirty       *bitset.BitSet
	dirtyMu     sync.Mutex
	empty       []byte
}

func NewTrackedSliceDevice(blockSize int64, device ReadonlyDevice) (*TrackedSliceDevice, error) {
	return &TrackedSliceDevice{
		data:      device,
		empty:     make([]byte, blockSize),
		blockSize: blockSize,
	}, nil
}

func (t *TrackedSliceDevice) Disable() error {
	size, err := t.data.Size()
	if err != nil {
		return fmt.Errorf("failed to get device size: %w", err)
	}

	t.dirty = bitset.New(uint(header.TotalBlocks(size, t.blockSize)))
	// We are starting with all being dirty.
	t.dirty.FlipRange(0, t.dirty.Len())

	t.nilTracking.Store(true)

	return nil
}

func (t *TrackedSliceDevice) Slice(off int64, length int64) ([]byte, error) {
	if t.nilTracking.Load() {
		t.dirtyMu.Lock()
		t.dirty.Clear(uint(header.BlockIdx(off, t.blockSize)))
		t.dirtyMu.Unlock()

		return t.empty, nil
	}

	return t.data.Slice(off, length)
}

// Return which bytes were not read since Disable.
// This effectively returns the bytes that have been requested after paused vm and are not dirty.
func (t *TrackedSliceDevice) Dirty() *bitset.BitSet {
	t.dirtyMu.Lock()
	defer t.dirtyMu.Unlock()

	return t.dirty.Clone()
}



================================================
File: internal/sandbox/build/build.go
================================================
package build

import (
	"fmt"
	"io"

	"github.com/google/uuid"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
)

type File struct {
	header   *header.Header
	store    *DiffStore
	fileType DiffType
	bucket   *gcs.BucketHandle
}

func NewFile(
	header *header.Header,
	store *DiffStore,
	fileType DiffType,
	bucket *gcs.BucketHandle,
) *File {
	return &File{
		header:   header,
		store:    store,
		fileType: fileType,
		bucket:   bucket,
	}
}

func min(a, b int64) int64 {
	if a < b {
		return a
	}

	return b
}

func (b *File) ReadAt(p []byte, off int64) (n int, err error) {
	for n < len(p) {
		mappedOffset, mappedLength, buildID, err := b.header.GetShiftedMapping(off + int64(n))
		if err != nil {
			return 0, fmt.Errorf("failed to get mapping: %w", err)
		}

		remainingReadLength := int64(len(p)) - int64(n)

		readLength := min(mappedLength, remainingReadLength)

		if readLength <= 0 {
			zap.L().Error(fmt.Sprintf(
				"(%d bytes left to read, off %d) reading %d bytes from %+v/%+v: [%d:] -> [%d:%d] <> %d (mapped length: %d, remaining read length: %d)\n>>> EOF\n",
				len(p)-n,
				off,
				readLength,
				buildID,
				b.fileType,
				mappedOffset,
				n,
				int64(n)+readLength,
				n,
				mappedLength,
				remainingReadLength,
			))

			return n, io.EOF
		}

		// Skip reading when the uuid is nil.
		// We will use this to handle base builds that are already diffs.
		// The passed slice p must start as empty, otherwise we would need to copy the empty values there.
		if *buildID == uuid.Nil {
			n += int(readLength)

			continue
		}

		mappedBuild, err := b.getBuild(buildID)
		if err != nil {
			return 0, fmt.Errorf("failed to get build: %w", err)
		}

		buildN, err := mappedBuild.ReadAt(
			p[n:int64(n)+readLength],
			mappedOffset,
		)
		if err != nil {
			return 0, fmt.Errorf("failed to read from source: %w", err)
		}

		n += buildN
	}

	return n, nil
}

// The slice access must be in the predefined blocksize of the build.
func (b *File) Slice(off, length int64) ([]byte, error) {
	mappedOffset, _, buildID, err := b.header.GetShiftedMapping(off)
	if err != nil {
		return nil, fmt.Errorf("failed to get mapping: %w", err)
	}

	if *buildID == uuid.Nil {
		return header.EmptyHugePage, nil
	}

	build, err := b.getBuild(buildID)
	if err != nil {
		return nil, fmt.Errorf("failed to get build: %w", err)
	}

	return build.Slice(mappedOffset, int64(b.header.Metadata.BlockSize))
}

func (b *File) getBuild(buildID *uuid.UUID) (Diff, error) {
	storageDiff := newStorageDiff(
		b.store.cachePath,
		buildID.String(),
		b.fileType,
		int64(b.header.Metadata.BlockSize),
		b.bucket,
	)

	source, err := b.store.Get(storageDiff)
	if err != nil {
		return nil, fmt.Errorf("failed to get build from store: %w", err)
	}

	return source, nil
}



================================================
File: internal/sandbox/build/cache.go
================================================
package build

import (
	"context"
	"fmt"
	"os"
	"sync"
	"time"

	"github.com/jellydator/ttlcache/v3"
	"go.uber.org/zap"
	"golang.org/x/sys/unix"
)

const DefaultCachePath = "/orchestrator/build"

type deleteDiff struct {
	size   int64
	cancel chan struct{}
}

type DiffStore struct {
	cachePath string
	cache     *ttlcache.Cache[DiffStoreKey, Diff]
	ctx       context.Context
	close     chan struct{}

	// pdSizes is used to keep track of the diff sizes
	// that are scheduled for deletion, as this won't show up in the disk usage.
	pdSizes map[DiffStoreKey]*deleteDiff
	pdMu    sync.RWMutex
	pdDelay time.Duration
}

func NewDiffStore(ctx context.Context, cachePath string, ttl, delay time.Duration, maxUsedPercentage float64) (*DiffStore, error) {
	err := os.MkdirAll(cachePath, 0o755)
	if err != nil {
		return nil, fmt.Errorf("failed to create cache directory: %w", err)
	}

	cache := ttlcache.New(
		ttlcache.WithTTL[DiffStoreKey, Diff](ttl),
	)

	ds := &DiffStore{
		cachePath: cachePath,
		cache:     cache,
		ctx:       ctx,
		close:     make(chan struct{}),
		pdSizes:   make(map[DiffStoreKey]*deleteDiff),
		pdDelay:   delay,
	}

	cache.OnEviction(func(ctx context.Context, reason ttlcache.EvictionReason, item *ttlcache.Item[DiffStoreKey, Diff]) {
		buildData := item.Value()
		// buildData will be deleted by calling buildData.Close()
		defer ds.resetDelete(item.Key())

		err = buildData.Close()
		if err != nil {
			zap.L().Warn("failed to cleanup build data cache for item", zap.Any("item_key", item.Key()), zap.Error(err))
		}
	})

	go cache.Start()
	go ds.startDiskSpaceEviction(maxUsedPercentage)

	return ds, nil
}

type DiffStoreKey string

func GetDiffStoreKey(buildID string, diffType DiffType) DiffStoreKey {
	return DiffStoreKey(fmt.Sprintf("%s/%s", buildID, diffType))
}

func (s *DiffStore) Close() {
	close(s.close)
	s.cache.Stop()
}

func (s *DiffStore) Get(diff Diff) (Diff, error) {
	s.resetDelete(diff.CacheKey())
	source, found := s.cache.GetOrSet(
		diff.CacheKey(),
		diff,
		ttlcache.WithTTL[DiffStoreKey, Diff](ttlcache.DefaultTTL),
	)

	value := source.Value()
	if value == nil {
		return nil, fmt.Errorf("failed to get source from cache: %s", diff.CacheKey())
	}

	if !found {
		err := diff.Init(s.ctx)
		if err != nil {
			return nil, fmt.Errorf("failed to init source: %w", err)
		}
	}

	return value, nil
}

func (s *DiffStore) Add(d Diff) {
	s.resetDelete(d.CacheKey())
	s.cache.Set(d.CacheKey(), d, ttlcache.DefaultTTL)
}

func (s *DiffStore) Has(d Diff) bool {
	return s.cache.Has(d.CacheKey())
}

func (s *DiffStore) startDiskSpaceEviction(threshold float64) {
	getDelay := func(fast bool) time.Duration {
		if fast {
			return time.Microsecond
		} else {
			return time.Second
		}
	}

	timer := time.NewTimer(getDelay(false))
	defer timer.Stop()

	for {
		select {
		case <-s.ctx.Done():
			return
		case <-s.close:
			return
		case <-timer.C:
			dUsed, dTotal, err := diskUsage(s.cachePath)
			if err != nil {
				zap.L().Error("failed to get disk usage", zap.Error(err))
				timer.Reset(getDelay(false))
				continue
			}

			pUsed := s.getPendingDeletesSize()
			used := int64(dUsed) - pUsed
			percentage := float64(used) / float64(dTotal) * 100

			if percentage <= threshold {
				timer.Reset(getDelay(false))
				continue
			}

			succ, err := s.deleteOldestFromCache()
			if err != nil {
				zap.L().Error("failed to delete oldest item from cache", zap.Error(err))
				timer.Reset(getDelay(false))
				continue
			}

			// Item evicted, reset timer to fast check
			timer.Reset(getDelay(succ))
		}
	}
}

func (s *DiffStore) getPendingDeletesSize() int64 {
	s.pdMu.RLock()
	defer s.pdMu.RUnlock()

	var pendingSize int64
	for _, value := range s.pdSizes {
		pendingSize += value.size
	}
	return pendingSize
}

// deleteOldestFromCache deletes the oldest item (smallest TTL) from the cache.
// ttlcache has items in order by TTL
func (s *DiffStore) deleteOldestFromCache() (bool, error) {
	success := false
	var e error
	s.cache.RangeBackwards(func(item *ttlcache.Item[DiffStoreKey, Diff]) bool {
		isDeleted := s.isBeingDeleted(item.Key())
		if isDeleted {
			return true
		}

		sfSize, err := item.Value().FileSize()
		if err != nil {
			e = fmt.Errorf("failed to get diff size: %w", err)
			return false
		}

		s.scheduleDelete(item.Key(), sfSize)

		success = true
		return false
	})

	return success, e
}

func (s *DiffStore) resetDelete(key DiffStoreKey) {
	s.pdMu.Lock()
	defer s.pdMu.Unlock()

	dDiff, f := s.pdSizes[key]
	if !f {
		return
	}

	close(dDiff.cancel)
	delete(s.pdSizes, key)
}

func (s *DiffStore) isBeingDeleted(key DiffStoreKey) bool {
	s.pdMu.RLock()
	defer s.pdMu.RUnlock()

	_, f := s.pdSizes[key]
	return f
}

func (s *DiffStore) scheduleDelete(key DiffStoreKey, dSize int64) {
	s.pdMu.Lock()
	defer s.pdMu.Unlock()

	cancelCh := make(chan struct{})
	s.pdSizes[key] = &deleteDiff{
		size:   dSize,
		cancel: cancelCh,
	}

	// Delay cache (file close/removal) deletion,
	// this is to prevent race conditions with exposed slices,
	// pending data fetching, or data upload
	go (func() {
		select {
		case <-s.ctx.Done():
		case <-cancelCh:
		case <-time.After(s.pdDelay):
			s.cache.Delete(key)
		}
	})()
}

func diskUsage(path string) (uint64, uint64, error) {
	var stat unix.Statfs_t
	err := unix.Statfs(path, &stat)
	if err != nil {
		return 0, 0, fmt.Errorf("failed to get disk stats for path %s: %w", path, err)
	}

	// Available blocks * size per block = available space in bytes
	free := stat.Bavail * uint64(stat.Bsize)
	total := stat.Blocks * uint64(stat.Bsize)
	used := total - free

	return used, total, nil
}



================================================
File: internal/sandbox/build/cache_test.go
================================================
package build

import (
	"context"
	"os"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
)

const (
	tmpBuildCachePrefix = "test-build-cache_"

	blockSize = int64(1024)
)

func newDiff(t *testing.T, cachePath, buildId string, diffType DiffType, blockSize int64) Diff {
	localDiff, err := NewLocalDiffFile(cachePath, buildId, diffType)
	assert.NoError(t, err)

	// Write 100 bytes to the file
	n, err := localDiff.WriteAt(make([]byte, 100), 0)
	assert.NoError(t, err)
	assert.Equal(t, 100, n)

	diff, err := localDiff.ToDiff(blockSize)
	assert.NoError(t, err)

	return diff
}

func createTempDir(t *testing.T) string {
	tempDir, err := os.MkdirTemp("", tmpBuildCachePrefix)
	if err != nil {
		t.Fatalf("Failed to create temp dir: %v", err)
	}
	t.Cleanup(func() {
		os.RemoveAll(tempDir)
	})

	t.Logf("Temp dir: %s\n", tempDir)
	return tempDir
}

func TestNewDiffStore(t *testing.T) {
	cachePath := createTempDir(t)
	ctx, cancel := context.WithCancel(context.Background())
	t.Cleanup(cancel)

	store, err := NewDiffStore(
		ctx,
		cachePath,
		25*time.Hour,
		60*time.Second,
		90.0,
	)
	t.Cleanup(store.Close)

	assert.NoError(t, err)
	assert.NotNil(t, store)
}

func TestDiffStoreTTLEviction(t *testing.T) {
	cachePath := createTempDir(t)
	ctx, cancel := context.WithCancel(context.Background())
	t.Cleanup(cancel)

	ttl := 1 * time.Second
	delay := 60 * time.Second
	store, err := NewDiffStore(
		ctx,
		cachePath,
		ttl,
		delay,
		100.0,
	)
	t.Cleanup(store.Close)
	assert.NoError(t, err)

	// Add an item to the cache
	diff := newDiff(t, cachePath, "build-test-id", Rootfs, blockSize)

	// Add an item to the cache
	store.Add(diff)

	// Expire diff
	time.Sleep(ttl + time.Second)

	found := store.Has(diff)
	assert.False(t, found)
}

func TestDiffStoreRefreshTTLEviction(t *testing.T) {
	cachePath := createTempDir(t)
	ctx, cancel := context.WithCancel(context.Background())
	t.Cleanup(cancel)

	ttl := 1 * time.Second
	delay := 60 * time.Second
	store, err := NewDiffStore(
		ctx,
		cachePath,
		ttl,
		delay,
		100.0,
	)
	t.Cleanup(store.Close)
	assert.NoError(t, err)

	// Add an item to the cache
	diff := newDiff(t, cachePath, "build-test-id", Rootfs, blockSize)

	// Add an item to the cache
	store.Add(diff)

	// Refresh diff expiration
	time.Sleep(ttl / 2)
	_, err = store.Get(diff)
	assert.NoError(t, err)

	// Try to expire diff
	time.Sleep(ttl/2 + time.Microsecond)

	// Is still in cache
	found2 := store.Has(diff)
	assert.True(t, found2)
}

func TestDiffStoreDelayEviction(t *testing.T) {
	cachePath := createTempDir(t)
	ctx, cancel := context.WithCancel(context.Background())
	t.Cleanup(cancel)

	ttl := 60 * time.Second
	delay := 4 * time.Second
	store, err := NewDiffStore(
		ctx,
		cachePath,
		ttl,
		delay,
		0.0,
	)
	t.Cleanup(store.Close)
	assert.NoError(t, err)

	// Add an item to the cache
	diff := newDiff(t, cachePath, "build-test-id", Rootfs, blockSize)

	// Add an item to the cache
	store.Add(diff)

	// Wait for removal trigger of diff
	time.Sleep(2 * time.Second)

	// Verify still in cache
	found := store.Has(diff)
	assert.True(t, found)
	dFound := store.isBeingDeleted(diff.CacheKey())
	assert.True(t, dFound)

	// Wait for complete removal of diff
	time.Sleep(delay)

	found = store.Has(diff)
	assert.False(t, found)
	dFound = store.isBeingDeleted(diff.CacheKey())
	assert.False(t, dFound)
}

func TestDiffStoreDelayEvictionAbort(t *testing.T) {
	cachePath := createTempDir(t)
	ctx, cancel := context.WithCancel(context.Background())
	t.Cleanup(cancel)

	ttl := 60 * time.Second
	delay := 4 * time.Second
	store, err := NewDiffStore(
		ctx,
		cachePath,
		ttl,
		delay,
		0.0,
	)
	t.Cleanup(store.Close)
	assert.NoError(t, err)

	// Add an item to the cache
	diff := newDiff(t, cachePath, "build-test-id", Rootfs, blockSize)

	// Add an item to the cache
	store.Add(diff)

	// Wait for removal trigger of diff
	time.Sleep(delay / 2)

	// Verify still in cache
	found := store.Has(diff)
	assert.True(t, found)
	dFound := store.isBeingDeleted(diff.CacheKey())
	assert.True(t, dFound)

	// Abort removal of diff
	_, err = store.Get(diff)
	assert.NoError(t, err)

	found = store.Has(diff)
	assert.True(t, found)
	dFound = store.isBeingDeleted(diff.CacheKey())
	assert.False(t, dFound)

	// Check insufficient delay cancellation of diff and verify it's still in the cache
	// after the delay period
	time.Sleep(delay/2 + time.Second)
	found = store.Has(diff)
	assert.True(t, found)
}

func TestDiffStoreOldestFromCache(t *testing.T) {
	cachePath := createTempDir(t)
	ctx, cancel := context.WithCancel(context.Background())
	t.Cleanup(cancel)

	ttl := 60 * time.Second
	delay := 4 * time.Second
	store, err := NewDiffStore(
		ctx,
		cachePath,
		ttl,
		delay,
		100.0,
	)
	t.Cleanup(store.Close)
	assert.NoError(t, err)

	// Add items to the cache
	diff := newDiff(t, cachePath, "build-test-id", Rootfs, blockSize)
	store.Add(diff)
	diff2 := newDiff(t, cachePath, "build-test-id-2", Rootfs, blockSize)
	store.Add(diff2)

	found := store.Has(diff)
	assert.True(t, found)

	// Delete oldest item
	_, err = store.deleteOldestFromCache()
	assert.NoError(t, err)

	assert.True(t, store.isBeingDeleted(diff.CacheKey()))
	// Wait for removal trigger of diff
	time.Sleep(delay + time.Second)

	// Verify oldest item is deleted
	found = store.Has(diff)
	assert.False(t, found)

	found = store.Has(diff2)
	assert.True(t, found)

	// Add another item to the cache
	diff3 := newDiff(t, cachePath, "build-test-id-3", Rootfs, blockSize)
	store.Add(diff3)

	// Delete oldest item
	_, err = store.deleteOldestFromCache()
	assert.NoError(t, err)

	assert.True(t, store.isBeingDeleted(diff2.CacheKey()))
	// Wait for removal trigger of diff
	time.Sleep(delay + time.Second)

	// Verify oldest item is deleted
	found = store.Has(diff2)
	assert.False(t, found)

	found = store.Has(diff3)
	assert.True(t, found)
}



================================================
File: internal/sandbox/build/diff.go
================================================
package build

import (
	"context"
	"io"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
)

type DiffType string

type ErrNoDiff struct{}

func (ErrNoDiff) Error() string {
	return "the diff is empty"
}

const (
	Memfile DiffType = storage.MemfileName
	Rootfs  DiffType = storage.RootfsName
)

type Diff interface {
	io.Closer
	io.ReaderAt
	Slice(off, length int64) ([]byte, error)
	CacheKey() DiffStoreKey
	CachePath() (string, error)
	FileSize() (int64, error)
	Init(ctx context.Context) error
}

type NoDiff struct{}

func (n *NoDiff) CachePath() (string, error) {
	return "", ErrNoDiff{}
}

func (n *NoDiff) Slice(off, length int64) ([]byte, error) {
	return nil, ErrNoDiff{}
}

func (n *NoDiff) Close() error {
	return nil
}

func (n *NoDiff) ReadAt(p []byte, off int64) (int, error) {
	return 0, ErrNoDiff{}
}

func (n *NoDiff) FileSize() (int64, error) {
	return 0, ErrNoDiff{}
}

func (n *NoDiff) CacheKey() DiffStoreKey {
	return ""
}

func (n *NoDiff) Init(ctx context.Context) error {
	return ErrNoDiff{}
}



================================================
File: internal/sandbox/build/local_diff.go
================================================
package build

import (
	"context"
	"fmt"
	"os"
	"path/filepath"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/block"
	"github.com/e2b-dev/infra/packages/shared/pkg/id"
)

type LocalDiffFile struct {
	*os.File
	cachePath string
	cacheKey  DiffStoreKey
}

func NewLocalDiffFile(
	basePath string,
	buildId string,
	diffType DiffType,
) (*LocalDiffFile, error) {
	cachePathSuffix := id.Generate()

	cacheFile := fmt.Sprintf("%s-%s-%s", buildId, diffType, cachePathSuffix)
	cachePath := filepath.Join(basePath, cacheFile)

	f, err := os.OpenFile(cachePath, os.O_RDWR|os.O_CREATE, 0o644)
	if err != nil {
		return nil, fmt.Errorf("failed to open file: %w", err)
	}

	return &LocalDiffFile{
		File:      f,
		cachePath: cachePath,
		cacheKey:  GetDiffStoreKey(buildId, diffType),
	}, nil
}

func (f *LocalDiffFile) ToDiff(
	blockSize int64,
) (Diff, error) {
	defer f.Close()

	err := f.Sync()
	if err != nil {
		return nil, fmt.Errorf("failed to sync file: %w", err)
	}

	size, err := f.Stat()
	if err != nil {
		return nil, fmt.Errorf("failed to get file size: %w", err)
	}

	if size.Size() == 0 {
		return &NoDiff{}, nil
	}

	return newLocalDiff(
		f.cacheKey,
		f.cachePath,
		size.Size(),
		blockSize,
	)
}

type localDiff struct {
	size      int64
	blockSize int64
	cacheKey  DiffStoreKey
	cachePath string
	cache     *block.Cache
}

func newLocalDiff(
	cacheKey DiffStoreKey,
	cachePath string,
	size int64,
	blockSize int64,
) (*localDiff, error) {
	cache, err := block.NewCache(size, blockSize, cachePath, true)
	if err != nil {
		return nil, fmt.Errorf("failed to create cache: %w", err)
	}

	return &localDiff{
		size:      size,
		blockSize: blockSize,
		cacheKey:  cacheKey,
		cachePath: cachePath,
		cache:     cache,
	}, nil
}

func (b *localDiff) CachePath() (string, error) {
	return b.cachePath, nil
}

func (b *localDiff) Close() error {
	return b.cache.Close()
}

func (b *localDiff) ReadAt(p []byte, off int64) (int, error) {
	return b.cache.ReadAt(p, off)
}

func (b *localDiff) Slice(off, length int64) ([]byte, error) {
	return b.cache.Slice(off, length)
}

func (b *localDiff) FileSize() (int64, error) {
	return b.cache.FileSize()
}

func (b *localDiff) CacheKey() DiffStoreKey {
	return b.cacheKey
}

func (b *localDiff) Init(ctx context.Context) error {
	return nil
}



================================================
File: internal/sandbox/build/storage_diff.go
================================================
package build

import (
	"context"
	"fmt"
	"io"
	"path/filepath"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/block"
	"github.com/e2b-dev/infra/packages/shared/pkg/id"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

func storagePath(buildId string, diffType DiffType) string {
	return fmt.Sprintf("%s/%s", buildId, diffType)
}

type StorageDiff struct {
	chunker     *utils.SetOnce[*block.Chunker]
	cachePath   string
	cacheKey    DiffStoreKey
	storagePath string
	blockSize   int64
	bucket      *gcs.BucketHandle
}

func newStorageDiff(
	basePath string,
	buildId string,
	diffType DiffType,
	blockSize int64,
	bucket *gcs.BucketHandle,
) *StorageDiff {
	cachePathSuffix := id.Generate()

	storagePath := storagePath(buildId, diffType)
	cacheFile := fmt.Sprintf("%s-%s-%s", buildId, diffType, cachePathSuffix)
	cachePath := filepath.Join(basePath, cacheFile)

	return &StorageDiff{
		storagePath: storagePath,
		cachePath:   cachePath,
		chunker:     utils.NewSetOnce[*block.Chunker](),
		blockSize:   blockSize,
		bucket:      bucket,
		cacheKey:    GetDiffStoreKey(buildId, diffType),
	}
}

func (b *StorageDiff) CacheKey() DiffStoreKey {
	return b.cacheKey
}

func (b *StorageDiff) Init(ctx context.Context) error {
	obj := gcs.NewObject(ctx, b.bucket, b.storagePath)

	size, err := obj.Size()
	if err != nil {
		errMsg := fmt.Errorf("failed to get object size: %w", err)

		b.chunker.SetError(errMsg)

		return errMsg
	}

	chunker, err := block.NewChunker(ctx, size, b.blockSize, obj, b.cachePath)
	if err != nil {
		errMsg := fmt.Errorf("failed to create chunker: %w", err)

		b.chunker.SetError(errMsg)

		return errMsg
	}

	return b.chunker.SetValue(chunker)
}

func (b *StorageDiff) Close() error {
	c, err := b.chunker.Wait()
	if err != nil {
		return err
	}

	return c.Close()
}

func (b *StorageDiff) ReadAt(p []byte, off int64) (int, error) {
	c, err := b.chunker.Wait()
	if err != nil {
		return 0, err
	}

	return c.ReadAt(p, off)
}

func (b *StorageDiff) Slice(off, length int64) ([]byte, error) {
	c, err := b.chunker.Wait()
	if err != nil {
		return nil, err
	}

	return c.Slice(off, length)
}

func (b *StorageDiff) WriteTo(w io.Writer) (int64, error) {
	c, err := b.chunker.Wait()
	if err != nil {
		return 0, err
	}

	return c.WriteTo(w)
}

// The local file might not be synced.
func (b *StorageDiff) CachePath() (string, error) {
	return b.cachePath, nil
}

func (b *StorageDiff) FileSize() (int64, error) {
	c, err := b.chunker.Wait()
	if err != nil {
		return 0, err
	}

	return c.FileSize()
}



================================================
File: internal/sandbox/fc/client_linux.go
================================================
//go:build linux
// +build linux

package fc

import (
	"context"
	"fmt"

	"github.com/firecracker-microvm/firecracker-go-sdk"
	"github.com/go-openapi/strfmt"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/socket"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
	"github.com/e2b-dev/infra/packages/shared/pkg/fc/client"
	"github.com/e2b-dev/infra/packages/shared/pkg/fc/client/operations"
	"github.com/e2b-dev/infra/packages/shared/pkg/fc/models"
)

type apiClient struct {
	client *client.Firecracker
}

func newApiClient(socketPath string) *apiClient {
	client := client.NewHTTPClient(strfmt.NewFormats())

	transport := firecracker.NewUnixSocketTransport(socketPath, nil, false)
	client.SetTransport(transport)

	return &apiClient{
		client: client,
	}
}

func (c *apiClient) loadSnapshot(
	ctx context.Context,
	uffdSocketPath string,
	uffdReady chan struct{},
	snapfile template.File,
) error {
	err := socket.Wait(ctx, uffdSocketPath)
	if err != nil {
		return fmt.Errorf("error waiting for uffd socket: %w", err)
	}

	backendType := models.MemoryBackendBackendTypeUffd
	backend := &models.MemoryBackend{
		BackendPath: &uffdSocketPath,
		BackendType: &backendType,
	}

	snapfilePath := snapfile.Path()
	snapshotConfig := operations.LoadSnapshotParams{
		Context: ctx,
		Body: &models.SnapshotLoadParams{
			ResumeVM:            false,
			EnableDiffSnapshots: false,
			MemBackend:          backend,
			SnapshotPath:        &snapfilePath,
		},
	}

	_, err = c.client.Operations.LoadSnapshot(&snapshotConfig)
	if err != nil {
		return fmt.Errorf("error loading snapshot: %w", err)
	}

	select {
	case <-ctx.Done():
		return fmt.Errorf("context canceled while waiting for uffd ready: %w", ctx.Err())
	case <-uffdReady:
		// Wait for the uffd to be ready to serve requests
	}

	return nil
}

func (c *apiClient) resumeVM(ctx context.Context) error {
	state := models.VMStateResumed
	pauseConfig := operations.PatchVMParams{
		Context: ctx,
		Body: &models.VM{
			State: &state,
		},
	}

	_, err := c.client.Operations.PatchVM(&pauseConfig)
	if err != nil {
		return fmt.Errorf("error resuming vm: %w", err)
	}

	return nil
}

func (c *apiClient) pauseVM(ctx context.Context) error {
	state := models.VMStatePaused
	pauseConfig := operations.PatchVMParams{
		Context: ctx,
		Body: &models.VM{
			State: &state,
		},
	}

	_, err := c.client.Operations.PatchVM(&pauseConfig)
	if err != nil {
		return fmt.Errorf("error pausing vm: %w", err)
	}

	return nil
}

func (c *apiClient) createSnapshot(
	ctx context.Context,
	snapfilePath string,
	memfilePath string,
) error {
	snapshotConfig := operations.CreateSnapshotParams{
		Context: ctx,
		Body: &models.SnapshotCreateParams{
			MemFilePath:  &memfilePath,
			SnapshotPath: &snapfilePath,
		},
	}

	_, err := c.client.Operations.CreateSnapshot(&snapshotConfig)
	if err != nil {
		return fmt.Errorf("error loading snapshot: %w", err)
	}

	return nil
}

func (c *apiClient) setMmds(ctx context.Context, metadata *MmdsMetadata) error {
	mmdsConfig := operations.PutMmdsParams{
		Context: ctx,
		Body:    metadata,
	}

	_, err := c.client.Operations.PutMmds(&mmdsConfig)
	if err != nil {
		return fmt.Errorf("error setting mmds data: %w", err)
	}

	return nil
}



================================================
File: internal/sandbox/fc/client_other.go
================================================
//go:build !linux
// +build !linux

package fc

import (
	"context"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
)

type apiClient struct {
}

func newApiClient(socketPath string) *apiClient {
	return nil
}

func (c *apiClient) loadSnapshot(ctx context.Context, uffdSocketPath string, uffdReady chan struct{}, snapfile template.File) error {
	return nil
}

func (c *apiClient) resumeVM(ctx context.Context) error {
	return nil
}

func (c *apiClient) setMmds(ctx context.Context, metadata *MmdsMetadata) error {
	return nil
}

func (c *apiClient) pauseVM(ctx context.Context) error {
	return nil
}

func (c *apiClient) createSnapshot(ctx context.Context, snapfilePath string, memfilePath string) error {
	return nil
}



================================================
File: internal/sandbox/fc/mmds.go
================================================
package fc

// The metadata serialization should not be changed — it is different from the field names we use here!
type MmdsMetadata struct {
	SandboxId            string `json:"instanceID"`
	TemplateId           string `json:"envID"`
	LogsCollectorAddress string `json:"address"`
	TraceId              string `json:"traceID"`
	TeamId               string `json:"teamID"`
}



================================================
File: internal/sandbox/fc/process.go
================================================
package fc

import (
	"bufio"
	"bytes"
	"context"
	"errors"
	"fmt"
	"os"
	"os/exec"
	"syscall"
	txtTemplate "text/template"

	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/trace"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/network"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/rootfs"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/socket"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/telemetry"
)

const startScript = `mount --make-rprivate / &&
mount -t tmpfs tmpfs {{ .buildDir }} -o X-mount.mkdir &&
mount -t tmpfs tmpfs {{ .buildKernelDir }} -o X-mount.mkdir &&
ln -s {{ .rootfsPath }} {{ .buildRootfsPath }} &&
ln -s {{ .kernelPath }} {{ .buildKernelPath }} &&
ip netns exec {{ .namespaceID }} {{ .firecrackerPath }} --api-sock {{ .firecrackerSocket }}`

var startScriptTemplate = txtTemplate.Must(txtTemplate.New("fc-start").Parse(startScript))

type Process struct {
	uffdReady chan struct{}
	snapfile  template.File

	cmd *exec.Cmd

	metadata *MmdsMetadata

	uffdSocketPath        string
	firecrackerSocketPath string

	rootfs *rootfs.CowDevice
	files  *storage.SandboxFiles

	Exit chan error

	client *apiClient
}

func (p *Process) LoggerMetadata() sbxlogger.SandboxMetadata {
	return sbxlogger.SandboxMetadata{
		SandboxID:  p.metadata.SandboxId,
		TemplateID: p.metadata.TemplateId,
		TeamID:     p.metadata.TeamId,
	}
}

func NewProcess(
	ctx context.Context,
	tracer trace.Tracer,
	slot network.Slot,
	files *storage.SandboxFiles,
	mmdsMetadata *MmdsMetadata,
	snapfile template.File,
	rootfs *rootfs.CowDevice,
	uffdReady chan struct{},
	baseTemplateID string,
) (*Process, error) {
	childCtx, childSpan := tracer.Start(ctx, "initialize-fc", trace.WithAttributes(
		attribute.String("sandbox.id", mmdsMetadata.SandboxId),
		attribute.Int("sandbox.slot.index", slot.Idx),
	))
	defer childSpan.End()

	var fcStartScript bytes.Buffer

	baseBuild := storage.NewTemplateFiles(
		baseTemplateID,
		rootfs.BaseBuildId,
		files.KernelVersion,
		files.FirecrackerVersion,
		files.Hugepages(),
	)

	err := startScriptTemplate.Execute(&fcStartScript, map[string]interface{}{
		"rootfsPath":        files.SandboxCacheRootfsLinkPath(),
		"kernelPath":        files.CacheKernelPath(),
		"buildDir":          baseBuild.BuildDir(),
		"buildRootfsPath":   baseBuild.BuildRootfsPath(),
		"buildKernelPath":   files.BuildKernelPath(),
		"buildKernelDir":    files.BuildKernelDir(),
		"namespaceID":       slot.NamespaceID(),
		"firecrackerPath":   files.FirecrackerPath(),
		"firecrackerSocket": files.SandboxFirecrackerSocketPath(),
	})
	if err != nil {
		return nil, fmt.Errorf("error executing fc start script template: %w", err)
	}

	telemetry.SetAttributes(childCtx,
		attribute.String("sandbox.cmd", fcStartScript.String()),
	)

	_, err = os.Stat(files.FirecrackerPath())
	if err != nil {
		return nil, fmt.Errorf("error stating firecracker binary: %w", err)
	}

	_, err = os.Stat(files.CacheKernelPath())
	if err != nil {
		return nil, fmt.Errorf("error stating kernel file: %w", err)
	}

	cmd := exec.Command(
		"unshare",
		"-pfm",
		"--kill-child",
		"--",
		"bash",
		"-c",
		fcStartScript.String(),
	)

	cmd.SysProcAttr = &syscall.SysProcAttr{
		Setsid: true, // Create a new session
	}

	return &Process{
		Exit:                  make(chan error, 1),
		uffdReady:             uffdReady,
		cmd:                   cmd,
		firecrackerSocketPath: files.SandboxFirecrackerSocketPath(),
		metadata:              mmdsMetadata,
		uffdSocketPath:        files.SandboxUffdSocketPath(),
		snapfile:              snapfile,
		client:                newApiClient(files.SandboxFirecrackerSocketPath()),
		rootfs:                rootfs,
		files:                 files,
	}, nil
}

func (p *Process) Start(
	ctx context.Context,
	tracer trace.Tracer,
) error {
	childCtx, childSpan := tracer.Start(ctx, "start-fc")
	defer childSpan.End()

	stdoutReader, err := p.cmd.StdoutPipe()
	if err != nil {
		return fmt.Errorf("error creating fc stdout pipe: %w", err)
	}

	go func() {
		// The stdout should be closed with the process cmd automatically, as it uses the StdoutPipe()
		// TODO: Better handling of processing all logs before calling wait
		scanner := bufio.NewScanner(stdoutReader)

		for scanner.Scan() {
			line := scanner.Text()

			sbxlogger.I(p).Info("stdout: "+line, zap.String("sandbox_id", p.metadata.SandboxId))
		}

		readerErr := scanner.Err()
		if readerErr != nil {
			sbxlogger.I(p).Error("error reading fc stdout", zap.Error(readerErr))
		}
	}()

	stderrReader, err := p.cmd.StderrPipe()
	if err != nil {
		return fmt.Errorf("error creating fc stderr pipe: %w", err)
	}

	go func() {
		// The stderr should be closed with the process cmd automatically, as it uses the StderrPipe()
		// TODO: Better handling of processing all logs before calling wait
		scanner := bufio.NewScanner(stderrReader)

		for scanner.Scan() {
			line := scanner.Text()
			sbxlogger.I(p).Error("stderr: "+line, zap.String("sandbox_id", p.metadata.SandboxId))
		}

		readerErr := scanner.Err()
		if readerErr != nil {
			sbxlogger.I(p).Error("error reading fc stderr", zap.Error(readerErr))
		}
	}()

	err = os.Symlink("/dev/null", p.files.SandboxCacheRootfsLinkPath())
	if err != nil {
		return fmt.Errorf("error symlinking rootfs: %w", err)
	}

	err = p.cmd.Start()
	if err != nil {
		return fmt.Errorf("error starting fc process: %w", err)
	}

	startCtx, cancelStart := context.WithCancelCause(childCtx)
	defer cancelStart(fmt.Errorf("fc finished starting"))

	go func() {
		waitErr := p.cmd.Wait()
		if waitErr != nil {
			var exitErr *exec.ExitError
			if errors.As(waitErr, &exitErr) {
				// Check if the process was killed by a signal
				if status, ok := exitErr.Sys().(syscall.WaitStatus); ok && status.Signaled() && status.Signal() == syscall.SIGKILL {
					p.Exit <- nil

					return
				}
			}

			errMsg := fmt.Errorf("error waiting for fc process: %w", waitErr)

			p.Exit <- errMsg

			cancelStart(errMsg)

			return
		}

		p.Exit <- nil
	}()

	// Wait for the FC process to start so we can use FC API
	err = socket.Wait(startCtx, p.firecrackerSocketPath)
	if err != nil {
		errMsg := fmt.Errorf("error waiting for fc socket: %w", err)

		fcStopErr := p.Stop()

		return errors.Join(errMsg, fcStopErr)
	}

	device, err := p.rootfs.Path()
	if err != nil {
		return fmt.Errorf("error getting rootfs path: %w", err)
	}

	err = os.Remove(p.files.SandboxCacheRootfsLinkPath())
	if err != nil {
		return fmt.Errorf("error removing rootfs symlink: %w", err)
	}

	err = os.Symlink(device, p.files.SandboxCacheRootfsLinkPath())
	if err != nil {
		return fmt.Errorf("error symlinking rootfs: %w", err)
	}

	err = p.client.loadSnapshot(
		startCtx,
		p.uffdSocketPath,
		p.uffdReady,
		p.snapfile,
	)
	if err != nil {
		fcStopErr := p.Stop()

		return errors.Join(fmt.Errorf("error loading snapshot: %w", err), fcStopErr)
	}

	err = p.client.resumeVM(startCtx)
	if err != nil {
		fcStopErr := p.Stop()

		return errors.Join(fmt.Errorf("error resuming vm: %w", err), fcStopErr)
	}

	err = p.client.setMmds(startCtx, p.metadata)
	if err != nil {
		fcStopErr := p.Stop()

		return errors.Join(fmt.Errorf("error setting mmds: %w", err), fcStopErr)
	}

	telemetry.SetAttributes(
		childCtx,
		attribute.String("sandbox.cmd.dir", p.cmd.Dir),
		attribute.String("sandbox.cmd.path", p.cmd.Path),
	)

	return nil
}

func (p *Process) Pid() (int, error) {
	if p.cmd.Process == nil {
		return 0, fmt.Errorf("fc process not started")
	}

	return p.cmd.Process.Pid, nil
}

func (p *Process) Stop() error {
	if p.cmd.Process == nil {
		return fmt.Errorf("fc process not started")
	}

	err := p.cmd.Process.Kill()
	if err != nil {
		return fmt.Errorf("failed to send KILL to FC process: %w", err)
	}

	return nil
}

func (p *Process) Pause(ctx context.Context, tracer trace.Tracer) error {
	ctx, childSpan := tracer.Start(ctx, "pause-fc")
	defer childSpan.End()

	return p.client.pauseVM(ctx)
}

// VM needs to be paused before creating a snapshot.
func (p *Process) CreateSnapshot(ctx context.Context, tracer trace.Tracer, snapfilePath string, memfilePath string) error {
	ctx, childSpan := tracer.Start(ctx, "create-snapshot-fc")
	defer childSpan.End()

	return p.client.createSnapshot(ctx, snapfilePath, memfilePath)
}



================================================
File: internal/sandbox/nbd/dispatch.go
================================================
package nbd

import (
	"context"
	"encoding/binary"
	"fmt"
	"io"
	"sync"

	"go.uber.org/zap"
)

type Provider interface {
	io.ReaderAt
	io.WriterAt
	Size() (int64, error)
}

const dispatchBufferSize = 4 * 1024 * 1024

// NBD Commands
const (
	NBDCmdRead       = 0
	NBDCmdWrite      = 1
	NBDCmdDisconnect = 2
	NBDCmdFlush      = 3
	NBDCmdTrim       = 4
)

const (
	NBDRequestMagic  = 0x25609513
	NBDResponseMagic = 0x67446698
)

// NBD Request packet
type Request struct {
	Magic  uint32
	Type   uint32
	Handle uint64
	From   uint64
	Length uint32
}

// NBD Response packet
type Response struct {
	Magic  uint32
	Error  uint32
	Handle uint64
}

type Dispatch struct {
	ctx              context.Context
	fp               io.ReadWriteCloser
	responseHeader   []byte
	writeLock        sync.Mutex
	prov             Provider
	pendingResponses sync.WaitGroup
	pendingMu        sync.Mutex
}

func NewDispatch(ctx context.Context, fp io.ReadWriteCloser, prov Provider) *Dispatch {
	d := &Dispatch{
		responseHeader: make([]byte, 16),
		fp:             fp,
		prov:           prov,
		ctx:            ctx,
	}

	binary.BigEndian.PutUint32(d.responseHeader, NBDResponseMagic)
	return d
}

func (d *Dispatch) Wait() {
	d.pendingMu.Lock()
	defer d.pendingMu.Unlock()

	// Wait for any pending responses
	d.pendingResponses.Wait()
}

/**
 * Write a response...
 *
 */
func (d *Dispatch) writeResponse(respError uint32, respHandle uint64, chunk []byte) error {
	d.writeLock.Lock()
	defer d.writeLock.Unlock()

	binary.BigEndian.PutUint32(d.responseHeader[4:], respError)
	binary.BigEndian.PutUint64(d.responseHeader[8:], respHandle)

	_, err := d.fp.Write(d.responseHeader)
	if err != nil {
		return err
	}
	if len(chunk) > 0 {
		_, err = d.fp.Write(chunk)
		if err != nil {
			return err
		}
	}

	return nil
}

/**
 * This dispatches incoming NBD requests sequentially to the provider.
 *
 */
func (d *Dispatch) Handle() error {
	buffer := make([]byte, dispatchBufferSize)
	wp := 0

	request := Request{}

	for {
		n, err := d.fp.Read(buffer[wp:])
		if err != nil {
			return err
		}
		wp += n

		// Now go through processing complete packets
		rp := 0
	process:
		for {

			// If the context has been cancelled, quit
			select {
			case <-d.ctx.Done():
				return d.ctx.Err()
			default:
			}

			// Make sure we have a complete header
			if wp-rp >= 28 {
				// We can read the neader...

				header := buffer[rp : rp+28]
				request.Magic = binary.BigEndian.Uint32(header)
				request.Type = binary.BigEndian.Uint32(header[4:8])
				request.Handle = binary.BigEndian.Uint64(header[8:16])
				request.From = binary.BigEndian.Uint64(header[16:24])
				request.Length = binary.BigEndian.Uint32(header[24:28])

				if request.Magic != NBDRequestMagic {
					return fmt.Errorf("received invalid MAGIC")
				}

				switch request.Type {
				case NBDCmdDisconnect:
					return nil // All done
				case NBDCmdFlush:
					return fmt.Errorf("not supported: Flush")
				case NBDCmdRead:
					rp += 28
					err := d.cmdRead(request.Handle, request.From, request.Length)
					if err != nil {
						return err
					}
				case NBDCmdWrite:
					rp += 28
					if wp-rp < int(request.Length) {
						rp -= 28
						break process // We don't have enough data yet... Wait for next read
					}
					data := make([]byte, request.Length)
					copy(data, buffer[rp:rp+int(request.Length)])
					rp += int(request.Length)
					err := d.cmdWrite(request.Handle, request.From, data)
					if err != nil {
						return err
					}
				case NBDCmdTrim:
					rp += 28
					err = d.cmdTrim(request.Handle, request.From, request.Length)
					if err != nil {
						return err
					}
				default:
					return fmt.Errorf("nbd not implemented %d", request.Type)
				}

			} else {
				break // Try again when we have more data...
			}
		}
		// Now we need to move any partial to the start
		if rp != 0 && rp != wp {
			copy(buffer, buffer[rp:wp])
		}
		wp -= rp
	}
}

func (d *Dispatch) cmdRead(cmdHandle uint64, cmdFrom uint64, cmdLength uint32) error {
	d.pendingMu.Lock()
	d.pendingResponses.Add(1)
	d.pendingMu.Unlock()

	performRead := func(handle uint64, from uint64, length uint32) error {
		errchan := make(chan error)
		data := make([]byte, length)

		go func() {
			_, e := d.prov.ReadAt(data, int64(from))
			errchan <- e
		}()

		// Wait until either the ReadAt completed, or our context is cancelled...
		var e error
		select {
		case <-d.ctx.Done():
			e = d.ctx.Err()
		case e = <-errchan:
		}

		errorValue := uint32(0)
		if e != nil {
			errorValue = 1
			data = make([]byte, 0) // If there was an error, don't send data
		}
		return d.writeResponse(errorValue, handle, data)
	}

	go func() {
		err := performRead(cmdHandle, cmdFrom, cmdLength)
		if err != nil {
			zap.L().Error("nbd error cmd read", zap.Error(err))
		}

		d.pendingResponses.Done()
	}()

	return nil
}

func (d *Dispatch) cmdWrite(cmdHandle uint64, cmdFrom uint64, cmdData []byte) error {
	d.pendingMu.Lock()
	d.pendingResponses.Add(1)
	d.pendingMu.Unlock()

	go func() {
		errchan := make(chan error)
		go func() {
			_, e := d.prov.WriteAt(cmdData, int64(cmdFrom))
			errchan <- e
		}()

		// Wait until either the WriteAt completed, or our context is cancelled...
		var e error
		select {
		case <-d.ctx.Done():
			e = d.ctx.Err()
		case e = <-errchan:
		}

		errorValue := uint32(0)
		if e != nil {
			errorValue = 1
		}
		err := d.writeResponse(errorValue, cmdHandle, []byte{})
		if err != nil {
			zap.L().Error("nbd error cmd write", zap.Error(err))
		}

		d.pendingResponses.Done()
	}()

	return nil
}

/**
 * cmdTrim
 *
 */
func (d *Dispatch) cmdTrim(handle uint64, _ uint64, _ uint32) error {
	// TODO: Ask the provider
	/*
		e := d.prov.Trim(from, length)
		if e != storage.StorageError_SUCCESS {
			err := d.writeResponse(1, handle, []byte{})
			if err != nil {
				return err
			}
		} else {
	*/
	err := d.writeResponse(0, handle, []byte{})
	if err != nil {
		return err
	}
	//	}
	return nil
}



================================================
File: internal/sandbox/nbd/path_direct_linux.go
================================================
//go:build linux
// +build linux

package nbd

import (
	"context"
	"net"
	"os"
	"strings"
	"syscall"
	"time"

	"github.com/Merovius/nbd/nbdnl"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/block"
)

type DirectPathMount struct {
	Backend     block.Device
	ctx         context.Context
	dispatcher  *Dispatch
	conn        net.Conn
	deviceIndex uint32
	blockSize   uint64
	cancelfn    context.CancelFunc
	devicePool  *DevicePool
}

func NewDirectPathMount(b block.Device, devicePool *DevicePool) *DirectPathMount {
	ctx, cancelfn := context.WithCancel(context.Background())

	return &DirectPathMount{
		Backend:    b,
		ctx:        ctx,
		cancelfn:   cancelfn,
		blockSize:  4096,
		devicePool: devicePool,
	}
}

func (d *DirectPathMount) Open(ctx context.Context) (uint32, error) {
	size, err := d.Backend.Size()
	if err != nil {
		return 0, err
	}

	for {
		d.deviceIndex, err = d.devicePool.GetDevice(ctx)
		if err != nil {
			return 0, err
		}

		// Create the socket pairs
		sockPair, err := syscall.Socketpair(syscall.AF_UNIX, syscall.SOCK_STREAM, 0)
		if err != nil {
			return 0, err
		}

		client := os.NewFile(uintptr(sockPair[0]), "client")
		server := os.NewFile(uintptr(sockPair[1]), "server")
		d.conn, err = net.FileConn(server)

		if err != nil {
			return 0, err
		}
		server.Close()

		d.dispatcher = NewDispatch(d.ctx, d.conn, d.Backend)
		// Start reading commands on the socket and dispatching them to our provider
		go func() {
			handleErr := d.dispatcher.Handle()
			if handleErr != nil {
				zap.L().Error("error handling NBD commands", zap.Error(handleErr))
			}
		}()

		var opts []nbdnl.ConnectOption
		opts = append(opts, nbdnl.WithBlockSize(d.blockSize))
		opts = append(opts, nbdnl.WithTimeout(5*time.Second))
		opts = append(opts, nbdnl.WithDeadconnTimeout(5*time.Second))

		serverFlags := nbdnl.FlagHasFlags | nbdnl.FlagCanMulticonn

		idx, err := nbdnl.Connect(d.deviceIndex, []*os.File{client}, uint64(size), 0, serverFlags, opts...)
		if err == nil {
			d.deviceIndex = idx
			break
		}

		// Sometimes (rare), there seems to be a BADF error here. Lets just retry for now...
		// Close things down and try again...
		_ = client.Close()

		connErr := d.conn.Close()
		if connErr != nil {
			zap.L().Error("error closing conn", zap.Error(connErr))
		}

		releaseErr := d.devicePool.ReleaseDevice(d.deviceIndex)
		if releaseErr != nil {
			zap.L().Error("error releasing device", zap.Error(releaseErr))
		}

		d.deviceIndex = 0

		if strings.Contains(err.Error(), "invalid argument") {
			return 0, err
		}

		time.Sleep(25 * time.Millisecond)
	}

	// Wait until it's connected...
	for {
		select {
		case <-d.ctx.Done():
			return 0, d.ctx.Err()
		default:
		}

		s, err := nbdnl.Status(d.deviceIndex)
		if err == nil && s.Connected {
			break
		}

		time.Sleep(100 * time.Nanosecond)
	}

	return d.deviceIndex, nil
}

func (d *DirectPathMount) Close() error {
	// First cancel the context, which will stop waiting on pending readAt/writeAt...
	d.ctx.Done()

	// Now wait for any pending responses to be sent
	if d.dispatcher != nil {
		d.dispatcher.Wait()
	}

	// Now ask to disconnect
	err := nbdnl.Disconnect(d.deviceIndex)
	if err != nil {
		return err
	}

	// Close all the socket pairs...
	err = d.conn.Close()
	if err != nil {
		return err
	}

	ctx, cancel := context.WithTimeout(context.Background(), 4*time.Second)
	defer cancel()
	// Wait until it's completely disconnected...
	for {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		s, err := nbdnl.Status(d.deviceIndex)
		if err == nil && !s.Connected {
			break
		}

		time.Sleep(100 * time.Nanosecond)
	}

	return nil
}



================================================
File: internal/sandbox/nbd/path_direct_other.go
================================================
//go:build !linux
// +build !linux

package nbd

import (
	"context"
	"errors"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/block"
)

type DirectPathMount struct {
	Backend block.Device
}

func NewDirectPathMount(b block.Device, devicePool *DevicePool) *DirectPathMount {
	return nil
}

func (d *DirectPathMount) Open(ctx context.Context) (uint32, error) {
	return 0, errors.New("platform does not support direct path mount")
}

func (d *DirectPathMount) Close() error {
	return errors.New("platform does not support direct path mount")
}



================================================
File: internal/sandbox/nbd/pool.go
================================================
package nbd

import (
	"context"
	"errors"
	"fmt"
	"os"
	"regexp"
	"strconv"
	"strings"
	"sync"

	"github.com/bits-and-blooms/bitset"
	"go.opentelemetry.io/otel/metric"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/shared/pkg/meters"
)

// maxSlotsReady is the number of slots that are ready to be used.
const maxSlotsReady = 64

// ErrNoFreeSlots is returned when there are no free slots.
// You can retry the request after some time.
type ErrNoFreeSlots struct{}

func (ErrNoFreeSlots) Error() string {
	return "no free slots"
}

// ErrDeviceInUse is returned when the device that you wanted to release is still in use.
// You can retry the request after ensuring that the device is not in use anymore.
type ErrDeviceInUse struct{}

func (ErrDeviceInUse) Error() string {
	return "device in use"
}

type (
	// DevicePath is the path to the nbd device.
	DevicePath = string
	// DeviceSlot is the slot number of the nbd device.
	DeviceSlot = uint32
)

// DevicePool requires the nbd module to be loaded before running.
//
// Use `sudo modprobe nbd nbds_max=4096` to set the max number of devices to 4096, which is a good default for now.
type DevicePool struct {
	ctx context.Context
	// We use the bitset to speedup the free device lookup.
	usedSlots *bitset.BitSet
	mu        sync.Mutex

	slots chan DeviceSlot

	slotCounter metric.Int64UpDownCounter
}

func NewDevicePool() (*DevicePool, error) {
	maxDevices, err := getMaxDevices()
	if err != nil {
		return nil, fmt.Errorf("failed to get max devices: %w", err)
	}

	if maxDevices == 0 {
		return nil, errors.New("max devices is 0")
	}

	counter, err := meters.GetUpDownCounter(meters.NBDkSlotSReadyPoolCounterMeterName)
	if err != nil {
		return nil, fmt.Errorf("failed to get slot pool counter: %w", err)
	}

	pool := &DevicePool{
		ctx:         context.Background(),
		usedSlots:   bitset.New(maxDevices),
		slots:       make(chan DeviceSlot, maxSlotsReady),
		slotCounter: counter,
	}

	go func() {
		err = pool.Populate()
		if err != nil {
			zap.L().Fatal("failed during populating device pool", zap.Error(err))
		}
	}()

	return pool, nil
}

func getMaxDevices() (uint, error) {
	data, err := os.ReadFile("/sys/module/nbd/parameters/nbds_max")

	if errors.Is(err, os.ErrNotExist) {
		return 0, nil
	}

	if err != nil {
		return 0, fmt.Errorf("failed to read nbds_max: %w", err)
	}

	maxDevices, err := strconv.ParseUint(strings.TrimSpace(string(data)), 10, 0)
	if err != nil {
		return 0, fmt.Errorf("failed to parse nbds_max: %w", err)
	}

	return uint(maxDevices), nil
}

func (d *DevicePool) Populate() error {
	defer close(d.slots)

	for {
		select {
		case <-d.ctx.Done():
			return d.ctx.Err()
		default:
			device, err := d.getFreeDeviceSlot()
			if err != nil {
				zap.L().Error("[nbd pool]: failed to create network", zap.Error(err))

				continue
			}

			d.slotCounter.Add(d.ctx, 1)
			d.slots <- *device
		}
	}
}

// The following files and resources are useful for checking if the device is free:
// /sys/devices/virtual/block/nbdX/pid
// /sys/block/nbdX/pid
// /sys/block/nbdX/size
// nbd-client -c
// https://unix.stackexchange.com/questions/33508/check-which-network-block-devices-are-in-use
// https://superuser.com/questions/919895/how-to-get-a-list-of-connected-nbd-devices-on-ubuntu
// https://github.com/NetworkBlockDevice/nbd/blob/17043b068f4323078637314258158aebbfff0a6c/nbd-client.c#L254
func (d *DevicePool) isDeviceFree(slot DeviceSlot) (bool, error) {
	// Continue only if the file doesn't exist.
	pidFile := fmt.Sprintf("/sys/block/nbd%d/pid", slot)

	_, err := os.Stat(pidFile)
	if err == nil {
		// File is present, therefore the device is in use.
		return false, nil
	}

	if !os.IsNotExist(err) {
		// Some other error occurred.
		return false, fmt.Errorf("failed to stat pid file: %w", err)
	}

	sizeFile := fmt.Sprintf("/sys/block/nbd%d/size", slot)

	data, err := os.ReadFile(sizeFile)
	if err != nil {
		return false, fmt.Errorf("failed to read size file: %w", err)
	}

	sizeStr := strings.TrimSpace(string(data))

	size, err := strconv.ParseUint(sizeStr, 10, 64)
	if err != nil {
		return false, fmt.Errorf("failed to parse size: %w", err)
	}

	return size == 0, nil
}

func (d *DevicePool) getMaybeEmptySlot(start DeviceSlot) (DeviceSlot, func(), bool) {
	d.mu.Lock()
	defer d.mu.Unlock()

	slot, ok := d.usedSlots.NextClear(uint(start))

	if !ok {
		return 0, func() {}, false
	}

	d.usedSlots.Set(slot)

	return uint32(slot), func() {
		d.mu.Lock()
		defer d.mu.Unlock()

		d.usedSlots.Clear(slot)
	}, true
}

// Get a free device slot.
func (d *DevicePool) getFreeDeviceSlot() (*DeviceSlot, error) {
	start := uint32(0)

	for {
		slot, cleanup, ok := d.getMaybeEmptySlot(start)

		if !ok {
			cleanup()

			return nil, ErrNoFreeSlots{}
		}

		free, err := d.isDeviceFree(slot)
		if err != nil {
			cleanup()

			return nil, fmt.Errorf("failed to check if device is free: %w", err)
		}

		if !free {
			// We clear the slot even though it is not free to prevent accidental accumulation of slots.
			cleanup()

			// We increment the start to avoid infinite loops.
			start++

			continue
		}

		return &slot, nil
	}
}

// Get device slot if there is one available.
func (d *DevicePool) GetDevice(ctx context.Context) (DeviceSlot, error) {
	for {
		select {
		case <-ctx.Done():
			return 0, ctx.Err()
		default:
		}

		slot := <-d.slots
		d.slotCounter.Add(d.ctx, -1)

		return slot, nil
	}
}

// ReleaseDevice will return an error if the device is not free and not release the slot — you can retry.
func (d *DevicePool) ReleaseDevice(idx DeviceSlot) error {
	free, err := d.isDeviceFree(idx)
	if err != nil {
		return fmt.Errorf("failed to check if device is free: %w", err)
	}

	if !free {
		return ErrDeviceInUse{}
	}

	d.mu.Lock()
	d.usedSlots.Clear(uint(idx))
	d.mu.Unlock()

	return nil
}

func GetDevicePath(slot DeviceSlot) DevicePath {
	return fmt.Sprintf("/dev/nbd%d", slot)
}

var reSlot = regexp.MustCompile(`^/dev/nbd(\d+)$`)

func GetDeviceSlot(path DevicePath) (DeviceSlot, error) {
	matches := reSlot.FindStringSubmatch(path)
	if len(matches) != 2 {
		return 0, fmt.Errorf("invalid nbd path: %s", path)
	}

	slot, err := strconv.ParseUint(matches[1], 10, 0)
	if err != nil {
		return 0, fmt.Errorf("failed to parse slot from path: %w", err)
	}

	return DeviceSlot(slot), nil
}



================================================
File: internal/sandbox/network/blocking_rules.go
================================================
package network

import (
	"fmt"
	"github.com/coreos/go-iptables/iptables"
)

var blockedRanges = []string{
	"10.0.0.0/8",
	"169.254.0.0/16",
	"192.168.0.0/16",
	"172.16.0.0/12",
}

func getBlockingRule(slot *Slot, ipRange string) []string {
	return []string{"-p", "all", "-i", slot.TapName(), "-d", ipRange, "-j", "DROP"}
}

func getAllowRule(slot *Slot) []string {
	return []string{"-p", "tcp", "-i", slot.TapName(), "-m", "conntrack", "--ctstate", "ESTABLISHED,RELATED", "-j", "ACCEPT"}
}

func (s *Slot) addBlockingRules(tables *iptables.IPTables) error {
	for _, ipRange := range blockedRanges {
		rule := getBlockingRule(s, ipRange)

		err := tables.Append("filter", "FORWARD", rule...)
		if err != nil {
			return fmt.Errorf("error adding blocking rule: %w", err)
		}
	}

	allowRule := getAllowRule(s)

	err := tables.Insert("filter", "FORWARD", 1, allowRule...)
	if err != nil {
		return fmt.Errorf("error adding response rule: %w", err)
	}

	return nil
}



================================================
File: internal/sandbox/network/host_linux.go
================================================
//go:build linux
// +build linux

package network

import (
	"fmt"

	"github.com/vishvananda/netlink"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

// Host loopback interface name
const loopbackInterface = "lo"

// Host default gateway name
var defaultGateway = utils.Must(getDefaultGateway())

//	func getDefaultGateway() (string, error) {
//		route, err := exec.Command(
//			"sh",
//			"-c",
//			"ip route show default | awk '{print $5}'",
//		).Output()
//		if err != nil {
//			return "", fmt.Errorf("error fetching default gateway: %w", err)
//		}
//
//		return string(route), nil
//	}
func getDefaultGateway() (string, error) {
	routes, err := netlink.RouteList(nil, netlink.FAMILY_ALL)
	if err != nil {
		return "", fmt.Errorf("error fetching routes: %w", err)
	}

	for _, route := range routes {
		// 0.0.0.0/0
		if route.Dst.String() == "0.0.0.0/0" && route.Gw != nil {
			zap.L().Info("default gateway", zap.String("gateway", route.Gw.String()))

			link, linkErr := netlink.LinkByIndex(route.LinkIndex)

			if linkErr != nil {
				return "", fmt.Errorf("error fetching interface for default gateway: %w", linkErr)
			}

			return link.Attrs().Name, nil
		}
	}

	return "", fmt.Errorf("cannot find default gateway")
}



================================================
File: internal/sandbox/network/host_other.go
================================================
//go:build !linux
// +build !linux

package network

// Host loopback interface name
const loopbackInterface = "lo"



================================================
File: internal/sandbox/network/network_linux.go
================================================
//go:build linux
// +build linux

package network

import (
	"errors"
	"fmt"
	"net"
	"runtime"

	"github.com/coreos/go-iptables/iptables"
	"github.com/vishvananda/netlink"
	"github.com/vishvananda/netns"
	"go.uber.org/zap"
)

func (s *Slot) CreateNetwork() error {
	// Prevent thread changes so we can safely manipulate with namespaces
	runtime.LockOSThread()
	defer runtime.UnlockOSThread()

	// Save the original (host) namespace and restore it upon function exit
	hostNS, err := netns.Get()
	if err != nil {
		return fmt.Errorf("cannot get current (host) namespace: %w", err)
	}

	defer func() {
		err = netns.Set(hostNS)
		if err != nil {
			zap.L().Error("error resetting network namespace back to the host namespace", zap.Error(err))
		}

		err = hostNS.Close()
		if err != nil {
			zap.L().Error("error closing host network namespace", zap.Error(err))
		}
	}()

	// Create NS for the sandbox
	ns, err := netns.NewNamed(s.NamespaceID())
	if err != nil {
		return fmt.Errorf("cannot create new namespace: %w", err)
	}

	defer ns.Close()

	// Create the Veth and Vpeer
	vethAttrs := netlink.NewLinkAttrs()
	vethAttrs.Name = s.VethName()
	veth := &netlink.Veth{
		LinkAttrs: vethAttrs,
		PeerName:  s.VpeerName(),
	}

	err = netlink.LinkAdd(veth)
	if err != nil {
		return fmt.Errorf("error creating veth device: %w", err)
	}

	vpeer, err := netlink.LinkByName(s.VpeerName())
	if err != nil {
		return fmt.Errorf("error finding vpeer: %w", err)
	}

	err = netlink.LinkSetUp(vpeer)
	if err != nil {
		return fmt.Errorf("error setting vpeer device up: %w", err)
	}

	ip, ipNet, err := net.ParseCIDR(s.VpeerCIDR())
	if err != nil {
		return fmt.Errorf("error parsing vpeer CIDR: %w", err)
	}

	err = netlink.AddrAdd(vpeer, &netlink.Addr{
		IPNet: &net.IPNet{
			IP:   ip,
			Mask: ipNet.Mask,
		},
	})
	if err != nil {
		return fmt.Errorf("error adding vpeer device address: %w", err)
	}

	// Move Veth device to the host NS
	err = netlink.LinkSetNsFd(veth, int(hostNS))
	if err != nil {
		return fmt.Errorf("error moving veth device to the host namespace: %w", err)
	}

	err = netns.Set(hostNS)
	if err != nil {
		return fmt.Errorf("error setting network namespace: %w", err)
	}

	vethInHost, err := netlink.LinkByName(s.VethName())
	if err != nil {
		return fmt.Errorf("error finding veth: %w", err)
	}

	err = netlink.LinkSetUp(vethInHost)
	if err != nil {
		return fmt.Errorf("error setting veth device up: %w", err)
	}

	ip, ipNet, err = net.ParseCIDR(s.VethCIDR())
	if err != nil {
		return fmt.Errorf("error parsing veth CIDR: %w", err)
	}

	err = netlink.AddrAdd(vethInHost, &netlink.Addr{
		IPNet: &net.IPNet{
			IP:   ip,
			Mask: ipNet.Mask,
		},
	})
	if err != nil {
		return fmt.Errorf("error adding veth device address: %w", err)
	}

	err = netns.Set(ns)
	if err != nil {
		return fmt.Errorf("error setting network namespace to %s: %w", ns.String(), err)
	}

	// Create Tap device for FC in NS
	tapAttrs := netlink.NewLinkAttrs()
	tapAttrs.Name = s.TapName()
	tapAttrs.Namespace = ns
	tap := &netlink.Tuntap{
		Mode:      netlink.TUNTAP_MODE_TAP,
		LinkAttrs: tapAttrs,
	}

	err = netlink.LinkAdd(tap)
	if err != nil {
		return fmt.Errorf("error creating tap device: %w", err)
	}

	err = netlink.LinkSetUp(tap)
	if err != nil {
		return fmt.Errorf("error setting tap device up: %w", err)
	}

	ip, ipNet, err = net.ParseCIDR(s.TapCIDR())
	if err != nil {
		return fmt.Errorf("error parsing tap CIDR: %w", err)
	}

	err = netlink.AddrAdd(tap, &netlink.Addr{
		IPNet: &net.IPNet{
			IP:   ip,
			Mask: ipNet.Mask,
		},
	})
	if err != nil {
		return fmt.Errorf("error setting address of the tap device: %w", err)
	}

	// Set NS lo device up
	lo, err := netlink.LinkByName(loopbackInterface)
	if err != nil {
		return fmt.Errorf("error finding lo: %w", err)
	}

	err = netlink.LinkSetUp(lo)
	if err != nil {
		return fmt.Errorf("error setting lo device up: %w", err)
	}

	// Add NS default route
	err = netlink.RouteAdd(&netlink.Route{
		Scope: netlink.SCOPE_UNIVERSE,
		Gw:    net.ParseIP(s.VethIP()),
	})
	if err != nil {
		return fmt.Errorf("error adding default NS route: %w", err)
	}

	tables, err := iptables.New()
	if err != nil {
		return fmt.Errorf("error initializing iptables: %w", err)
	}

	// Add NAT routing rules to NS
	err = tables.Append("nat", "POSTROUTING", "-o", s.VpeerName(), "-s", s.NamespaceIP(), "-j", "SNAT", "--to", s.HostIP())
	if err != nil {
		return fmt.Errorf("error creating postrouting rule to vpeer: %w", err)
	}

	err = tables.Append("nat", "PREROUTING", "-i", s.VpeerName(), "-d", s.HostIP(), "-j", "DNAT", "--to", s.NamespaceIP())
	if err != nil {
		return fmt.Errorf("error creating postrouting rule from vpeer: %w", err)
	}

	err = s.addBlockingRules(tables)
	if err != nil {
		return fmt.Errorf("error adding blocking rules: %w", err)
	}

	// Go back to original namespace
	err = netns.Set(hostNS)
	if err != nil {
		return fmt.Errorf("error setting network namespace to %s: %w", hostNS.String(), err)
	}

	// Add routing from host to FC namespace
	_, ipNet, err = net.ParseCIDR(s.HostCIDR())
	if err != nil {
		return fmt.Errorf("error parsing host snapshot CIDR: %w", err)
	}

	err = netlink.RouteAdd(&netlink.Route{
		Gw:  net.ParseIP(s.VpeerIP()),
		Dst: ipNet,
	})
	if err != nil {
		return fmt.Errorf("error adding route from host to FC: %w", err)
	}

	// Add host forwarding rules
	err = tables.Append("filter", "FORWARD", "-i", s.VethName(), "-o", defaultGateway, "-j", "ACCEPT")
	if err != nil {
		return fmt.Errorf("error creating forwarding rule to default gateway: %w", err)
	}

	err = tables.Append("filter", "FORWARD", "-i", defaultGateway, "-o", s.VethName(), "-j", "ACCEPT")
	if err != nil {
		return fmt.Errorf("error creating forwarding rule from default gateway: %w", err)
	}

	// Add host postrouting rules
	err = tables.Append("nat", "POSTROUTING", "-s", s.HostCIDR(), "-o", defaultGateway, "-j", "MASQUERADE")
	if err != nil {
		return fmt.Errorf("error creating postrouting rule: %w", err)
	}

	return nil
}

func (s *Slot) RemoveNetwork() error {
	var errs []error

	tables, err := iptables.New()
	if err != nil {
		errs = append(errs, fmt.Errorf("error initializing iptables: %w", err))
	} else {
		// Delete host forwarding rules
		err = tables.Delete("filter", "FORWARD", "-i", s.VethName(), "-o", defaultGateway, "-j", "ACCEPT")
		if err != nil {
			errs = append(errs, fmt.Errorf("error deleting host forwarding rule to default gateway: %w", err))
		}

		err = tables.Delete("filter", "FORWARD", "-i", defaultGateway, "-o", s.VethName(), "-j", "ACCEPT")
		if err != nil {
			errs = append(errs, fmt.Errorf("error deleting host forwarding rule from default gateway: %w", err))
		}

		// Delete host postrouting rules
		err = tables.Delete("nat", "POSTROUTING", "-s", s.HostCIDR(), "-o", defaultGateway, "-j", "MASQUERADE")
		if err != nil {
			errs = append(errs, fmt.Errorf("error deleting host postrouting rule: %w", err))
		}
	}

	// Delete routing from host to FC namespace
	_, ipNet, err := net.ParseCIDR(s.HostCIDR())
	if err != nil {
		errs = append(errs, fmt.Errorf("error parsing host snapshot CIDR: %w", err))
	} else {
		err = netlink.RouteDel(&netlink.Route{
			Gw:  net.ParseIP(s.VpeerIP()),
			Dst: ipNet,
		})
		if err != nil {
			errs = append(errs, fmt.Errorf("error deleting route from host to FC: %w", err))
		}
	}

	// Delete veth device
	// We explicitly delete the veth device from the host namespace because even though deleting
	// is deleting the device there may be a race condition when creating a new veth device with
	// the same name immediately after deleting the namespace.
	veth, err := netlink.LinkByName(s.VethName())
	if err != nil {
		errs = append(errs, fmt.Errorf("error finding veth: %w", err))
	} else {
		err = netlink.LinkDel(veth)
		if err != nil {
			errs = append(errs, fmt.Errorf("error deleting veth device: %w", err))
		}
	}

	err = netns.DeleteNamed(s.NamespaceID())
	if err != nil {
		errs = append(errs, fmt.Errorf("error deleting namespace: %w", err))
	}

	return errors.Join(errs...)
}



================================================
File: internal/sandbox/network/network_other.go
================================================
//go:build !linux
// +build !linux

package network

import (
	"errors"
)

func (s *Slot) CreateNetwork() error {
	return errors.New("platform does not support network creation")
}

func (s *Slot) RemoveNetwork() error {
	return errors.New("platform does not support network removal")
}



================================================
File: internal/sandbox/network/pool.go
================================================
package network

import (
	"context"
	"errors"
	"fmt"

	"go.opentelemetry.io/otel/metric"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/shared/pkg/meters"
	"github.com/e2b-dev/infra/packages/shared/pkg/telemetry"
)

const (
	NewSlotsPoolSize    = 32
	ReusedSlotsPoolSize = 100
)

type Pool struct {
	ctx    context.Context
	cancel context.CancelFunc

	newSlots          chan Slot
	reusedSlots       chan Slot
	newSlotCounter    metric.Int64UpDownCounter
	reusedSlotCounter metric.Int64UpDownCounter

	slotStorage Storage
}

func NewPool(ctx context.Context, newSlotsPoolSize, reusedSlotsPoolSize int, clientID string) (*Pool, error) {
	newSlots := make(chan Slot, newSlotsPoolSize-1)
	reusedSlots := make(chan Slot, reusedSlotsPoolSize)

	newSlotCounter, err := meters.GetUpDownCounter(meters.NewNetworkSlotSPoolCounterMeterName)
	if err != nil {
		return nil, fmt.Errorf("failed to create new slot counter: %w", err)
	}

	reusedSlotsCounter, err := meters.GetUpDownCounter(meters.ReusedNetworkSlotSPoolCounterMeterName)
	if err != nil {
		return nil, fmt.Errorf("failed to create reused slot counter: %w", err)
	}

	slotStorage, err := NewStorage(slotsSize, clientID)
	if err != nil {
		return nil, fmt.Errorf("failed to create slot storage: %w", err)
	}

	ctx, cancel := context.WithCancel(ctx)
	pool := &Pool{
		newSlots:          newSlots,
		reusedSlots:       reusedSlots,
		newSlotCounter:    newSlotCounter,
		reusedSlotCounter: reusedSlotsCounter,
		ctx:               ctx,
		cancel:            cancel,
		slotStorage:       slotStorage,
	}

	go func() {
		err := pool.populate(ctx)
		if err != nil {
			zap.L().Fatal("error when populating network slot pool", zap.Error(err))
		}
	}()

	return pool, nil
}

func (p *Pool) createNetworkSlot() (*Slot, error) {
	ips, err := p.slotStorage.Acquire()
	if err != nil {
		return nil, fmt.Errorf("failed to create network: %w", err)
	}

	err = ips.CreateNetwork()
	if err != nil {
		releaseErr := p.slotStorage.Release(ips)
		err = errors.Join(err, releaseErr)

		return nil, fmt.Errorf("failed to create network: %w", err)
	}

	return ips, nil
}

func (p *Pool) populate(ctx context.Context) error {
	for {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
			slot, err := p.createNetworkSlot()
			if err != nil {
				zap.L().Error("[network slot pool]: failed to create network", zap.Error(err))

				continue
			}

			p.newSlotCounter.Add(ctx, 1)
			p.newSlots <- *slot
		}
	}
}

func (p *Pool) Get(ctx context.Context) (Slot, error) {
	select {
	case slot := <-p.reusedSlots:
		p.reusedSlotCounter.Add(ctx, -1)
		telemetry.ReportEvent(ctx, "reused network slot")

		return slot, nil
	default:
		select {
		case <-ctx.Done():
			return Slot{}, ctx.Err()
		case slot := <-p.newSlots:
			p.newSlotCounter.Add(ctx, -1)
			telemetry.ReportEvent(ctx, "new network slot")

			return slot, nil
		}
	}
}

func (p *Pool) Return(slot Slot) error {
	select {
	case p.reusedSlots <- slot:
		p.reusedSlotCounter.Add(context.Background(), 1)
	default:
		err := p.cleanup(slot)
		if err != nil {
			return fmt.Errorf("failed to return slot '%d': %w", slot.Idx, err)
		}
	}

	return nil
}

func (p *Pool) cleanup(slot Slot) error {
	var errs []error

	err := slot.RemoveNetwork()
	if err != nil {
		errs = append(errs, fmt.Errorf("cannot remove network when releasing slot '%d': %w", slot.Idx, err))
	}

	err = p.slotStorage.Release(&slot)
	if err != nil {
		errs = append(errs, fmt.Errorf("failed to release slot '%d': %w", slot.Idx, err))
	}

	return errors.Join(errs...)
}

func (p *Pool) Close() error {
	p.cancel()

	for slot := range p.newSlots {
		err := p.cleanup(slot)
		if err != nil {
			return fmt.Errorf("failed to cleanup slot '%d': %w", slot.Idx, err)
		}
	}

	for slot := range p.reusedSlots {
		err := p.cleanup(slot)
		if err != nil {
			return fmt.Errorf("failed to cleanup slot '%d': %w", slot.Idx, err)
		}
	}

	return nil
}



================================================
File: internal/sandbox/network/slot.go
================================================
package network

import "fmt"

// We are using a more debuggable IP address allocation for now that only covers 255 addresses.
const (
	octetSize = 256
	octetMax  = octetSize - 1
	// This is the maximum number of IP addresses that can be allocated.
	slotsSize = octetSize * octetSize

	hostMask = 32
	vMask    = 30
	tapMask  = 30
)

type Slot struct {
	Key string
	Idx int
}

func NewSlot(key string, idx int) *Slot {
	return &Slot{
		Key: key,
		Idx: idx,
	}
}

func (s *Slot) VpeerName() string {
	return "eth0"
}

func (s *Slot) getOctets() (int, int) {
	rem := s.Idx % octetSize
	octet := (s.Idx - rem) / octetSize

	return octet, rem
}

func (s *Slot) VpeerIP() string {
	firstOctet, secondOctet := s.getOctets()

	return fmt.Sprintf("10.%d.%d.2", firstOctet, secondOctet)
}

func (s *Slot) VethIP() string {
	firstOctet, secondOctet := s.getOctets()

	return fmt.Sprintf("10.%d.%d.1", firstOctet, secondOctet)
}

func (s *Slot) VMask() int {
	return vMask
}

func (s *Slot) VethName() string {
	return fmt.Sprintf("veth-%d", s.Idx)
}

func (s *Slot) VethCIDR() string {
	return fmt.Sprintf("%s/%d", s.VethIP(), s.VMask())
}

func (s *Slot) VpeerCIDR() string {
	return fmt.Sprintf("%s/%d", s.VpeerIP(), s.VMask())
}

func (s *Slot) HostCIDR() string {
	return fmt.Sprintf("%s/%d", s.HostIP(), s.HostMask())
}

func (s *Slot) HostMask() int {
	return hostMask
}

// IP address for the sandbox from the host machine.
// You can use it to make requests to the sandbox.
func (s *Slot) HostIP() string {
	firstOctet, secondOctet := s.getOctets()

	return fmt.Sprintf("192.168.%d.%d", firstOctet, secondOctet)
}

func (s *Slot) NamespaceIP() string {
	return "169.254.0.21"
}

func (s *Slot) NamespaceID() string {
	return fmt.Sprintf("ns-%d", s.Idx)
}

func (s *Slot) TapName() string {
	return "tap0"
}

func (s *Slot) TapIP() string {
	return "169.254.0.22"
}

func (s *Slot) TapMask() int {
	return tapMask
}

func (s *Slot) TapCIDR() string {
	return fmt.Sprintf("%s/%d", s.TapIP(), s.TapMask())
}



================================================
File: internal/sandbox/network/storage.go
================================================
package network

import "github.com/e2b-dev/infra/packages/shared/pkg/env"

type Storage interface {
	Acquire() (*Slot, error)
	Release(*Slot) error
}

// NewStorage creates a new slot storage based on the environment, we are ok with using a memory storage for local
func NewStorage(slotsSize int, clientID string) (Storage, error) {
	if env.IsLocal() {
		return NewStorageMemory(slotsSize)
	} else {
		return NewStorageKV(slotsSize, clientID)
	}
}



================================================
File: internal/sandbox/network/storage_kv.go
================================================
package network

import (
	"fmt"
	"math/rand"
	"slices"

	consulApi "github.com/hashicorp/consul/api"

	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

type StorageKV struct {
	slotsSize    int
	consulClient *consulApi.Client
	clientID     string
}

func (s *StorageKV) getKVKey(slotIdx int) string {
	return fmt.Sprintf("%s/%d", s.clientID, slotIdx)
}

func NewStorageKV(slotsSize int, clientID string) (*StorageKV, error) {
	consulToken := utils.RequiredEnv("CONSUL_TOKEN", "Consul token for authenticating requests to the Consul API")

	consulClient, err := newConsulClient(consulToken)
	if err != nil {
		return nil, fmt.Errorf("failed to init StorageKV consul client: %w", err)
	}

	return &StorageKV{
		slotsSize:    slotsSize,
		consulClient: consulClient,
		clientID:     clientID,
	}, nil
}

func newConsulClient(token string) (*consulApi.Client, error) {
	config := consulApi.DefaultConfig()
	config.Token = token

	consulClient, err := consulApi.NewClient(config)
	if err != nil {
		return nil, fmt.Errorf("failed to initialize Consul client: %w", err)
	}

	return consulClient, nil
}

func (s *StorageKV) Acquire() (*Slot, error) {
	kv := s.consulClient.KV()

	var slot *Slot

	trySlot := func(slotIdx int, key string) (*Slot, error) {
		status, _, err := kv.CAS(&consulApi.KVPair{
			Key:         key,
			ModifyIndex: 0,
		}, nil)
		if err != nil {
			return nil, fmt.Errorf("failed to write to Consul KV: %w", err)
		}

		if status {
			return NewSlot(key, slotIdx), nil
		}

		return nil, nil
	}

	for randomTry := 1; randomTry <= 10; randomTry++ {
		slotIdx := rand.Intn(s.slotsSize)
		key := s.getKVKey(slotIdx)

		maybeSlot, err := trySlot(slotIdx, key)
		if err != nil {
			return nil, err
		}

		if maybeSlot != nil {
			slot = maybeSlot

			break
		}
	}

	if slot == nil {
		// This is a fallback for the case when all slots are taken.
		// There is no Consul lock so it's possible that multiple sandboxes will try to acquire the same slot.
		// In this case, only one of them will succeed and other will try with different slots.
		reservedKeys, _, keysErr := kv.Keys(s.clientID+"/", "", nil)
		if keysErr != nil {
			return nil, fmt.Errorf("failed to read Consul KV: %w", keysErr)
		}

		for slotIdx := 0; slotIdx < s.slotsSize; slotIdx++ {
			key := s.getKVKey(slotIdx)

			if slices.Contains(reservedKeys, key) {
				continue
			}

			maybeSlot, err := trySlot(slotIdx, key)
			if err != nil {
				return nil, err
			}

			if maybeSlot != nil {
				slot = maybeSlot

				break
			}
		}
	}

	if slot == nil {
		return nil, fmt.Errorf("failed to acquire IP slot: no empty slots found")
	}

	return slot, nil
}

func (s *StorageKV) Release(ips *Slot) error {
	kv := s.consulClient.KV()

	pair, _, err := kv.Get(ips.Key, nil)
	if err != nil {
		return fmt.Errorf("failed to release IPSlot: Failed to read Consul KV: %w", err)
	}

	if pair == nil {
		return fmt.Errorf("IP slot %d was already released", ips.Idx)
	}

	status, _, err := kv.DeleteCAS(&consulApi.KVPair{
		Key:         ips.Key,
		ModifyIndex: pair.ModifyIndex,
	}, nil)
	if err != nil {
		return fmt.Errorf("failed to release IPSlot: Failed to delete slot from Consul KV: %w", err)
	}

	if !status {
		return fmt.Errorf("IP slot '%d' for was already realocated", ips.Idx)
	}

	return nil
}



================================================
File: internal/sandbox/network/storage_memory.go
================================================
package network

import (
	"fmt"
	"strconv"
	"sync"
)

type StorageMemory struct {
	slotsSize   int
	freeSlots   []bool
	freeSlotsMu sync.Mutex
}

func NewStorageMemory(slotsSize int) (*StorageMemory, error) {
	return &StorageMemory{
		slotsSize:   slotsSize,
		freeSlots:   make([]bool, slotsSize),
		freeSlotsMu: sync.Mutex{},
	}, nil
}

func (s *StorageMemory) Acquire() (*Slot, error) {
	s.freeSlotsMu.Lock()
	defer s.freeSlotsMu.Unlock()

	// Simple slot tracking in memory
	// We skip the first slot because it's the host slot
	for slotIdx := 1; slotIdx < s.slotsSize; slotIdx++ {
		key := getMemoryKey(slotIdx)
		if !s.freeSlots[slotIdx] {
			s.freeSlots[slotIdx] = true
			return NewSlot(key, slotIdx), nil
		}
	}

	return nil, fmt.Errorf("failed to acquire IP slot: no empty slots found")
}

func (s *StorageMemory) Release(ips *Slot) error {
	s.freeSlotsMu.Lock()
	defer s.freeSlotsMu.Unlock()

	s.freeSlots[ips.Idx] = false

	return nil
}

func getMemoryKey(slotIdx int) string {
	return strconv.Itoa(slotIdx)
}



================================================
File: internal/sandbox/rootfs/cow.go
================================================
package rootfs

import (
	"context"
	"errors"
	"fmt"
	"io"
	"time"

	"github.com/bits-and-blooms/bitset"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/block"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/nbd"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

type CowDevice struct {
	overlay *block.Overlay
	mnt     *nbd.DirectPathMount

	ready *utils.SetOnce[string]

	blockSize   int64
	BaseBuildId string

	finishedOperations chan struct{}
	devicePool         *nbd.DevicePool
}

func NewCowDevice(rootfs *template.Storage, cachePath string, blockSize int64, devicePool *nbd.DevicePool) (*CowDevice, error) {
	size, err := rootfs.Size()
	if err != nil {
		return nil, fmt.Errorf("error getting device size: %w", err)
	}

	cache, err := block.NewCache(size, blockSize, cachePath, false)
	if err != nil {
		return nil, fmt.Errorf("error creating cache: %w", err)
	}

	overlay := block.NewOverlay(rootfs, cache, blockSize)

	mnt := nbd.NewDirectPathMount(overlay, devicePool)

	return &CowDevice{
		mnt:                mnt,
		overlay:            overlay,
		ready:              utils.NewSetOnce[string](),
		blockSize:          blockSize,
		finishedOperations: make(chan struct{}, 1),
		BaseBuildId:        rootfs.Header().Metadata.BaseBuildId.String(),
		devicePool:         devicePool,
	}, nil
}

func (o *CowDevice) Start(ctx context.Context) error {
	deviceIndex, err := o.mnt.Open(ctx)
	if err != nil {
		return o.ready.SetError(fmt.Errorf("error opening overlay file: %w", err))
	}

	return o.ready.SetValue(nbd.GetDevicePath(deviceIndex))
}

func (o *CowDevice) Export(ctx context.Context, out io.Writer, stopSandbox func() error) (*bitset.BitSet, error) {
	cache, err := o.overlay.EjectCache()
	if err != nil {
		return nil, fmt.Errorf("error ejecting cache: %w", err)
	}

	// the error is already logged in go routine in SandboxCreate handler
	go stopSandbox()

	select {
	case <-o.finishedOperations:
		break
	case <-ctx.Done():
		return nil, fmt.Errorf("timeout waiting for overlay device to be released")
	}

	dirty, err := cache.Export(out)
	if err != nil {
		return nil, fmt.Errorf("error exporting cache: %w", err)
	}

	err = cache.Close()
	if err != nil {
		return nil, fmt.Errorf("error closing cache: %w", err)
	}

	return dirty, nil
}

func (o *CowDevice) Close() error {
	var errs []error

	err := o.mnt.Close()
	if err != nil {
		errs = append(errs, fmt.Errorf("error closing overlay mount: %w", err))
	}

	o.finishedOperations <- struct{}{}

	err = o.overlay.Close()
	if err != nil {
		errs = append(errs, fmt.Errorf("error closing overlay cache: %w", err))
	}

	devicePath, err := o.ready.Wait()
	if err != nil {
		errs = append(errs, fmt.Errorf("error getting overlay path: %w", err))

		return errors.Join(errs...)
	}

	slot, err := nbd.GetDeviceSlot(devicePath)
	if err != nil {
		errs = append(errs, fmt.Errorf("error getting overlay slot: %w", err))

		return errors.Join(errs...)
	}

	attempts := 0
	for {
		attempts++
		err := o.devicePool.ReleaseDevice(slot)
		if errors.Is(err, nbd.ErrDeviceInUse{}) {
			if attempts%100 == 0 {
				zap.L().Info("error releasing overlay device", zap.Int("attempts", attempts), zap.Error(err))
			}

			time.Sleep(500 * time.Millisecond)

			continue
		}

		if err != nil {
			return fmt.Errorf("error releasing overlay device: %w", err)
		}

		break
	}

	zap.L().Info("overlay device released")

	return nil
}

func (o *CowDevice) Path() (string, error) {
	return o.ready.Wait()
}



================================================
File: internal/sandbox/socket/socket.go
================================================
package socket

import (
	"context"
	"fmt"
	"os"
	"time"
)

const waitInterval = 10 * time.Millisecond

// Wait waits for the given file to exist.
func Wait(ctx context.Context, socketPath string) error {
	ticker := time.NewTicker(waitInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return fmt.Errorf("cancelled wait for socket '%s': %w", socketPath, ctx.Err())
		case <-ticker.C:
			if _, err := os.Stat(socketPath); err != nil {
				continue
			}

			return nil
		}
	}
}



================================================
File: internal/sandbox/template/cache.go
================================================
package template

import (
	"context"
	"fmt"
	"time"

	"github.com/jellydator/ttlcache/v3"
	"go.uber.org/zap"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/build"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
)

// How long to keep the template in the cache since the last access.
// Should be longer than the maximum possible sandbox lifetime.
const (
	templateExpiration = time.Hour * 25

	buildCacheTTL           = time.Hour * 25
	buildCacheDelayEviction = time.Second * 60

	// buildCacheMaxUsedPercentage the maximum percentage of the cache disk storage
	// that can be used before the cache starts evicting items.
	buildCacheMaxUsedPercentage = 75.0
)

type Cache struct {
	cache      *ttlcache.Cache[string, Template]
	bucket     *gcs.BucketHandle
	ctx        context.Context
	buildStore *build.DiffStore
}

func NewCache(ctx context.Context) (*Cache, error) {
	cache := ttlcache.New(
		ttlcache.WithTTL[string, Template](templateExpiration),
	)

	cache.OnEviction(func(ctx context.Context, reason ttlcache.EvictionReason, item *ttlcache.Item[string, Template]) {
		template := item.Value()

		err := template.Close()
		if err != nil {
			zap.L().Warn("failed to cleanup template data", zap.String("item_key", item.Key()), zap.Error(err))
		}
	})

	go cache.Start()

	buildStore, err := build.NewDiffStore(
		ctx,
		build.DefaultCachePath,
		buildCacheTTL,
		buildCacheDelayEviction,
		buildCacheMaxUsedPercentage,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create build store: %w", err)
	}

	return &Cache{
		bucket:     gcs.GetTemplateBucket(),
		buildStore: buildStore,
		cache:      cache,
		ctx:        ctx,
	}, nil
}

func (c *Cache) Items() map[string]*ttlcache.Item[string, Template] {
	return c.cache.Items()
}

func (c *Cache) GetTemplate(
	templateId,
	buildId,
	kernelVersion,
	firecrackerVersion string,
	hugePages bool,
	isSnapshot bool,
) (Template, error) {
	storageTemplate, err := newTemplateFromStorage(
		templateId,
		buildId,
		kernelVersion,
		firecrackerVersion,
		hugePages,
		isSnapshot,
		nil,
		nil,
		c.bucket,
		nil,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to create template cache from storage: %w", err)
	}

	t, found := c.cache.GetOrSet(
		storageTemplate.Files().CacheKey(),
		storageTemplate,
		ttlcache.WithTTL[string, Template](templateExpiration),
	)

	if !found {
		go storageTemplate.Fetch(c.ctx, c.buildStore)
	}

	return t.Value(), nil
}

func (c *Cache) AddSnapshot(
	templateId,
	buildId,
	kernelVersion,
	firecrackerVersion string,
	hugePages bool,
	memfileHeader *header.Header,
	rootfsHeader *header.Header,
	localSnapfile *LocalFile,
	memfileDiff build.Diff,
	rootfsDiff build.Diff,
) error {
	switch memfileDiff.(type) {
	case *build.NoDiff:
		break
	default:
		c.buildStore.Add(memfileDiff)
	}

	switch rootfsDiff.(type) {
	case *build.NoDiff:
		break
	default:
		c.buildStore.Add(rootfsDiff)
	}

	storageTemplate, err := newTemplateFromStorage(
		templateId,
		buildId,
		kernelVersion,
		firecrackerVersion,
		hugePages,
		true,
		memfileHeader,
		rootfsHeader,
		c.bucket,
		localSnapfile,
	)
	if err != nil {
		return fmt.Errorf("failed to create template cache from storage: %w", err)
	}

	_, found := c.cache.GetOrSet(
		storageTemplate.Files().CacheKey(),
		storageTemplate,
		ttlcache.WithTTL[string, Template](templateExpiration),
	)

	if !found {
		go storageTemplate.Fetch(c.ctx, c.buildStore)
	}

	return nil
}



================================================
File: internal/sandbox/template/file.go
================================================
package template

type File interface {
	Path() string
	Close() error
}



================================================
File: internal/sandbox/template/local_file.go
================================================
package template

import (
	"os"
)

type LocalFile struct {
	path string
}

func NewLocalFile(
	path string,
) (*LocalFile, error) {
	return &LocalFile{
		path: path,
	}, nil
}

func (f *LocalFile) Path() string {
	return f.path
}

func (f *LocalFile) Close() error {
	return os.RemoveAll(f.path)
}



================================================
File: internal/sandbox/template/storage.go
================================================
package template

import (
	"context"
	"fmt"

	"github.com/google/uuid"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/build"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
)

type Storage struct {
	header *header.Header
	source *build.File
}

func NewStorage(
	ctx context.Context,
	store *build.DiffStore,
	buildId string,
	fileType build.DiffType,
	blockSize int64,
	isSnapshot bool,
	h *header.Header,
	bucket *gcs.BucketHandle,
) (*Storage, error) {
	if isSnapshot && h == nil {
		headerObject := gcs.NewObject(ctx, bucket, buildId+"/"+string(fileType)+storage.HeaderSuffix)

		diffHeader, err := header.Deserialize(headerObject)
		if err != nil {
			return nil, fmt.Errorf("failed to deserialize header: %w", err)
		}

		h = diffHeader
	} else if h == nil {
		object := gcs.NewObject(ctx, bucket, buildId+"/"+string(fileType))

		size, err := object.Size()
		if err != nil {
			return nil, fmt.Errorf("failed to get object size: %w", err)
		}

		id, err := uuid.Parse(buildId)
		if err != nil {
			return nil, fmt.Errorf("failed to parse build id: %w", err)
		}

		h = header.NewHeader(&header.Metadata{
			BuildId:     id,
			BaseBuildId: id,
			Size:        uint64(size),
			Version:     1,
			BlockSize:   uint64(blockSize),
			Generation:  1,
		}, nil)
	}

	b := build.NewFile(h, store, fileType, bucket)

	return &Storage{
		source: b,
		header: h,
	}, nil
}

func (d *Storage) ReadAt(p []byte, off int64) (int, error) {
	return d.source.ReadAt(p, off)
}

func (d *Storage) Size() (int64, error) {
	return int64(d.header.Metadata.Size), nil
}

func (d *Storage) Slice(off, length int64) ([]byte, error) {
	return d.source.Slice(off, length)
}

func (d *Storage) Header() *header.Header {
	return d.header
}



================================================
File: internal/sandbox/template/storage_file.go
================================================
package template

import (
	"context"
	"errors"
	"fmt"
	"os"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
)

type storageFile struct {
	path string
}

func newStorageFile(
	ctx context.Context,
	bucket *gcs.BucketHandle,
	bucketObjectPath string,
	path string,
) (*storageFile, error) {
	f, err := os.Create(path)
	if err != nil {
		return nil, fmt.Errorf("failed to create file: %w", err)
	}

	defer f.Close()

	object := gcs.NewObject(ctx, bucket, bucketObjectPath)

	_, err = object.WriteTo(f)
	if err != nil {
		cleanupErr := os.Remove(path)

		return nil, fmt.Errorf("failed to write to file: %w", errors.Join(err, cleanupErr))
	}

	return &storageFile{
		path: path,
	}, nil
}

func (f *storageFile) Path() string {
	return f.path
}

func (f *storageFile) Close() error {
	return os.RemoveAll(f.path)
}



================================================
File: internal/sandbox/template/storage_template.go
================================================
package template

import (
	"context"
	"fmt"
	"os"
	"sync"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/build"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/gcs"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage/header"
	"github.com/e2b-dev/infra/packages/shared/pkg/utils"
)

type storageTemplate struct {
	files *storage.TemplateCacheFiles

	memfile  *utils.SetOnce[*Storage]
	rootfs   *utils.SetOnce[*Storage]
	snapfile *utils.SetOnce[File]

	isSnapshot bool

	memfileHeader *header.Header
	rootfsHeader  *header.Header
	localSnapfile *LocalFile

	bucket *gcs.BucketHandle
}

func newTemplateFromStorage(
	templateId,
	buildId,
	kernelVersion,
	firecrackerVersion string,
	hugePages bool,
	isSnapshot bool,
	memfileHeader *header.Header,
	rootfsHeader *header.Header,
	bucket *gcs.BucketHandle,
	localSnapfile *LocalFile,
) (*storageTemplate, error) {
	files, err := storage.NewTemplateFiles(
		templateId,
		buildId,
		kernelVersion,
		firecrackerVersion,
		hugePages,
	).NewTemplateCacheFiles()
	if err != nil {
		return nil, fmt.Errorf("failed to create template cache files: %w", err)
	}

	return &storageTemplate{
		files:         files,
		localSnapfile: localSnapfile,
		isSnapshot:    isSnapshot,
		memfileHeader: memfileHeader,
		rootfsHeader:  rootfsHeader,
		bucket:        bucket,
		memfile:       utils.NewSetOnce[*Storage](),
		rootfs:        utils.NewSetOnce[*Storage](),
		snapfile:      utils.NewSetOnce[File](),
	}, nil
}

func (t *storageTemplate) Fetch(ctx context.Context, buildStore *build.DiffStore) {
	err := os.MkdirAll(t.files.CacheDir(), os.ModePerm)
	if err != nil {
		errMsg := fmt.Errorf("failed to create directory %s: %w", t.files.CacheDir(), err)

		t.memfile.SetError(errMsg)
		t.rootfs.SetError(errMsg)
		t.snapfile.SetError(errMsg)

		return
	}

	var wg sync.WaitGroup

	wg.Add(1)
	go func() error {
		defer wg.Done()
		if t.localSnapfile != nil {
			return t.snapfile.SetValue(t.localSnapfile)
		}

		snapfile, snapfileErr := newStorageFile(
			ctx,
			t.bucket,
			t.files.StorageSnapfilePath(),
			t.files.CacheSnapfilePath(),
		)
		if snapfileErr != nil {
			errMsg := fmt.Errorf("failed to fetch snapfile: %w", snapfileErr)

			return t.snapfile.SetError(errMsg)
		}

		return t.snapfile.SetValue(snapfile)
	}()

	wg.Add(1)
	go func() error {
		defer wg.Done()

		memfileStorage, memfileErr := NewStorage(
			ctx,
			buildStore,
			t.files.BuildId,
			build.Memfile,
			t.files.MemfilePageSize(),
			t.isSnapshot,
			t.memfileHeader,
			t.bucket,
		)
		if memfileErr != nil {
			errMsg := fmt.Errorf("failed to create memfile storage: %w", memfileErr)

			return t.memfile.SetError(errMsg)
		}

		return t.memfile.SetValue(memfileStorage)
	}()

	wg.Add(1)
	go func() error {
		defer wg.Done()

		rootfsStorage, rootfsErr := NewStorage(
			ctx,
			buildStore,
			t.files.BuildId,
			build.Rootfs,
			t.files.RootfsBlockSize(),
			t.isSnapshot,
			t.rootfsHeader,
			t.bucket,
		)
		if rootfsErr != nil {
			errMsg := fmt.Errorf("failed to create rootfs storage: %w", rootfsErr)

			return t.rootfs.SetError(errMsg)
		}

		return t.rootfs.SetValue(rootfsStorage)
	}()

	wg.Wait()
}

func (t *storageTemplate) Close() error {
	return closeTemplate(t)
}

func (t *storageTemplate) Files() *storage.TemplateCacheFiles {
	return t.files
}

func (t *storageTemplate) Memfile() (*Storage, error) {
	return t.memfile.Wait()
}

func (t *storageTemplate) Rootfs() (*Storage, error) {
	return t.rootfs.Wait()
}

func (t *storageTemplate) Snapfile() (File, error) {
	return t.snapfile.Wait()
}



================================================
File: internal/sandbox/template/template.go
================================================
package template

import (
	"errors"

	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
)

type Template interface {
	Files() *storage.TemplateCacheFiles
	Memfile() (*Storage, error)
	Rootfs() (*Storage, error)
	Snapfile() (File, error)
	Close() error
}

func closeTemplate(t Template) error {
	var errs []error

	snapfile, err := t.Snapfile()
	if err == nil {
		errs = append(errs, snapfile.Close())
	}

	return errors.Join(errs...)
}



================================================
File: internal/sandbox/uffd/handler.go
================================================
package uffd

import (
	"encoding/json"
	"errors"
	"fmt"
	"net"
	"os"
	"sync"
	"syscall"
	"time"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/block"
	"go.uber.org/zap"

	"github.com/bits-and-blooms/bitset"
)

const (
	uffdMsgListenerTimeout = 10 * time.Second
	fdSize                 = 4
	mappingsSize           = 1024
)

type UffdSetup struct {
	Mappings []GuestRegionUffdMapping
	Fd       uintptr
}

func (u *Uffd) TrackAndReturnNil() error {
	return u.lis.Close()
}

type Uffd struct {
	Exit  chan error
	Ready chan struct{}

	exitReader *os.File
	exitWriter *os.File

	Stop func() error

	lis *net.UnixListener

	memfile    *block.TrackedSliceDevice
	socketPath string
	clientID   string
}

func (u *Uffd) Disable() error {
	return u.memfile.Disable()
}

func (u *Uffd) Dirty() *bitset.BitSet {
	return u.memfile.Dirty()
}

func New(memfile block.ReadonlyDevice, socketPath string, blockSize int64, clientID string) (*Uffd, error) {
	pRead, pWrite, err := os.Pipe()
	if err != nil {
		return nil, fmt.Errorf("failed to create exit fd: %w", err)
	}

	trackedMemfile, err := block.NewTrackedSliceDevice(blockSize, memfile)
	if err != nil {
		return nil, fmt.Errorf("failed to create tracked slice device: %w", err)
	}

	return &Uffd{
		Exit:       make(chan error, 1),
		Ready:      make(chan struct{}, 1),
		exitReader: pRead,
		exitWriter: pWrite,
		memfile:    trackedMemfile,
		socketPath: socketPath,
		Stop: sync.OnceValue(func() error {
			_, writeErr := pWrite.Write([]byte{0})
			if writeErr != nil {
				return fmt.Errorf("failed write to exit writer: %w", writeErr)
			}

			return nil
		}),
		clientID: clientID,
	}, nil
}

func (u *Uffd) Start(sandboxId string) error {
	lis, err := net.ListenUnix("unix", &net.UnixAddr{Name: u.socketPath, Net: "unix"})
	if err != nil {
		return fmt.Errorf("failed listening on socket: %w", err)
	}

	u.lis = lis

	err = os.Chmod(u.socketPath, 0o777)
	if err != nil {
		return fmt.Errorf("failed setting socket permissions: %w", err)
	}

	go func() {
		// TODO: If the handle function fails, we should kill the sandbox
		handleErr := u.handle(sandboxId)
		closeErr := u.lis.Close()
		writerErr := u.exitWriter.Close()

		u.Exit <- errors.Join(handleErr, closeErr, writerErr)

		close(u.Ready)
		close(u.Exit)
	}()

	return nil
}

func (u *Uffd) receiveSetup() (*UffdSetup, error) {
	err := u.lis.SetDeadline(time.Now().Add(uffdMsgListenerTimeout))
	if err != nil {
		return nil, fmt.Errorf("failed setting listener deadline: %w", err)
	}

	conn, err := u.lis.Accept()
	if err != nil {
		return nil, fmt.Errorf("failed accepting firecracker connection: %w", err)
	}

	unixConn := conn.(*net.UnixConn)

	mappingsBuf := make([]byte, mappingsSize)
	uffdBuf := make([]byte, syscall.CmsgSpace(fdSize))

	numBytesMappings, numBytesFd, _, _, err := unixConn.ReadMsgUnix(mappingsBuf, uffdBuf)
	if err != nil {
		return nil, fmt.Errorf("failed to read unix msg from connection: %w", err)
	}

	mappingsBuf = mappingsBuf[:numBytesMappings]

	var mappings []GuestRegionUffdMapping

	err = json.Unmarshal(mappingsBuf, &mappings)
	if err != nil {
		return nil, fmt.Errorf("failed parsing memory mapping data: %w", err)
	}

	controlMsgs, err := syscall.ParseSocketControlMessage(uffdBuf[:numBytesFd])
	if err != nil {
		return nil, fmt.Errorf("failed parsing control messages: %w", err)
	}

	if len(controlMsgs) != 1 {
		return nil, fmt.Errorf("expected 1 control message containing UFFD: found %d", len(controlMsgs))
	}

	fds, err := syscall.ParseUnixRights(&controlMsgs[0])
	if err != nil {
		return nil, fmt.Errorf("failed parsing unix write: %w", err)
	}

	if len(fds) != 1 {
		return nil, fmt.Errorf("expected 1 fd: found %d", len(fds))
	}

	return &UffdSetup{
		Mappings: mappings,
		Fd:       uintptr(fds[0]),
	}, nil
}

func (u *Uffd) handle(sandboxId string) (err error) {
	setup, err := u.receiveSetup()
	if err != nil {
		return fmt.Errorf("failed to receive setup message from firecracker: %w", err)
	}

	uffd := setup.Fd
	defer func() {
		closeErr := syscall.Close(int(uffd))
		if closeErr != nil {
			zap.L().Error("failed to close uffd", zap.String("sandbox_id", sandboxId), zap.String("socket_path", u.socketPath), zap.Error(closeErr))
		}
	}()

	u.Ready <- struct{}{}

	err = Serve(
		int(uffd),
		setup.Mappings,
		u.memfile,
		u.exitReader.Fd(),
		u.Stop,
		sandboxId,
		u.clientID,
	)
	if err != nil {
		return fmt.Errorf("failed handling uffd: %w", err)
	}

	return nil
}



================================================
File: internal/sandbox/uffd/serve_linux.go
================================================
//go:build linux
// +build linux

package uffd

import (
	"errors"
	"fmt"
	"syscall"
	"unsafe"

	"github.com/loopholelabs/userfaultfd-go/pkg/constants"
	"go.uber.org/zap"
	"golang.org/x/sync/errgroup"
	"golang.org/x/sys/unix"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/block"
)

var ErrUnexpectedEventType = errors.New("unexpected event type")

type GuestRegionUffdMapping struct {
	BaseHostVirtAddr uintptr `json:"base_host_virt_addr"`
	Size             uintptr `json:"size"`
	Offset           uintptr `json:"offset"`
	PageSize         uintptr `json:"page_size_kib"`
}

func getMapping(addr uintptr, mappings []GuestRegionUffdMapping) (*GuestRegionUffdMapping, error) {
	for _, m := range mappings {
		if !(addr >= m.BaseHostVirtAddr && addr < m.BaseHostVirtAddr+m.Size) {
			continue
		}

		return &m, nil
	}

	return nil, fmt.Errorf("address %d not found in any mapping", addr)
}

func Serve(
	uffd int,
	mappings []GuestRegionUffdMapping,
	src *block.TrackedSliceDevice,
	fd uintptr,
	stop func() error,
	sandboxId string,
	nodeID string,
) error {
	pollFds := []unix.PollFd{
		{Fd: int32(uffd), Events: unix.POLLIN},
		{Fd: int32(fd), Events: unix.POLLIN},
	}

	var eg errgroup.Group

outerLoop:
	for {
		if _, err := unix.Poll(
			pollFds,
			-1,
		); err != nil {
			if err == unix.EINTR {
				zap.L().Debug("uffd: interrupted polling, going back to polling", zap.String("sandbox_id", sandboxId))

				continue
			}

			if err == unix.EAGAIN {
				zap.L().Debug("uffd: eagain during polling, going back to polling", zap.String("sandbox_id", sandboxId))

				continue
			}

			zap.L().Error("UFFD serve polling error", zap.String("sandbox_id", sandboxId), zap.Error(err), zap.String("node_id", nodeID))

			return fmt.Errorf("failed polling: %w", err)
		}

		exitFd := pollFds[1]
		if exitFd.Revents&unix.POLLIN != 0 {
			errMsg := eg.Wait()
			if errMsg != nil {
				zap.L().Warn("UFFD fd exit error while waiting for goroutines to finish", zap.String("sandbox_id", sandboxId), zap.Error(errMsg), zap.String("node_id", nodeID))

				return fmt.Errorf("failed to handle uffd: %w", errMsg)
			}

			return nil
		}

		uffdFd := pollFds[0]
		if uffdFd.Revents&unix.POLLIN == 0 {
			// Uffd is not ready for reading as there is nothing to read on the fd.
			// https://github.com/firecracker-microvm/firecracker/issues/5056
			// https://elixir.bootlin.com/linux/v6.8.12/source/fs/userfaultfd.c#L1149
			// TODO: Check for all the errors
			// - https://docs.kernel.org/admin-guide/mm/userfaultfd.html
			// - https://elixir.bootlin.com/linux/v6.8.12/source/fs/userfaultfd.c
			// - https://man7.org/linux/man-pages/man2/userfaultfd.2.html
			// It might be possible to just check for data != 0 in the syscall.Read loop
			// but I don't feel confident about doing that.
			zap.L().Debug("uffd: no data in fd, going back to polling", zap.String("sandbox_id", sandboxId))

			continue
		}

		buf := make([]byte, unsafe.Sizeof(constants.UffdMsg{}))

		for {
			n, err := syscall.Read(uffd, buf)
			if err == syscall.EINTR {
				zap.L().Debug("uffd: interrupted read, reading again", zap.String("sandbox_id", sandboxId))

				continue
			}

			if err == nil {
				// There is no error so we can proceed.
				break
			}

			if err == syscall.EAGAIN {
				zap.L().Debug("uffd: eagain error, going back to polling", zap.String("sandbox_id", sandboxId), zap.Error(err), zap.String("node_id", nodeID), zap.Int("read_bytes", n))

				// Continue polling the fd.
				continue outerLoop
			}

			zap.L().Error("uffd: read error", zap.String("sandbox_id", sandboxId), zap.Error(err), zap.String("node_id", nodeID))

			return fmt.Errorf("failed to read: %w", err)
		}

		msg := (*(*constants.UffdMsg)(unsafe.Pointer(&buf[0])))
		if constants.GetMsgEvent(&msg) != constants.UFFD_EVENT_PAGEFAULT {
			zap.L().Error("UFFD serve unexpected event type", zap.String("sandbox_id", sandboxId), zap.String("node_id", nodeID), zap.Any("event_type", constants.GetMsgEvent(&msg)))

			return ErrUnexpectedEventType
		}

		arg := constants.GetMsgArg(&msg)
		pagefault := (*(*constants.UffdPagefault)(unsafe.Pointer(&arg[0])))

		addr := constants.GetPagefaultAddress(&pagefault)

		mapping, err := getMapping(uintptr(addr), mappings)
		if err != nil {
			zap.L().Error("UFFD serve get mapping error", zap.String("sandbox_id", sandboxId), zap.Error(err), zap.String("node_id", nodeID))

			return fmt.Errorf("failed to map: %w", err)
		}

		offset := int64(mapping.Offset + uintptr(addr) - mapping.BaseHostVirtAddr)
		pagesize := int64(mapping.PageSize)

		eg.Go(func() error {
			defer func() {
				if r := recover(); r != nil {
					zap.L().Error("UFFD serve panic", zap.String("sandbox_id", sandboxId), zap.String("node_id", nodeID), zap.Any("offset", offset), zap.Any("pagesize", pagesize), zap.Any("panic", r))
					fmt.Printf("[sandbox %s]: recovered from panic in uffd serve (offset: %d, pagesize: %d): %v\n", sandboxId, offset, pagesize, r)
				}
			}()

			b, err := src.Slice(offset, pagesize)
			if err != nil {

				stop()

				zap.L().Error("UFFD serve slice error", zap.String("sandbox_id", sandboxId), zap.Error(err), zap.String("node_id", nodeID))

				return fmt.Errorf("failed to read from source: %w", err)
			}

			cpy := constants.NewUffdioCopy(
				b,
				addr&^constants.CULong(pagesize-1),
				constants.CULong(pagesize),
				0,
				0,
			)

			if _, _, errno := syscall.Syscall(
				syscall.SYS_IOCTL,
				uintptr(uffd),
				constants.UFFDIO_COPY,
				uintptr(unsafe.Pointer(&cpy)),
			); errno != 0 {
				if errno == unix.EEXIST {
					zap.L().Debug("UFFD serve page already mapped", zap.String("sandbox_id", sandboxId), zap.String("node_id", nodeID), zap.Any("offset", offset), zap.Any("pagesize", pagesize))

					// Page is already mapped
					return nil
				}

				stop()

				zap.L().Error("UFFD serve uffdio copy error", zap.String("sandbox_id", sandboxId), zap.Error(err), zap.String("node_id", nodeID))

				return fmt.Errorf("failed uffdio copy %w", errno)
			}

			return nil
		})
	}
}



================================================
File: internal/sandbox/uffd/serve_other.go
================================================
//go:build !linux
// +build !linux

package uffd

import (
	"errors"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/block"
)

var ErrUnexpectedEventType = errors.New("unexpected event type")

type GuestRegionUffdMapping struct {
	BaseHostVirtAddr uintptr `json:"base_host_virt_addr"`
	Size             uintptr `json:"size"`
	Offset           uintptr `json:"offset"`
	PageSize         uintptr `json:"page_size_kib"`
}

func Serve(uffd int, mappings []GuestRegionUffdMapping, src *block.TrackedSliceDevice, fd uintptr, stop func() error, sandboxId string, nodeID string) error {
	return errors.New("platform does not support UFFD")
}



================================================
File: internal/server/main.go
================================================
package server

import (
	"context"
	"errors"
	"fmt"
	"math"
	"net"
	"os"
	"sync"

	"github.com/grpc-ecosystem/go-grpc-middleware/v2/interceptors/logging"
	"github.com/grpc-ecosystem/go-grpc-middleware/v2/interceptors/recovery"
	"github.com/grpc-ecosystem/go-grpc-middleware/v2/interceptors/selector"
	"go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/trace"
	"go.uber.org/zap"
	"google.golang.org/grpc"
	"google.golang.org/grpc/health"
	"google.golang.org/grpc/health/grpc_health_v1"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/dns"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/nbd"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/network"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/template"
	"github.com/e2b-dev/infra/packages/shared/pkg/chdb"
	e2bgrpc "github.com/e2b-dev/infra/packages/shared/pkg/grpc"
	"github.com/e2b-dev/infra/packages/shared/pkg/grpc/orchestrator"
	"github.com/e2b-dev/infra/packages/shared/pkg/logger"
	"github.com/e2b-dev/infra/packages/shared/pkg/smap"
)

const ServiceName = "orchestrator"

type server struct {
	orchestrator.UnimplementedSandboxServiceServer
	sandboxes       *smap.Map[*sandbox.Sandbox]
	dns             *dns.DNS
	tracer          trace.Tracer
	networkPool     *network.Pool
	templateCache   *template.Cache
	pauseMu         sync.Mutex
	clientID        string // nomad node id
	devicePool      *nbd.DevicePool
	clickhouseStore chdb.Store

	useLokiMetrics       string
	useClickhouseMetrics string
}

type Service struct {
	server   *server
	grpc     *grpc.Server
	dns      *dns.DNS
	port     uint16
	shutdown struct {
		once sync.Once
		op   func(context.Context) error
		err  error
	}
	// there really should be a config struct for this
	// using something like viper to read the config
	// but for now this is just a quick hack
	// see https://linear.app/e2b/issue/E2B-1731/use-viper-to-read-env-vars
	useLokiMetrics       string
	useClickhouseMetrics string
}

func New(ctx context.Context, port uint, clientID string) (*Service, error) {
	if port > math.MaxUint16 {
		return nil, fmt.Errorf("%d is larger than maximum possible port %d", port, math.MaxInt16)
	}

	if clientID == "" {
		return nil, errors.New("clientID is required")
	}

	srv := &Service{port: uint16(port)}

	templateCache, err := template.NewCache(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to create template cache: %w", err)
	}

	networkPool, err := network.NewPool(ctx, network.NewSlotsPoolSize, network.ReusedSlotsPoolSize, clientID)
	if err != nil {
		return nil, fmt.Errorf("failed to create network pool: %w", err)
	}

	// BLOCK: initialize services
	{
		srv.dns = dns.New()

		opts := []logging.Option{
			logging.WithLogOnEvents(logging.StartCall, logging.PayloadReceived, logging.PayloadSent, logging.FinishCall),
			logging.WithLevels(logging.DefaultServerCodeToLevel),
			logging.WithFieldsFromContext(logging.ExtractFields),
		}
		srv.grpc = grpc.NewServer(
			grpc.StatsHandler(e2bgrpc.NewStatsWrapper(otelgrpc.NewServerHandler())),
			grpc.ChainUnaryInterceptor(
				recovery.UnaryServerInterceptor(),
				selector.UnaryServerInterceptor(
					logging.UnaryServerInterceptor(logger.GRPCLogger(zap.L()), opts...),
					logger.WithoutHealthCheck(),
				),
			),
			grpc.ChainStreamInterceptor(
				selector.StreamServerInterceptor(
					logging.StreamServerInterceptor(logger.GRPCLogger(zap.L()), opts...),
					logger.WithoutHealthCheck(),
				),
			),
		)

		devicePool, err := nbd.NewDevicePool()

		if err != nil {
			return nil, fmt.Errorf("failed to create device pool: %w", err)
		}

		useLokiMetrics := os.Getenv("WRITE_LOKI_METRICS")
		useClickhouseMetrics := os.Getenv("WRITE_CLICKHOUSE_METRICS")
		readClickhouseMetrics := os.Getenv("READ_CLICKHOUSE_METRICS")

		var clickhouseStore chdb.Store = nil

		if readClickhouseMetrics == "true" || useClickhouseMetrics == "true" {
			clickhouseStore, err = chdb.NewStore(chdb.ClickHouseConfig{
				ConnectionString: os.Getenv("CLICKHOUSE_CONNECTION_STRING"),
				Username:         os.Getenv("CLICKHOUSE_USERNAME"),
				Password:         os.Getenv("CLICKHOUSE_PASSWORD"),
				Database:         os.Getenv("CLICKHOUSE_DATABASE"),
				Debug:            os.Getenv("CLICKHOUSE_DEBUG") == "true",
			})
			if err != nil {
				return nil, fmt.Errorf("failed to create clickhouse store: %w", err)
			}
		}

		srv.server = &server{
			tracer:               otel.Tracer(ServiceName),
			dns:                  srv.dns,
			sandboxes:            smap.New[*sandbox.Sandbox](),
			networkPool:          networkPool,
			templateCache:        templateCache,
			clientID:             clientID,
			devicePool:           devicePool,
			clickhouseStore:      clickhouseStore,
			useLokiMetrics:       useLokiMetrics,
			useClickhouseMetrics: useClickhouseMetrics,
		}
	}

	orchestrator.RegisterSandboxServiceServer(srv.grpc, srv.server)
	grpc_health_v1.RegisterHealthServer(srv.grpc, health.NewServer())

	return srv, nil
}

// Start launches
func (srv *Service) Start(context.Context) error {
	if srv.server == nil || srv.dns == nil || srv.grpc == nil {
		return errors.New("orchestrator services are not initialized")
	}

	go func() {
		zap.L().Info("Starting DNS server")
		if err := srv.dns.Start("127.0.0.4", 53); err != nil {
			zap.L().Fatal("Failed running DNS server", zap.Error(err))
		}
	}()

	// the listener is closed by the shutdown operation
	lis, err := net.Listen("tcp", fmt.Sprintf(":%d", srv.port))
	if err != nil {
		return fmt.Errorf("failed to listen on port %d: %w", srv.port, err)
	}

	zap.L().Info("Starting orchestrator server", zap.Uint16("port", srv.port))

	go func() {
		if err := srv.grpc.Serve(lis); err != nil {
			zap.L().Fatal("grpc server failed to serve", zap.Error(err))
		}
	}()

	srv.shutdown.op = func(ctx context.Context) error {
		var errs []error

		srv.grpc.GracefulStop()

		if err := lis.Close(); err != nil {
			errs = append(errs, err)
		}

		if err := srv.dns.Close(ctx); err != nil {
			errs = append(errs, err)
		}

		return errors.Join(errs...)
	}

	return nil
}

func (srv *Service) Close(ctx context.Context) error {
	srv.shutdown.once.Do(func() {
		if srv.shutdown.op == nil {
			// should only be true if there was an error
			// during startup.
			return
		}

		srv.shutdown.err = srv.shutdown.op(ctx)
		srv.shutdown.op = nil
	})
	return srv.shutdown.err
}



================================================
File: internal/server/sandboxes.go
================================================
package server

import (
	"context"
	"errors"
	"fmt"
	"os"
	"sync"
	"time"

	"go.opentelemetry.io/otel/attribute"
	"go.uber.org/zap"
	"golang.org/x/sync/semaphore"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/status"
	"google.golang.org/protobuf/types/known/emptypb"
	"google.golang.org/protobuf/types/known/timestamppb"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox"
	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox/build"
	"github.com/e2b-dev/infra/packages/shared/pkg/grpc/orchestrator"
	sbxlogger "github.com/e2b-dev/infra/packages/shared/pkg/logger/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/storage"
	"github.com/e2b-dev/infra/packages/shared/pkg/telemetry"
)

const (
	requestTimeout = 60 * time.Second

	maxParalellSnapshotting = 8
)

func (s *server) Create(ctxConn context.Context, req *orchestrator.SandboxCreateRequest) (*orchestrator.SandboxCreateResponse, error) {
	ctx, cancel := context.WithTimeoutCause(ctxConn, requestTimeout, fmt.Errorf("request timed out"))
	defer cancel()

	childCtx, childSpan := s.tracer.Start(ctx, "sandbox-create")
	defer childSpan.End()

	childSpan.SetAttributes(
		attribute.String("template.id", req.Sandbox.TemplateId),
		attribute.String("kernel.version", req.Sandbox.KernelVersion),
		attribute.String("sandbox.id", req.Sandbox.SandboxId),
		attribute.String("client.id", s.clientID),
		attribute.String("envd.version", req.Sandbox.EnvdVersion),
	)

	sbx, cleanup, err := sandbox.NewSandbox(
		childCtx,
		s.tracer,
		s.dns,
		s.networkPool,
		s.templateCache,
		req.Sandbox,
		childSpan.SpanContext().TraceID().String(),
		req.StartTime.AsTime(),
		req.EndTime.AsTime(),
		req.Sandbox.Snapshot,
		req.Sandbox.BaseTemplateId,
		s.clientID,
		s.devicePool,
		s.clickhouseStore,
		s.useLokiMetrics,
		s.useClickhouseMetrics,
	)
	if err != nil {
		zap.L().Error("failed to create sandbox, cleaning up", zap.Error(err))
		cleanupErr := cleanup.Run()

		errMsg := fmt.Errorf("failed to cleanup sandbox: %w", errors.Join(err, context.Cause(ctx), cleanupErr))
		telemetry.ReportCriticalError(ctx, errMsg)

		return nil, status.New(codes.Internal, errMsg.Error()).Err()
	}

	s.sandboxes.Insert(req.Sandbox.SandboxId, sbx)
	go func() {
		waitErr := sbx.Wait()
		if waitErr != nil {
			sbxlogger.I(sbx).Error("failed to wait for sandbox, cleaning up", zap.Error(waitErr))
		}

		cleanupErr := cleanup.Run()
		if cleanupErr != nil {
			sbxlogger.I(sbx).Error("failed to cleanup sandbox, will remove from cache", zap.Error(cleanupErr))
		}

		// Remove the sandbox from cache only if the cleanup IDs match.
		// This prevents us from accidentally removing started sandbox (via resume) from the cache if cleanup is taking longer than the request timeout.
		// This could have caused the "invisible" sandboxes that are not in orchestrator or API, but are still on client.
		s.sandboxes.RemoveCb(req.Sandbox.SandboxId, func(_ string, v *sandbox.Sandbox, exists bool) bool {
			if !exists {
				return false
			}

			if v == nil {
				return false
			}

			return sbx.CleanupID == v.CleanupID
		})

		sbxlogger.E(sbx).Info("Sandbox killed")
	}()

	return &orchestrator.SandboxCreateResponse{
		ClientId: s.clientID,
	}, nil
}

func (s *server) Update(ctx context.Context, req *orchestrator.SandboxUpdateRequest) (*emptypb.Empty, error) {
	ctx, childSpan := s.tracer.Start(ctx, "sandbox-update")
	defer childSpan.End()

	childSpan.SetAttributes(
		attribute.String("sandbox.id", req.SandboxId),
		attribute.String("client.id", s.clientID),
	)

	item, ok := s.sandboxes.Get(req.SandboxId)
	if !ok {
		errMsg := fmt.Errorf("sandbox not found")
		telemetry.ReportCriticalError(ctx, errMsg)

		return nil, status.New(codes.NotFound, errMsg.Error()).Err()
	}

	item.EndAt = req.EndTime.AsTime()

	return &emptypb.Empty{}, nil
}

func (s *server) List(ctx context.Context, _ *emptypb.Empty) (*orchestrator.SandboxListResponse, error) {
	_, childSpan := s.tracer.Start(ctx, "sandbox-list")
	defer childSpan.End()

	items := s.sandboxes.Items()

	sandboxes := make([]*orchestrator.RunningSandbox, 0, len(items))

	for _, sbx := range items {
		if sbx == nil {
			continue
		}

		if sbx.Config == nil {
			continue
		}

		sandboxes = append(sandboxes, &orchestrator.RunningSandbox{
			Config:    sbx.Config,
			ClientId:  s.clientID,
			StartTime: timestamppb.New(sbx.StartedAt),
			EndTime:   timestamppb.New(sbx.EndAt),
		})
	}

	return &orchestrator.SandboxListResponse{
		Sandboxes: sandboxes,
	}, nil
}

func (s *server) Delete(ctxConn context.Context, in *orchestrator.SandboxDeleteRequest) (*emptypb.Empty, error) {
	ctx, cancel := context.WithTimeoutCause(ctxConn, requestTimeout, fmt.Errorf("request timed out"))
	defer cancel()

	ctx, childSpan := s.tracer.Start(ctx, "sandbox-delete")
	defer childSpan.End()

	childSpan.SetAttributes(
		attribute.String("sandbox.id", in.SandboxId),
		attribute.String("client.id", s.clientID),
	)

	sbx, ok := s.sandboxes.Get(in.SandboxId)
	if !ok {
		errMsg := fmt.Errorf("sandbox '%s' not found", in.SandboxId)
		telemetry.ReportCriticalError(ctx, errMsg)

		return nil, status.New(codes.NotFound, errMsg.Error()).Err()
	}

	// Don't allow connecting to the sandbox anymore.
	s.dns.Remove(in.SandboxId, sbx.Slot.HostIP())

	// Remove the sandbox from the cache to prevent loading it again in API during the time the instance is stopping.
	// Old comment:
	// 	Ensure the sandbox is removed from cache.
	// 	Ideally we would rely only on the goroutine defer.
	s.sandboxes.Remove(in.SandboxId)

	loggingCtx, cancelLogginCtx := context.WithTimeout(ctx, 2*time.Second)
	defer cancelLogginCtx()

	// Check health metrics before stopping the sandbox
	sbx.Healthcheck(loggingCtx, true)
	sbx.LogMetrics(loggingCtx)

	err := sbx.Stop()
	if err != nil {
		sbxlogger.I(sbx).Error("error stopping sandbox", zap.String("sandbox_id", in.SandboxId), zap.Error(err))
	}

	return &emptypb.Empty{}, nil
}

var pauseQueue = semaphore.NewWeighted(maxParalellSnapshotting)

func (s *server) Pause(ctx context.Context, in *orchestrator.SandboxPauseRequest) (*emptypb.Empty, error) {
	ctx, childSpan := s.tracer.Start(ctx, "sandbox-pause")
	defer childSpan.End()

	err := pauseQueue.Acquire(ctx, 1)
	if err != nil {
		telemetry.ReportCriticalError(ctx, err)

		return nil, status.New(codes.ResourceExhausted, err.Error()).Err()
	}

	releaseOnce := sync.OnceFunc(func() {
		pauseQueue.Release(1)
	})

	defer releaseOnce()

	s.pauseMu.Lock()

	sbx, ok := s.sandboxes.Get(in.SandboxId)
	if !ok {
		s.pauseMu.Unlock()

		errMsg := fmt.Errorf("sandbox not found")
		telemetry.ReportCriticalError(ctx, errMsg)

		return nil, status.New(codes.NotFound, errMsg.Error()).Err()
	}

	s.dns.Remove(in.SandboxId, sbx.Slot.HostIP())
	s.sandboxes.Remove(in.SandboxId)

	s.pauseMu.Unlock()

	snapshotTemplateFiles, err := storage.NewTemplateFiles(
		in.TemplateId,
		in.BuildId,
		sbx.Config.KernelVersion,
		sbx.Config.FirecrackerVersion,
		sbx.Config.HugePages,
	).NewTemplateCacheFiles()
	if err != nil {
		errMsg := fmt.Errorf("error creating template files: %w", err)
		telemetry.ReportCriticalError(ctx, errMsg)

		return nil, status.New(codes.Internal, errMsg.Error()).Err()
	}

	defer func() {
		// sbx.Stop sometimes blocks for several seconds,
		// so we don't want to block the request and do the cleanup in a goroutine after we already removed sandbox from cache and DNS.
		go func() {
			err := sbx.Stop()
			if err != nil {
				sbxlogger.I(sbx).Error("error stopping sandbox after snapshot", zap.String("sandbox_id", in.SandboxId), zap.Error(err))
			}
		}()
	}()

	err = os.MkdirAll(snapshotTemplateFiles.CacheDir(), 0o755)
	if err != nil {
		errMsg := fmt.Errorf("error creating sandbox cache dir '%s': %w", snapshotTemplateFiles.CacheDir(), err)
		telemetry.ReportCriticalError(ctx, errMsg)

		return nil, status.New(codes.Internal, errMsg.Error()).Err()
	}

	snapshot, err := sbx.Snapshot(ctx, s.tracer, snapshotTemplateFiles, releaseOnce)
	if err != nil {
		errMsg := fmt.Errorf("error snapshotting sandbox '%s': %w", in.SandboxId, err)
		telemetry.ReportCriticalError(ctx, errMsg)

		return nil, status.New(codes.Internal, errMsg.Error()).Err()
	}

	err = s.templateCache.AddSnapshot(
		snapshotTemplateFiles.TemplateId,
		snapshotTemplateFiles.BuildId,
		snapshotTemplateFiles.KernelVersion,
		snapshotTemplateFiles.FirecrackerVersion,
		snapshotTemplateFiles.Hugepages(),
		snapshot.MemfileDiffHeader,
		snapshot.RootfsDiffHeader,
		snapshot.Snapfile,
		snapshot.MemfileDiff,
		snapshot.RootfsDiff,
	)
	if err != nil {
		errMsg := fmt.Errorf("error adding snapshot to template cache: %w", err)
		telemetry.ReportCriticalError(ctx, errMsg)

		return nil, status.New(codes.Internal, errMsg.Error()).Err()
	}

	telemetry.ReportEvent(ctx, "added snapshot to template cache")

	go func() {
		var memfilePath *string

		switch r := snapshot.MemfileDiff.(type) {
		case *build.NoDiff:
			break
		default:
			memfileLocalPath, err := r.CachePath()
			if err != nil {
				sbxlogger.I(sbx).Error("error getting memfile diff path", zap.Error(err))

				return
			}

			memfilePath = &memfileLocalPath
		}

		var rootfsPath *string

		switch r := snapshot.RootfsDiff.(type) {
		case *build.NoDiff:
			break
		default:
			rootfsLocalPath, err := r.CachePath()
			if err != nil {
				sbxlogger.I(sbx).Error("error getting rootfs diff path", zap.Error(err))

				return
			}

			rootfsPath = &rootfsLocalPath
		}

		b := storage.NewTemplateBuild(
			snapshot.MemfileDiffHeader,
			snapshot.RootfsDiffHeader,
			snapshotTemplateFiles.TemplateFiles,
		)

		err = <-b.Upload(
			context.Background(),
			snapshotTemplateFiles.CacheSnapfilePath(),
			memfilePath,
			rootfsPath,
		)
		if err != nil {
			sbxlogger.I(sbx).Error("error uploading sandbox snapshot", zap.Error(err))

			return
		}
	}()

	return &emptypb.Empty{}, nil
}



================================================
File: internal/server/sandboxes_test.go
================================================
package server

import (
	"context"
	"reflect"
	"testing"
	"time"

	"github.com/e2b-dev/infra/packages/orchestrator/internal/sandbox"
	"github.com/e2b-dev/infra/packages/shared/pkg/grpc/orchestrator"
	"github.com/e2b-dev/infra/packages/shared/pkg/smap"
	"go.opentelemetry.io/otel/trace/noop"
	"google.golang.org/protobuf/types/known/emptypb"
	"google.golang.org/protobuf/types/known/timestamppb"
)

var (
	startTime = time.Now()
	endTime   = time.Now().Add(time.Hour)
)

func Test_server_List(t *testing.T) {
	type args struct {
		ctx context.Context
		in1 *emptypb.Empty
	}
	tests := []struct {
		name    string
		args    args
		want    *orchestrator.SandboxListResponse
		wantErr bool
		data    []*sandbox.Sandbox
	}{
		{
			name: "should return all sandboxes",

			args: args{
				ctx: context.Background(),
				in1: &emptypb.Empty{},
			},
			data: []*sandbox.Sandbox{
				{
					Config: &orchestrator.SandboxConfig{
						TemplateId: "template-id",
					},
					StartedAt: startTime,
					EndAt:     endTime,
				},
			},
			want: &orchestrator.SandboxListResponse{
				Sandboxes: []*orchestrator.RunningSandbox{
					{
						Config: &orchestrator.SandboxConfig{TemplateId: "template-id"},
						// ClientId:  "client-id",
						StartTime: timestamppb.New(startTime),
						EndTime:   timestamppb.New(endTime),
					},
				},
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			s := &server{
				sandboxes: smap.New[*sandbox.Sandbox](),
				tracer:    noop.NewTracerProvider().Tracer(""),
			}
			for _, sbx := range tt.data {
				s.sandboxes.Insert(sbx.Config.SandboxId, sbx)
			}
			got, err := s.List(tt.args.ctx, tt.args.in1)
			if (err != nil) != tt.wantErr {
				t.Errorf("server.List() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			if !reflect.DeepEqual(got, tt.want) {
				t.Errorf("server.List() = %v, want %v", got, tt.want)
			}
		})
	}
}



================================================
File: internal/server/template_cache.go
================================================
package server

import (
	"context"

	"google.golang.org/protobuf/types/known/emptypb"
	"google.golang.org/protobuf/types/known/timestamppb"

	"github.com/e2b-dev/infra/packages/shared/pkg/grpc/orchestrator"
)

func (s *server) ListCachedBuilds(ctx context.Context, _ *emptypb.Empty) (*orchestrator.SandboxListCachedBuildsResponse, error) {
	_, childSpan := s.tracer.Start(ctx, "list-cached-templates")
	defer childSpan.End()

	var builds []*orchestrator.CachedBuildInfo

	for key, item := range s.templateCache.Items() {
		builds = append(builds, &orchestrator.CachedBuildInfo{
			BuildId:        key,
			ExpirationTime: timestamppb.New(item.ExpiresAt()),
		})
	}

	return &orchestrator.SandboxListCachedBuildsResponse{
		Builds: builds,
	}, nil
}


